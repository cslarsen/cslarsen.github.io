<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
  xmlns:content="http://purl.org/rss/1.0/modules/content/"
  xmlns:dc="http://purl.org/dc/elements/1.1/"
  xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd"
  xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
  <channel>
    <title>Christian Stigen Larsen</title>
    <link>https://csl.name</link>
    <description>Personal homepage with posts mainly on programming</description>
    <copyright>&copy; 1996-2017 Christian Stigen Larsen.<br/>Verbatim copying and redistribution of content from this site is permitted provided this notice is preserved.<br/><a href='https://github.com/cslarsen/csl.name'>Site source</a>.</copyright>
    <pubDate>Thu, 16 Nov 2017 19:52:12 +0000</pubDate>
    <item>
      <title>JIT compiling a subset of Python to x86-64</title>
      <link>https://csl.name/post/python-compiler/</link>
      <description><![CDATA[This post shows how to write a basic JIT compiler for the Python bytecode,
using nothing but stock Python modules.
]]></description>
      <pubDate>Thu, 16 Nov 2017 19:52:12 +0000</pubDate>
      <guid>https://csl.name/post/python-compiler/</guid>
      <content:encoded><![CDATA[This post shows how to write a basic JIT compiler for the Python bytecode,
using nothing but stock Python modules.

We will leverage the code written in [a previous post][previous-post] to bind
native code to callable Python functions. The complete code is available at
[github.com/cslarsen/minijit][minijit.github].

At the end of this post, we will be able to compile branchless Python functions
performing arithmetic on signed 64-bit values:

    >>> from jitcompiler import *
    >>> @jit
    ... def foo(a, b): return a*a - b*b
    ...
    --- Installing JIT for <function foo at 0x100c28c08>
    >>> foo(2, 3)
    --- JIT-compiling <function foo at 0x100c28c08>
    -5
    >>> foo(3, 4)
    -7
    >>> print(disassemble(foo))
    0x100b1d000 48 89 fb       mov rbx, rdi
    0x100b1d003 48 89 f8       mov rax, rdi
    0x100b1d006 48 0f af c3    imul rax, rbx
    0x100b1d00a 50             push rax
    0x100b1d00b 48 89 f3       mov rbx, rsi
    0x100b1d00e 48 89 f0       mov rax, rsi
    0x100b1d011 48 0f af c3    imul rax, rbx
    0x100b1d015 48 89 c3       mov rbx, rax
    0x100b1d018 58             pop rax
    0x100b1d019 48 29 d8       sub rax, rbx
    0x100b1d01c c3             ret 

Our strategy is to translate Python bytecode to an [intermediate
representation][ir.wiki], which will then be optimized before being emitted as
x86-64 machine code. So the first part will be to understand how the bytecode
works.

Part one: How the Python bytecode works
---------------------------------------

You can see the raw bytecode for the `foo` function at the top in Python 3 by
typing

    >>> foo.__code__.co_code
    b'|\x00|\x00\x14\x00|\x01|\x01\x14\x00\x18\x00S\x00'

In Python 2.7, that would be

    >>> foo.func_code.co_code
    '|\x00\x00|\x00\x00\x14|\x01\x00|\x01\x00\x14\x18S'

Because the two bytecode sequences are near identical, it doesn't matter which
one will be used for the explanation. **I've picked Python 2.7 for the
remainder of this post**, but the [GitHub code][minijit.github] supports both
2.7 and 3+.

Let's have a look at the disassembly of `foo`.

    >>> import dis
    >>> dis.dis(foo)
      2           0 LOAD_FAST                0 (a)
                  3 LOAD_FAST                0 (a)
                  6 BINARY_MULTIPLY
                  7 LOAD_FAST                1 (b)
                 10 LOAD_FAST                1 (b)
                 13 BINARY_MULTIPLY
                 14 BINARY_SUBTRACT
                 15 RETURN_VALUE

The leftmost number `2` is the Python source code line number. The next column
contains the bytecode offsets.  We clearly see that the `LOAD_FAST` instruction
takes three bytes: One for the opcode (which instruction it is) and two for a
16-bit argument. That argument is zero, referring to the first function
argument `a`. 

CPython — like the JVM, CLR, Forth and many others – is implemented as a [stack
machine][stack-machine]. All the bytecode instructions operate on a _stack_ of
objects. For example, `LOAD_FAST` will _push_ `a` reference to the variable `a`
on the stack, while `BINARY_MULTIPLY` will pop off two, multiply them together
and put their product on the stack. For our purposes, we will treat the stack
as holding _values_.

A beautiful property of postfix systems is that
operations can be serialized. For example, to compute an infix expression like

    2*2 - 3*3

we need to jump back and forth, calculating products before subtracting.
But in a _postfix_ system, we only need to scan forward.  For example, the
above expression can be translated to [Reverse Polish Notation (RPN)][rpn.wiki]
using the [shunting-yard algorithm][shunting-yard.wiki]:

    2 2 * 3 3 * -

Moving from left to right, we push 2 on the stack, then another 2. The `*` pops
them both off the stack and pushes the their product 4. Push 3 and 3, pop them
off and push their product 9. The stack will now contain 9 on the top and 4 at
the bottom. For the final subtraction, we pop them off, perform the subtraction
and push the result -5 on the stack.

Now, imagine that the expression was actually written in a programming
language.

    subtract(multiply(2, 2), multiply(3, 3))

The thing is, in postfix form, the order of evaluation becomes explicit:

    push 2, push 2, multiply, push 3, push 3, multiply, subtract

The use of a stack makes it possible to execute instructions linearly, and this
is essentially how stack machines operate. With that, you will probably
understand most of the [CPython opcodes][python.opcodes] and its [interpreter
loop][python.eval].

Part two: Translating Python bytecode to IR
-------------------------------------------

We will now translate the bytecode instructions to an [intermediate
representation (IR)][ir.wiki]. That is, in a form suitable for performing
things like analysis, translation and optimization. Ours will be blissfully
naive. We will forego things [single-static assignment (SSA)][ssa.wiki] and
[register allocation][register-allocation.wiki] for the sake of simplicity, we
will use something that resembles [three-address codes (TAC)][tac.wiki].

Our IR will consist of pseudo-assembly instructions in a list. For example

    ir = [("mov", "rax", 101),
          ("push", "rax", None)]

Contrary to TAC, we put operation first, followed by the destination and source
registers.  We use `None` to indicate unused registers and arguments.  It would
be a very good idea to use unique, abstract registers names like `reg1`, `reg2`
and so on, because it facilitates [register
allocation][register-allocation.wiki].  Out of scope.

We will reserve registers RAX and RBX for menial work like arithmetic, pushing
and popping.  RAX must also hold the return value, because that's the
convention.  The CPU already has a stack, so we'll use that as our data stack
mechanism.

Registers RDI, RSI, RDC and RCX will be reserved for variables and arguments.
Per [AMD64 convention][amd64.abi], we expect to see function arguments passed
in those registers, in that order. In real programs, the matter is a bit more
involved.

Constants in the bytecode can be looked up with `co_consts`:

    >>> def bar(n): return n*101
    ...
    >>> bar.func_code.co_consts
    (None, 101)

We can now build a compiler that translates Python bytecode to our intermediate
representation. Its general form will be

    class Compiler(object):
        """Compiles Python bytecode to intermediate representation (IR)."""

        def __init__(self, bytecode, constants):
            self.bytecode = bytecode
            self.constants = constants
            self.index = 0

        def fetch(self):
            byte = self.bytecode[self.index]
            self.index += 1
            return byte

        def decode(self):
            opcode = self.fetch()
            opname = dis.opname[opcode]

            if opname.startswith(("UNARY", "BINARY", "INPLACE", "RETURN")):
                argument = None
            else:
                argument = self.fetch()

            return opname, argument

        # ...

It takes some bytecode and constants, and keeps a running index of the current
bytecode position. It is wise to split the translation up into fetch and decode
steps. The `fetch` method simply retrieves the next bytecode, while `decode`
will fetch the opcode, look up its name and fetch any arguments.

We need to look up which registers holds which variable:

    def variable(self, number):
        # AMD64 argument passing order for our purposes.
        order = ("rdi", "rsi", "rdx", "rcx")
        return order[number]

The main method will look like


    def compile(self):
        while self.index < len(self.bytecode):
            op, arg = self.decode()

            if op == "LOAD_FAST":
                yield "push", self.variable(arg), None
            # ...
            else:
                raise NotImplementedError(op)

Here you can already see how we translate `LOAD_FAST`. We just push the
corresponding register onto the stack. So, if the function we are compiling has
one argument, the bytecode will refer to the zeroth variable. Through the
`variable` method, we see that this is register RDI. So it will output

    ("push", "rdi", "None")

The `STORE_FAST` instruction does the reverse. It pops a value off the stack
and stores it in the argument register:

        yield "pop", "rax", None
        yield "mov", self.variable(arg), "rax"

A binary instruction will pop two values off the stack. For example

        elif op == "BINARY_MULTIPLY":
            yield "pop", "rax", None
            yield "pop", "rbx", None
            yield "imul", "rax", "rbx"
            yield "push", "rax", None

That's just about it. `LOAD_CONST` will use a special instruction for storing
immediate values (i.e., constant integers). Here is the entire method:

    def compile(self):
        while self.index < len(self.bytecode):
            op, arg = self.decode()

            if op == "LOAD_FAST":
                yield "push", self.variable(arg), None

            elif op == "STORE_FAST":
                yield "pop", "rax", None
                yield "mov", self.variable(arg), "rax"

            elif op == "LOAD_CONST":
                yield "immediate", "rax", self.constants[arg]
                yield "push", "rax", None

            elif op == "BINARY_MULTIPLY":
                yield "pop", "rax", None
                yield "pop", "rbx", None
                yield "imul", "rax", "rbx"
                yield "push", "rax", None

            elif op in ("BINARY_ADD", "INPLACE_ADD"):
                yield "pop", "rax", None
                yield "pop", "rbx", None
                yield "add", "rax", "rbx"
                yield "push", "rax", None

            elif op in ("BINARY_SUBTRACT", "INPLACE_SUBTRACT"):
                yield "pop", "rbx", None
                yield "pop", "rax", None
                yield "sub", "rax", "rbx"
                yield "push", "rax", None

            elif op == "UNARY_NEGATIVE":
                yield "pop", "rax", None
                yield "neg", "rax", None
                yield "push", "rax", None

            elif op == "RETURN_VALUE":
                yield "pop", "rax", None
                yield "ret", None, None
            else:
                raise NotImplementedError(op)

We can now compile the `foo` function at the top to our IR.

    >>> def foo(a, b):
    ...   return a*a - b*b
    ...
    >>> bytecode = map(ord, foo.func_code.co_code)
    >>> constants = foo.func_code.co_consts
    >>> ir = Compiler(bytecode, constants).compile()
    >>> ir = list(ir)
    >>>
    >>> from pprint import pprint
    >>> pprint(ir)
    [('push', 'rdi', None),
     ('push', 'rdi', None),
     ('pop', 'rax', None),
     ('pop', 'rbx', None),
     ('imul', 'rax', 'rbx'),
     ('push', 'rax', None),
     ('push', 'rsi', None),
     ('push', 'rsi', None),
     ('pop', 'rax', None),
     ('pop', 'rbx', None),
     ('imul', 'rax', 'rbx'),
     ('push', 'rax', None),
     ('pop', 'rbx', None),
     ('pop', 'rax', None),
     ('sub', 'rax', 'rbx'),
     ('push', 'rax', None),
     ('pop', 'rax', None),
     ('ret', None, None)]

Wow, that sure is a lot of stack operations!

Part three: Writing a simple optimizer
--------------------------------------

We're going to perform [peephole optimizations][peephole.wiki] on our IR. Such
optimizations work on only a few instructions at at time, and translate them
equivalent but _better_ code. We will go for fewer instructions.

In the IR above, we see an obvious improvement. Instructions like

    push rdi
    pop rax

can surely be translated to

    mov rax, rdi

Let's write a function for that. We'll also eliminate nonsensical instructions
like `mov rax, rax`.

    def optimize(ir):
        def fetch(n):
            if n < len(ir):
                return ir[n]
            else:
                return None, None, None

        index = 0
        while index < len(ir):
            op1, a1, b1 = fetch(index)
            op2, a2, b2 = fetch(index + 1)
            op3, a3, b3 = fetch(index + 2)
            op4, a4, b4 = fetch(index + 3)

            # Removed no-op movs
            if op1 == "mov" and a1 == b1:
                index += 1
                continue

            # Short-circuit push x/pop y
            if op1 == "push" and op2 == "pop":
                index += 2
                yield "mov", a2, a1
                continue

            index += 1
            yield op1, a1, b1

Instead of showing that this actually works, we'll just throw in a few other
optimizations. Just note that writing such optimizations are deceptively
simple. It's very easy to do something that seem to make sense, only to see
your program crash.

A construct like

    mov rsi, rax
    mov rbx, rsi

can surely be translated to

    mov rbx, rax

so we'll add that as well:

    if op1 == op2 == "mov" and a1 == b2:
        index += 2
        yield "mov", a2, b1
        continue

Finally, the short-circuit of pop and push can be extended so that it works
over one or several unrelated instructions. Take

    push rax
    mov rsi, rax
    pop rbx

Since RAX isn't modified in `mov rsi, rax`, we can just write

    mov rsi, rax
    mov rbx, rax

We have to be careful that the middle instruction isn't a push, though.
So we'll add

    if op1 == "push" and op3 == "pop" and op2 not in ("push", "pop"):
        if a2 != a3:
            index += 3
            yield "mov", a3, a1
            yield op2, a2, b2
            continue

There is nothing wrong with supporting an indefinite amount of middle
instructions, but we won't do that here.

With these instructions, let's try to optimize the above IR. The complete
optimization function is

    def optimize(ir):
        def fetch(n):
            if n < len(ir):
                return ir[n]
            else:
                return None, None, None

        index = 0
        while index < len(ir):
            op1, a1, b1 = fetch(index)
            op2, a2, b2 = fetch(index + 1)
            op3, a3, b3 = fetch(index + 2)

            if op1 == "mov" and a1 == b1:
                index += 1
                continue

            if op1 == op2 == "mov" and a1 == b2:
                index += 2
                yield "mov", a2, b1
                continue

            if op1 == "push" and op2 == "pop":
                index += 2
                yield "mov", a2, a1
                continue

            if op1 == "push" and op3 == "pop" and op2 not in ("push", "pop"):
                if a2 != a3:
                    index += 3
                    yield "mov", a3, a1
                    yield op2, a2, b2
                    continue

            index += 1
            yield op1, a1, b1

The IR code was

    [('push', 'rdi', None),
     ('push', 'rdi', None),
     ('pop', 'rax', None),
     ('pop', 'rbx', None),
     ('imul', 'rax', 'rbx'),
     ('push', 'rax', None),
     ('push', 'rsi', None),
     ('push', 'rsi', None),
     ('pop', 'rax', None),
     ('pop', 'rbx', None),
     ('imul', 'rax', 'rbx'),
     ('push', 'rax', None),
     ('pop', 'rbx', None),
     ('pop', 'rax', None),
     ('sub', 'rax', 'rbx'),
     ('push', 'rax', None),
     ('pop', 'rax', None),
     ('ret', None, None)]

Running that through `optimize` yields

    >>> pprint(list(optimize(ir)))
    [('push', 'rdi', None),
     ('mov', 'rax', 'rdi'),
     ('pop', 'rbx', None),
     ('imul', 'rax', 'rbx'),
     ('push', 'rax', None),
     ('push', 'rsi', None),
     ('mov', 'rax', 'rsi'),
     ('pop', 'rbx', None),
     ('imul', 'rax', 'rbx'),
     ('mov', 'rbx', 'rax'),
     ('pop', 'rax', None),
     ('sub', 'rax', 'rbx'),
     ('mov', 'rax', 'rax'),
     ('ret', None, None)]

saving us four instructions. But we still got a few spots left. The first three
instructions should be optimizable. Let's run two passes on the IR:

    >>> pprint(list(optimize(list(optimize(ir)))))
    [('mov', 'rbx', 'rdi'),
     ('mov', 'rax', 'rdi'),
     ('imul', 'rax', 'rbx'),
     ('push', 'rax', None),
     ('mov', 'rbx', 'rsi'),
     ('mov', 'rax', 'rsi'),
     ('imul', 'rax', 'rbx'),
     ('mov', 'rbx', 'rax'),
     ('pop', 'rax', None),
     ('sub', 'rax', 'rbx'),
     ('ret', None, None)]

We've now saved seven instructions. Our optimizer won't be able to improve this
code any further. We could add more peephole optimizations, but another good
technique would be to use a real register allocated so that we use the full
spectrum of available registers. The IR compiler could then just assign values
to unique registers like `reg1`, `reg2` and so on, then the allocator would
choose how to populate the available registers properly. This is actually a hot
topic for research, and especially for JIT compilation because the general
problem is NP-complete.

Part four: Translating IR to x86-64 machine code
------------------------------------------------

So, we have translated Python bytecode to IR and we have done some
optimizations on it. We are finally ready to assemble it to machine code!

Our approach will be to write an assembler class that emits instructions. If we
use the same name for the emitter methods as in the IR code, and use the same
signature for all, then we can just blindly assemble the whole IR in a short
loop:

    assembler = Assembler(mj.PAGESIZE)

    for name, a, b in ir:
        emit = getattr(assembler, name)
        emit(a, b)

If the instruction is `mov rax, rbx`, then `emit` will point to `assembler.mov`
and the call will therefore be `assembler.mov("rax", "rbx")`.

Let's write an assembler class. We copy the code for `address`, `little_endian`
and import `create_block` from the [code in the previous post][previous-post].

    class Assembler(object):
        def __init__(self, size):
            self.block = mj.create_block(size)
            self.index = 0
            self.size = size

        @property
        def address(self):
            """Returns address of block in memory."""
            return ctypes.cast(self.block, ctypes.c_void_p).value

        def little_endian(self, n):
            """Converts 64-bit number to little-endian format."""
            return [(n & (0xff << i*2)) >> i*8 for i in range(8)]

        def emit(self, *args):
            """Writes machine code to memory block."""
            for code in args:
                self.block[self.index] = code
                self.index += 1

        def ret(self, a, b):
            self.emit(0xc3)

        # ...

So calling `assembler.ret(None, None)` will set the first machine code byte to
0xc3. That's how `retq` is encoded. To find the encoding of other instructions,
I mainly used the [NASM][nasm] assembler. Putting the following in a file
`sandbox.asm`,

    bits 64
    section .text
    mov rax, rcx
    mov rax, rdx
    mov rax, rbx
    mov rax, rsp

I assembled it with

    $ nasm -felf64 sandbox.asm -osandbox.o

(`-fmacho64` for macOS) and dumped the machine code with

    $ objdump -d sandbox.o

    sandbox.o:     file format elf64-x86-64


    Disassembly of section .text:

    0000000000000000 <.text>:
       0:   48 89 c8                mov    %rcx,%rax
       3:   48 89 d0                mov    %rdx,%rax
       6:   48 89 d8                mov    %rbx,%rax
       9:   48 89 e0                mov    %rsp,%rax

It seems like the 64-bit `movq` (which _we_ just call `mov`) is encoded with
the prefix `0x48 0x89` with the source and destination registers stored in the
last byte.  Digging into a few manuals, we see that they are encoded using
three bits each.  We'll write a method for that.

    def registers(self, a, b=None):
        """Encodes one or two registers for machine code instructions."""
        order = ("rax", "rcx", "rdx", "rbx", "rsp", "rbp", "rsi", "rdi")
        enc = order.index(a)
        if b is not None:
            enc = enc << 3 | order.index(b)
        return enc

For the `movq` instruction, we can now write

    def mov(self, a, b):
        self.emit(0x48, 0x89, 0xc0 | self.registers(b, a))

The rest of the instructions are done in a similar manner, except for moving
immediate (i.e., constant) values into registers.

    def ret(self, a, b):
        self.emit(0xc3)

    def push(self, a, _):
        self.emit(0x50 | self.registers(a))

    def pop(self, a, _):
        self.emit(0x58 | self.registers(a))

    def imul(self, a, b):
        self.emit(0x48, 0x0f, 0xaf, 0xc0 | self.registers(a, b))

    def add(self, a, b):
        self.emit(0x48, 0x01, 0xc0 | self.registers(b, a))

    def sub(self, a, b):
        self.emit(0x48, 0x29, 0xc0 | self.registers(b, a))

    def neg(self, a, _):
        self.emit(0x48, 0xf7, 0xd8 | self.register(a))

    def mov(self, a, b):
        self.emit(0x48, 0x89, 0xc0 | self.registers(b, a))

    def immediate(self, a, number):
        self.emit(0x48, 0xb8 | self.registers(a), *self.little_endian(number))

The only special thing about the last method is that we have to use a different
prefix and encode the number in little-endian format.

The final part
--------------

Finally, we can tie everything together. Given the function

    def foo(a, b):
      return a*a - b*b

we first extract the Python bytecode, using `ord` to map bytes to integers, and
any constants

    bytecode = map(ord, foo.func_code.co_code)
    constants = foo.func_code.co_consts

Compiling to IR

    ir = Compiler(bytecode, constants).compile()
    ir = list(ir)

Perform a few optimization passes:

    ir = list(optimize(ir))
    ir = list(optimize(ir))
    ir = list(optimize(ir))

Assemble to native code

    assembler = Assembler(mj.PAGESIZE)
    for name, a, b in ir:
        emit = getattr(assembler, name)
        emit(a, b)

Make the memory block executable

    mj.make_executable(assembler.block, assembler.size)

We use `ctypes` to set the correct signature and cast the code to a callable
Python function. We can get the number of arguments with `co_argcount`, and we
treat input arguments as signed 64-bit integers.

    argcount = foo.func_code.co_argcount
    signature = ctypes.CFUNCTYPE(*[ctypes.c_int64] * argcount)
    signature.restype = ctypes.c_int64

Finally,

    native_foo = signature(assembler.address)
    print(native_foo(2, 3))

It prints -5. Yay!

To disassemble the code, we can use the [Capstone disassembler][capstone] right
within Python. It's not a built-in module, so you need to install it yourself.
Or you can break into the Python process with a debugger. Here is the
disassembly for `native_foo`:

    0x7f1133351000:       mov     rbx, rdi
    0x7f1133351003:       mov     rax, rdi
    0x7f1133351006:       imul    rax, rbx
    0x7f113335100a:       push    rax
    0x7f113335100b:       mov     rbx, rsi
    0x7f113335100e:       mov     rax, rsi
    0x7f1133351011:       imul    rax, rbx
    0x7f1133351015:       mov     rbx, rax
    0x7f1133351018:       pop     rax
    0x7f1133351019:       sub     rax, rbx
    0x7f113335101c:       ret

You can try out different functions, for example

    def bar(n):
      return n * 0x101

turns into

    0x7f07d16a7000:       mov     rbx, rdi
    0x7f07d16a7003:       movabs  rax, 0x101
    0x7f07d16a700d:       imul    rax, rbx
    0x7f07d16a7011:       ret

and

    def baz(a, b, c):
      a -= 1
      return a + 2*b -c

becomes

    0x7f13fba09000:       push    rdi
    0x7f13fba09001:       movabs  rax, 1
    0x7f13fba0900b:       mov     rbx, rax
    0x7f13fba0900e:       pop     rax
    0x7f13fba0900f:       sub     rax, rbx
    0x7f13fba09012:       mov     rdi, rax
    0x7f13fba09015:       push    rdi
    0x7f13fba09016:       movabs  rax, 2
    0x7f13fba09020:       mov     rbx, rax
    0x7f13fba09023:       mov     rax, rsi
    0x7f13fba09026:       imul    rax, rbx
    0x7f13fba0902a:       pop     rbx
    0x7f13fba0902b:       add     rax, rbx
    0x7f13fba0902e:       mov     rbx, rdx
    0x7f13fba09031:       sub     rax, rbx
    0x7f13fba09034:       ret

You may wonder how fast this runs. The answer is: Slow. The reason is: Because
there is inherent overhead when calling into native code with `ctypes`. It
needs to convert arguments and so on. I also believe (but haven't
double-checked) that it saves some registers, because per the convention, we
should have restored RBX before exiting.

But it would be interesting to compile larger functions with native function
calls, loops and so on, and compare that with Python. At that point, I believe
you'll see the native code going much faster.

JITing automatically
--------------------

On [/r/compsci][reddit.comscpi] there was a comment that this really isn't
just-in-time compilation until there is some mechanism that automatically swaps
out a function with a compiled version. So let's try to do something about
that.

A pretty obvious approach is to require a little help from the user. Use a
decorator. Recall that a decorator is really just a function that gets the
freshly defined object as the first argument. If we install a little closure
there that remembers the original function, we can then *literally* compile
just-in-time when it is called for the first time. Again, *only* the decorated
functions that are actually called will be compiled to native code.

We'll start without anything:

    def jit(function):
        def frontend(*args, **kw):
            # Just pass on the call to the original function
            return function(*args, **kw)
        return frontend

With this, we can mark functions that we want to be compiled:

    @jit
    def foo(a, b):
        return a*a - b*b

So the inner `frontend` function then just needs to check if the function has
already been compiled. If not, compile it and install it as the local function
reference. If the compilation fails, don't swap out anything. The complete
decorator looks like this:

    def jit(function):
        def frontend(*args, **kw):
            if not hasattr(frontend, "function"):
                # We haven't tried to compile the function yet.
                try:
                    # Compile function and set it as the active one
                    native, asm = compile_native(function, verbose=False)
                    frontend.function = native
                except Exception as e:
                    # Oops, the compilation failed. Just fall silently back to
                    # the original function.
                    frontend.function = function

            # Call either the original or compiled function with the
            # user-supplied arguments
            return frontend.function(*args, **kw)

        # Make all calls to the decorated function go through "frontend"
        return frontend

See the GitHub repository for an example program that uses this.

What's next?
------------

I believe this is good for learning, so play around a bit. Try to make a
register allocator, for example. Create more peephole optimizations. Add
support for calling other functions, loops.

With a decorator, you should be able to swap out class methods on the fly with
compiled ones. That's exactly what  [Numba][numba] does, but ours is just a
drop in the ocean compared to that.

While we took the approach of translating Python bytecode, another good
technique is to use the `ast` module to traverse the abstract syntax tree. A
guy [did that][pyast64].

[amd64.abi]: https://software.intel.com/sites/default/files/article/402129/mpx-linux64-abi.pdf
[capstone]: http://www.capstone-engine.org/lang_python.html
[constant-folding]: https://en.wikipedia.org/wiki/Constant_folding
[cpython-eval]: https://github.com/python/cpython/blob/1896793/Python/ceval.c#L1055
[github]: https://github.com/cslarsen/minijit
[hn.front]: https://news.ycombinator.com/front?day=2017-11-09
[hn]: https://news.ycombinator.com/item?id=15665581
[ir.wiki]: https://en.wikipedia.org/wiki/Intermediate_representation
[minijit.github]: https://github.com/cslarsen/minijit
[mj.github]: https://github.com/cslarsen/minijit
[nasm]: http://www.nasm.us
[numba]: https://numba.pydata.org/
[peephole.wiki]: https://en.wikipedia.org/wiki/Peephole_optimization
[previous-post]: /post/python-jit/
[pyast64]: https://github.com/benhoyt/pyast64
[python.eval]: https://github.com/python/cpython/blob/1896793/Python/ceval.c#L1055
[python.opcodes]: https://github.com/python/cpython/blob/master/Include/opcode.h
[reddit.comscpi]: https://www.reddit.com/r/compsci/comments/7dfic7/jit_compiling_a_tiny_subset_of_python_to_x8664/
[register-allocation.wiki]: https://en.wikipedia.org/wiki/Register_allocation
[registers.wiki]: https://en.wikipedia.org/wiki/Processor_register
[rpn.wiki]: https://en.wikipedia.org/wiki/Reverse_Polish_notation
[shunting-yard.wiki]: https://en.wikipedia.org/wiki/Shunting-yard_algorithm
[ssa.wiki]: https://en.wikipedia.org/wiki/Static_single_assignment_form
[stack-machine]: https://en.wikipedia.org/wiki/Stack_machine
[stack-register.wiki]: https://en.wikipedia.org/wiki/Stack_register
[tac.wiki]: https://en.wikipedia.org/wiki/Three-address_code
]]></content:encoded>
      <dc:date>2017-11-16T19:52:12+00:00</dc:date>
    </item>
    <item>
      <title>Writing a basic x86-64 JIT compiler from scratch in stock Python</title>
      <link>https://csl.name/post/python-jit/</link>
      <description><![CDATA[In this post I&#39;ll show how to write a rudimentary, native x86-64 just-in-time
compiler (JIT) in CPython, using only the built-in modules.
]]></description>
      <pubDate>Wed, 08 Nov 2017 21:03:00 +0000</pubDate>
      <guid>https://csl.name/post/python-jit/</guid>
      <content:encoded><![CDATA[In this post I'll show how to write a rudimentary, native x86-64 [just-in-time
compiler (JIT)][jit.wiki] in CPython, using only the built-in modules.

Update: This post made the [front page of HN][hn.front], and I've incorporated
some of the [discussion feedback][hn]. I've also written a [follow-up
post][follow.up] that JITs Python bytecode to x86-64.

The code here specifically targets the UNIX systems macOS and Linux, but should
be easily translated to other systems such as Windows. The complete code is
available on [github.com/cslarsen/minijit][github].

The goal is to generate new versions of the below assembly code at runtime and
execute it.

    48 b8 ed ef be ad de  movabs $0xdeadbeefed, %rax
    00 00 00
    48 0f af c7           imul   %rdi,%rax
    c3                    retq

We will mainly deal with the left hand side — the byte sequence `48 b8 ed ...`
and so on.  Those fifteen [machine code bytes][machine.code.wiki] comprise an
x86-64 function that multiplies its argument with the constant
[`0xdeadbeefed`][deadbeef]. The JIT step will create functions with different
such constants.  While being a contrived form of
[specialization][specialization.wiki], it illuminates the basic mechanics of
just-in-time compilation.

Our general strategy is to rely on the built-in [`ctypes`][ctypes.doc] Python
module to load the C standard library. From there, we can access system
functions to interface with the virtual memory manager. We'll use
[`mmap`][mmap.man] to fetch a page-aligned block of memory. It needs to be
aligned for it to become executable. That's the reason why we can't simply use
the usual C function `malloc`, because it may return memory that spans page
boundaries.

The function [`mprotect`][mprotect.man] will be used to mark the memory block as
read-only and executable. After that, we should be able to call into our
freshly compiled block of code through ctypes.

The boiler-plate part
---------------------

Before we can do anything, we need to load the standard C library.

    import ctypes
    import sys

    if sys.platform.startswith("darwin"):
        libc = ctypes.cdll.LoadLibrary("libc.dylib")
        # ...
    elif sys.platform.startswith("linux"):
        libc = ctypes.cdll.LoadLibrary("libc.so.6")
        # ...
    else:
        raise RuntimeError("Unsupported platform")

There are other ways to achieve this, for example

    >>> import ctypes
    >>> import ctypes.util
    >>> libc = ctypes.CDLL(ctypes.util.find_library("c"))
    >>> libc
    <CDLL '/usr/lib/libc.dylib', handle 110d466f0 at 103725ad0>

To find the page size, we'll call [`sysconf(_SC_PAGESIZE)`][sysconf.man]. The
`_SC_PAGESIZE` constant is 29 on macOS but 30 on Linux. We'll just hard-code
those in our program. You can find them by digging into system header files or
writing a simple C program that print them out. A more robust and elegant
solution would be to use the [`cffi` module][cffi.github] instead of ctypes,
because it can automatically parse header files. However, since I wanted to
stick to the default CPython distribution, we'll continue using ctypes.

We need a few additional constants for `mmap` and friends. They're just written
out below. You may have to look them up for other UNIX variants.

    import ctypes
    import sys

    if sys.platform.startswith("darwin"):
        libc = ctypes.cdll.LoadLibrary("libc.dylib")
        _SC_PAGESIZE = 29
        MAP_ANONYMOUS = 0x1000
        MAP_PRIVATE = 0x0002
        PROT_EXEC = 0x04
        PROT_NONE = 0x00
        PROT_READ = 0x01
        PROT_WRITE = 0x02
        MAP_FAILED = -1 # voidptr actually
    elif sys.platform.startswith("linux"):
        libc = ctypes.cdll.LoadLibrary("libc.so.6")
        _SC_PAGESIZE = 30
        MAP_ANONYMOUS = 0x20
        MAP_PRIVATE = 0x0002
        PROT_EXEC = 0x04
        PROT_NONE = 0x00
        PROT_READ = 0x01
        PROT_WRITE = 0x02
        MAP_FAILED = -1 # voidptr actually
    else:
        raise RuntimeError("Unsupported platform")

Although not strictly required, it is very useful to tell ctypes the signature
of the functions we'll use. That way, we'll get exceptions if we mix invalid
types. For example

    # Set up sysconf
    sysconf = libc.sysconf
    sysconf.argtypes = [ctypes.c_int]
    sysconf.restype = ctypes.c_long

tells ctypes that `sysconf` is a function that takes a single integer and
produces a long integer. After this, we can get the current page size with

    pagesize = sysconf(_SC_PAGESIZE)

The machine code we are going to generate will be interpreted as unsigned 8-bit
bytes, so we need to declare a new pointer type:

    # 8-bit unsigned pointer type
    c_uint8_p = ctypes.POINTER(ctypes.c_uint8)

Below we just dish out the remaining signatures for the functions that we'll
use. For error reporting, it's good to have the [`strerror`][strerror.man]
function available.  We'll use [`munmap`][munmap.man] to destroy the machine
code block after we're done with it.  It lets the operating system reclaim that
memory.

    strerror = libc.strerror
    strerror.argtypes = [ctypes.c_int]
    strerror.restype = ctypes.c_char_p

    mmap = libc.mmap
    mmap.argtypes = [ctypes.c_void_p,
                     ctypes.c_size_t,
                     ctypes.c_int,
                     ctypes.c_int,
                     ctypes.c_int,
                     # Below is actually off_t, which is 64-bit on macOS
                     ctypes.c_int64]
    mmap.restype = c_uint8_p

    munmap = libc.munmap
    munmap.argtypes = [ctypes.c_void_p, ctypes.c_size_t]
    munmap.restype = ctypes.c_int

    mprotect = libc.mprotect
    mprotect.argtypes = [ctypes.c_void_p, ctypes.c_size_t, ctypes.c_int]
    mprotect.restype = ctypes.c_int

At this point, it's hard to justify using Python rather than C. With C, we
don't need any of the above boiler-plate code. But down the line, Python will
allow us to experiment much more easily.

Now we're ready to write the `mmap` wrapper.

    def create_block(size):
        ptr = mmap(0, size, PROT_WRITE | PROT_READ,
                MAP_PRIVATE | MAP_ANONYMOUS, 0, 0)

        if ptr == MAP_FAILED:
            raise RuntimeError(strerror(ctypes.get_errno()))

        return ptr

This function uses `mmap` to allocate page-aligned memory for us. We mark the
memory region as readable and writable with the `PROT` flags, and we also mark
it as private and anonymous. The latter means the memory will not be visible
from other processes and that it will not be file-backed. The [Linux `mmap`
manual page][mmap.man] covers the details (but be sure to view the man page for
your system). If the `mmap` call fails, we raise it as a Python error.

To mark memory as executable,

    def make_executable(block, size):
        if mprotect(block, size, PROT_READ | PROT_EXEC) != 0:
            raise RuntimeError(strerror(ctypes.get_errno()))

With this `mprotect` call, we mark the region as readable and executable. If we
wanted to, we could have made it writable as well, but some systems will refuse
to execute writable memory.  This is sometimes called [the W^X security
feature][wx.wiki].

To destroy the memory block, we'll use

    def destroy_block(block, size):
        if munmap(block, size) == -1:
            raise RuntimeError(strerror(ctypes.get_errno()))

I edited out a badly placed `del` in that function after the HN submission.

The fun part
------------

Now we're finally ready to create an insanely simple piece of JIT code!

Recall the assembly listing at the top: It's a small function — without a local
stack frame — that multiplies an input number with a constant. In Python, we'd
write that as

    def create_multiplication_function(constant):
        return lambda n: n * constant

This is indeed a contrived example, but qualifies as JIT. After all, we do
create native code at runtime and execute it. It's easy to imagine more
advanced examples such as JIT-compiling [Brainfuck][brainfuck.wiki] to x86-64
machine code. Or using [AVX][avx.wiki] instructions for blazing fast,
vectorized math ops.

The disassembly at the top of this post was actually generated by compiling and
disassembling the following C code:

    #include <stdint.h>

    uint64_t multiply(uint64_t n)
    {
      return n*0xdeadbeefedULL;
    }

If you want to compile it yourself, use something like

    $ gcc -Os -fPIC -shared -fomit-frame-pointer \
        -march=native multiply.c -olibmultiply.so

Here I optimized for space (`-Os`) to generate as little machine code as
possible, with position-independent code (`-fPIC`) to prevent using absolute
jumps, without any frame pointers (`-fomit-frame-pointer`) to remove
superfluous stack setup code (but it may be required for more advanced
functions) and using the current CPU's native instruction set
(`-march=native`).

We could have passed `-S` to produce a disassembly listing, but we're actually
interested in the _machine code_, so we'll rather use a tool like `objdump`:

    $ objdump -d libmultiply.so
    ...
    0000000000000f71 <_multiply>:
     f71:	48 b8 ed ef be ad de 	movabs $0xdeadbeefed,%rax
     f78:	00 00 00 
     f7b:	48 0f af c7          	imul   %rdi,%rax
     f7f:	c3                   	retq

In case you are not familiar with assembly, I'll let you know how this function
works. First, the `movabs` function just puts an _immediate_ number in the RAX
register. _Immediate_ is assembly-jargon for encoding something right in the
machine code. In other words, it's an embedded argument for the `movabs`
instruction. So RAX now holds the constant `0xdeadbeefed`.

Also — by [AMD64][amd64.abi] convention — the first integer argument will be in RDI, and the return value in RAX.
 So RDI will hold the number to multiply with. That's what `imul` does. It
multiplies RAX and RDI and puts the result in RAX. Finally, we pop a 64-bit
return address off the stack and jump to it with RETQ. At this level, it's easy
to imagine how one could implement [continuation-passing style][cps].

Note that the constant `0xdeadbeefed` is in little-endian format. We need to
remember to do the same when we patch the code. (By the way, a good mnemonic
for remembering the word order is that little endian means "little-end first").

We are now ready to put everything in a Python function.

    def make_multiplier(block, multiplier):
        # Encoding of: movabs <multiplier>, rax
        block[0] = 0x48
        block[1] = 0xb8

        # Little-endian encoding of multiplication constant
        block[2] = (multiplier & 0x00000000000000ff) >>  0
        block[3] = (multiplier & 0x000000000000ff00) >>  8
        block[4] = (multiplier & 0x0000000000ff0000) >> 16
        block[5] = (multiplier & 0x00000000ff000000) >> 24
        block[6] = (multiplier & 0x000000ff00000000) >> 32
        block[7] = (multiplier & 0x0000ff0000000000) >> 40
        block[8] = (multiplier & 0x00ff000000000000) >> 48
        block[9] = (multiplier & 0xff00000000000000) >> 56

        # Encoding of: imul rdi, rax
        block[10] = 0x48
        block[11] = 0x0f
        block[12] = 0xaf
        block[13] = 0xc7

        # Encoding of: retq
        block[14] = 0xc3

        # Return a ctypes function with the right prototype
        function = ctypes.CFUNCTYPE(ctypes.c_uint64)
        function.restype = ctypes.c_uint64
        return function

At the bottom, we return the ctypes function signature to be used with this
code. It's somewhat arbitrarily placed, but I thought it was good to keep the
signature close to the machine code.

The final part
--------------

Now that we have the basic parts we can weave everything together. The first
part is to allocate one page of memory:

    pagesize = sysconf(_SC_PAGESIZE)
    block = create_block(pagesize)

Next, we generate the machine code. Let's pick the number 101 to use as a
multiplier.

    mul101_signature = make_multiplier(block, 101)

We now mark the memory region as executable and read-only:

    make_executable(block, pagesize)

Take the address of the first byte in the memory block and cast it to a
callable ctypes function with proper signature:

    address = ctypes.cast(block, ctypes.c_void_p).value
    mul101 = mul101_signature(address)

To get the memory address of the block, we use ctypes to cast it to a void
pointer and extract its value. Finally, we instantiate an actual function from
this address using the `mul101_signature` constructor.

Voila! We now have a piece of _native_ code that we can call from Python. If
you're in a REPL, you can try it directly:

    >>> print(mul101(8))
    808

Note that this small multiplication function will run slower than a native
Python calculation. That's mainly because ctypes, being a foreign-function
library, has a lot of overhead: It needs to inspect what dynamic types you pass
the function every time you call it, then unbox them, convert them and then do
the same with the return value. So the trick is to either use assembly because
you have to access some new Intel instruction, or because you compile something
like Brainfuck to native code.

Finally, if you want to, you can let the system reclaim the memory holding the
function. Beware that after this, you will probably crash the process if you
try calling the code again. So probably best to delete all references in
Python as well:

    destroy_block(block, pagesize)

    del block
    del mul101

If you run the code in its complete form from the [GitHub][github] repository,
you can put the multiplication constant on the command line:

    $ python mj.py 101
    Pagesize: 4096
    Allocating one page of memory
    JIT-compiling a native mul-function w/arg 101
    Making function block executable
    Testing function
    OK   mul(0) = 0
    OK   mul(1) = 101
    OK   mul(2) = 202
    OK   mul(3) = 303
    OK   mul(4) = 404
    OK   mul(5) = 505
    OK   mul(6) = 606
    OK   mul(7) = 707
    OK   mul(8) = 808
    OK   mul(9) = 909
    Deallocating function

Debugging JIT-code
------------------

If you want to continue learning with this simple program, you'll quickly want
to disassemble the machine code you generate. One option is to simply use gdb
or lldb, but you need to know where to break. One trick is to just print the
hex value of the `block` address and then wait for a keystroke:

    print("address: 0x%x" % address)
    print("Press ENTER to continue")
    raw_input()

Then you just run the program in the debugger, break into the debugger while
the program is pausing, and disassemble the memory location. Of course you can
also step-debug through the assembly code if you want to see what's going on.
Here's an example lldb session:

    $ lldb python
    ...
    (lldb) run mj.py 101
    ...
    (lldb) c
    Process 19329 resuming
    ...
    address 0x1002fd000
    Press ENTER to continue

At this point, hit CTRL+C to break back into the debugger, then disassemble
from the memory location:

    (lldb) x/3i 0x1002fd000
        0x1002fd000: 48 b8 65 00 00 00 00 00 00 00  movabsq $0x65, %rax
        0x1002fd00a: 48 0f af c7                    imulq  %rdi, %rax
        0x1002fd00e: c3                             retq

Notice that 65 hex is 101 in decimal, which was the command line argument we
passed above.

If you only want a disassembler inside Python, I recommend the
[Capstone][capstone] module.

What's next?
------------

A good exercise would be to JIT-compile [Brainfuck programs][brainfuck.wiki] to
native code. If you want to jump right in, I've made a GitHub repository at
[github.com/cslarsen/brainfuck-jit][brainfuck.github]. I even have a
[Speaker Deck presentation][speakerdeck] to go with it. It performs JIT-ing and
optimizations, but uses GNU Lightning to compile native code instead of this
approach. It should be extremely simple to boot out GNU Lightning in favor or
some code generation of your own. An interesting note on the Brainfuck project
is that if you just JIT-compile each Brainfuck instruction one-by-one, you
won't get much of a speed boost, even if you run native code. The entire speed
boost is done in the _code optimization_ stage, where you can bulk up integer
operations into one or a few x86 instructions. Another candidate for such
compilation would be the [Forth language][jonesforth].

Also, before you get serious about expanding this JIT-compiler, take a look at
the [PeachPy project][peachpy]. It goes way beyond this and includes a
disassembler and supports seemingly the entire x86-64 instruction set right up
to [AVX][avx.wiki].

As mentioned, there is a good deal of overhad when using ctypes to call into
functions. You can use the `cffi` module to overcome some of this, but the fact
remains that if you want to call very small JIT-ed functions a large number of
times, it's usually faster to just use pure Python.

What other cool uses are there? I've seen some math libraries in Python that
switch to vector operations for higher performance. But I can imagine other fun
things as well. For example, tools to compress and decompress native code,
access virtualization primitives, sign code and so on. I do know that some
[BPF][bpf.wiki] tools and regex modules JIT-compile queries for faster
processing.

What I think is fun about this exercise is to get into deeper territory than
pure assembly. One thing that comes to mind is how different instructions are
disassembled to the same mnemonic. For example, the RETQ instruction has a
different opcode than an ordinary RET, because it operates on 64-bit values.
This is something that may not be important when doing assembly programming,
because it's a detail that may not always matter, but it's worth being aware of
the difference. I saw that gcc, lldb and objdump gave slightly different
disassembly listings of the same code for RETQ and MOVABSQ.

There's another takeaway. I've mentioned that the native Brainfuck compiler I
made didn't initially produce very fast code. I had to optimize to get it fast.
So things won't go fast just because you use AVX, Cuda or whatever. The cold
truth is that gcc contains a vast database of optimizations that you cannot
possibly replicate by hand. Felix von Letiner has a [classic talk about source
code optimization][fefe] that I recommend for more on this.

What about actual compilation?
------------------------------

A few people [commented][hn] that they had expected to see more about the
actual compilation step. Fair point. As it stands, this is indeed a _very_
restricted form of compilation, where we barely do anything with the code at
runtime — we just patch in a constant. I _may_ write a follow-up post that
focuses solely on the compilation stage. Stay tuned!

[amd64.abi]: https://software.intel.com/sites/default/files/article/402129/mpx-linux64-abi.pdf
[avx.wiki]: https://en.wikipedia.org/wiki/Advanced_Vector_Extensions
[bpf.wiki]: https://en.wikipedia.org/wiki/Berkeley_Packet_Filter
[brainfuck.github]: https://github.com/cslarsen/brainfuck-jit
[brainfuck.wiki]: https://en.wikipedia.org/wiki/Brainfuck
[capstone]: http://www.capstone-engine.org/lang_python.html
[cffi.github]: https://github.com/cffi/cffi
[cps]: https://en.wikipedia.org/wiki/Continuation-passing_style
[ctypes.doc]: https://docs.python.org/3/library/ctypes.html#module-ctypes
[deadbeef]: https://en.wikipedia.org/wiki/Magic_number_(programming)
[fefe]: http://www.fefe.de/source-code-optimization.pdf
[follow.up]: /post/python-compiler/
[forth.wiki]: https://en.wikipedia.org/wiki/Forth_(programming_language)
[github]: https://github.com/cslarsen/minijit
[hn.front]: https://news.ycombinator.com/front?day=2017-11-09
[hn]: https://news.ycombinator.com/item?id=15665581
[jit.wiki]: https://en.wikipedia.org/wiki/Just-in-time_compilation
[jonesforth]: https://github.com/nornagon/jonesforth/blob/master/jonesforth.S
[machine.code.wiki]: https://en.wikipedia.org/wiki/Machine_code
[mmap.man]: http://man7.org/linux/man-pages/man2/mmap.2.html
[mprotect.man]: http://man7.org/linux/man-pages/man2/mprotect.2.html
[munmap.man]: http://man7.org/linux/man-pages/man3/munmap.3p.html
[peachpy]: https://github.com/Maratyszcza/PeachPy
[speakerdeck]: https://speakerdeck.com/csl/how-to-make-a-simple-virtual-machine
[specialization.wiki]: https://en.wikipedia.org/wiki/Run-time_algorithm_specialisation
[strerror.man]: http://man7.org/linux/man-pages/man3/strerror.3.html
[sysconf.man]: http://man7.org/linux/man-pages/man3/sysconf.3.html
[wx.wiki]: https://en.wikipedia.org/wiki/W%5EX
]]></content:encoded>
      <dc:date>2017-11-08T21:03:00+00:00</dc:date>
    </item>
    <item>
      <title>Embedding binary data in executables</title>
      <link>https://csl.name/post/embedding-binary-data/</link>
      <description><![CDATA[Applications usually load resources like images from disk. But, in some
situations, it may be better to embed binary data right into the executable
file. Here are a few ways to do that.
]]></description>
      <pubDate>Tue, 07 Jun 2016 23:45:40 +0000</pubDate>
      <guid>https://csl.name/post/embedding-binary-data/</guid>
      <content:encoded><![CDATA[Applications usually load resources like images from disk. But, in some
situations, it may be better to embed binary data right into the executable
file. Here are a few ways to do that.

In language like C and C++, the straight-forward way to include binary data in
the executable is to convert it to a character array:

    const char data[] = {0x00, 0x01, ...};

While there's nothing wrong with that approach, it requires a tool to convert
binary data to code (for example, `xxd -i`). Besides, I find it a bit
inelegant, and I'll present some alternatives for you.

Using the GNU linker
--------------------

This is by far the easiest solution — but, unfortunately, doesn't work on Mac
OS X. It does for Linux, though!

Let's say you have an image `cat.png` and want to embed it into your
application. You can create an object file with

    $ ld -r -b binary cat.png -o cat.o

The object file will have three symbols in it,

    $ nm cat.o
    _cat_start
    _cat_end
    _cat_size

(That's not actual output, but gives you an idea on how to see the symbols. You
can also use `objdump -x cat.o`).

To use them from C, declare some extern variables

    extern const char cat_start;
    extern const char cat_end;
    extern const int cat_size;

and add `cat.o` to the compiler:

    $ gcc cat.o program.c -oprogram

If you have a function `display_png_image`, you can simply call

    display_png_image(&cat_start);

Using assembly
--------------

If you don't have a working GNU `ld` — or, if you want utter and complete
control — you can use an assembler like <a href="http://www.nasm.us">nasm</a>.

Just use the `incbin` directive to include the data. Add some helper symbols
for the end of the data, and use a macro to calculate the byte length:

    bits 64

    section .rodata

    global _cat_start
    global _cat_end
    global _cat_size

    _cat_start:   incbin "cat.png"
    _cat_end:
    _cat_size:    dd $-_cat_start

On OS X, compile with

    $ nasm -fmacho64 cat.asm -o cat.o

Finally, link your program exactly as before:

    $ gcc cat.o program.c -o program

So what's so good about this approach? It lets you put the binary data in the
read-only data section, meaning it will be truly read-only. If you used `ld`
instead, you'd have to use `objcopy` to move the symbols to the `.rodata`
section.

Using `objcopy`
---------------

You can also use `objcopy` from the GNU binutils package. However, I wasn't
able to get it working completely on OS X. Meaning, there isn't an easy,
cross-platform way to use it, so I won't write much about it.

But, you *can* start by doing something like

    $ objcopy -I binary -O mach-o-x86_64 \
      --rename-section .data=.const [...] cat.png cat.o

On my system, the linker didn't like the resulting object file. I'll post an
update if I get it working. On Linux, it should be straight-forward.

What about other languages?
---------------------------

Most statically compiled languages will let you link in object files. I haven't
tried, but I guess you could easily do it in languages like Swift, Rust and so
on.

What would you use it for?
--------------------------

Of course, you can embed stuff like images and music into your application. For
desktop applications, this may be an advantage in certain situations. But there
are other cool uses as well.

Mike Pall, the original author of <a href="http://luajit.org">LuaJIT</a>, gives
an example where he <a
href="http://stackoverflow.com/a/11318414/21028">compiles Lua programs to
object files and wrap them up in an archive file</a>. He then proceeds to link
a host program in C with them, so the scripts can be executed without loading
anything from disk.

I'm sure there are many other use cases as well.
]]></content:encoded>
      <dc:date>2016-06-07T23:45:40+00:00</dc:date>
    </item>
    <item>
      <title>Sending raw Ethernet frames from Python</title>
      <link>https://csl.name/post/raw-ethernet-frames/</link>
      <description><![CDATA[Sometimes you just need to send raw Ethernet frames directly on the network.
Seriously, it has its uses: Perhaps you&#39;re constructing your own non-IP network
for research or performance, you need to interface with some weird device on
the local network, or you&#39;re just playing around for fun.  I used it
extensively while researching use of OpenFlow with Mininet.
]]></description>
      <pubDate>Mon, 06 Jun 2016 22:30:30 +0000</pubDate>
      <guid>https://csl.name/post/raw-ethernet-frames/</guid>
      <content:encoded><![CDATA[Sometimes you just *need* to send raw Ethernet frames directly on the network.
Seriously, it has its uses: Perhaps you're constructing your own non-IP network
for research or performance, you need to interface with some weird device on
the local network, or you're just playing around for fun.  I used it
extensively while researching use of <a
href="https://en.wikipedia.org/wiki/OpenFlow">OpenFlow</a> with <a
href="http://mininet.org">Mininet</a>.

Anyway, here's how to do it from Python. You most likely need to run the
program as superuser, or else you won't be able to inject Ethernet frames on
the network interface card.

There really isn't anything to it. Just create a raw socket and paste the
bytes. The only thing to note is the argument to `bind`.

    from socket import *

    def send_ether(src, dst, type, payload, interface="eth0"):
      # 48-bit Ethernet addresses
      assert(len(src) == len(dst) == 6)

      # 16-bit Ethernet type
      assert(len(type) == 2) # 16-bit Ethernet type

      s = socket(AF_PACKET, SOCK_RAW)
      s.bind((interface, 0))
      return s.send(dst + src + type + payload)

Here's a full example using IPv4 + ICMP PING REQ payload:

    """Demonstrates how to construct and send raw Ethernet packets on the
    network.

    You probably need root privs to be able to bind to the network interface,
    e.g.:

        $ sudo python sendeth.py
    """

    from socket import *

    def sendeth(ethernet_packet, payload, interface = "eth0"):
      """Send raw Ethernet packet on interface."""
      s = socket(AF_PACKET, SOCK_RAW)

      # From the docs: "For raw packet
      # sockets the address is a tuple (ifname, proto [,pkttype [,hatype]])"
      s.bind((interface, 0))
      return s.send(ethernet_packet + payload)

    def pack(byte_sequence):
      """Convert list of bytes to byte string."""
      return b"".join(map(chr, byte_sequence))

    if __name__ == "__main__":
          # Note that this example contains HARDCODED packets, meaning that
          # it will ONLY work on the system it was designed for.

          # I got these values by sending a ping while running Wireshark.
          # You can do so yourself.  Another way to construct these manually is to use
          # the impacket library (sudo pip install impacket)

          # src=fe:ed:fa:ce:be:ef, dst=52:54:00:12:35:02, type=0x0800 (IP)
          ethernet_packet = [0x52, 0x54, 0x00, 0x12, 0x35, 0x02, 0xfe, 0xed, 0xfa,
                             0xce, 0xbe, 0xef, 0x08, 0x00]

          # src=10.0.2.15, dst=195.88.54.16 (vg.no), checksum, etc.
          ipv4_header = [0x45, 0x00, 0x00, 0x54, 0x05, 0x9f, 0x40, 0x00, 0x40, 0x01,
                         0x2f, 0x93, 0x0a, 0x00, 0x02, 0x0f, 0xc3, 0x58, 0x36, 0x10]

          # echo (ping) request, checksum 2b45, etc
          icmp_ping = [0x08, 0x00, 0x2b, 0x45, 0x11, 0x22, 0x00, 0x02, 0xa9, 0xf4, 0x5c,
                       0x53, 0x00, 0x00, 0x00, 0x00, 0xf5, 0x7b, 0x01, 0x00, 0x00, 0x00,
                       0x00, 0x00, 0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17, 0x18,
                       0x19, 0x1a, 0x1b, 0x1c, 0x1d, 0x1e, 0x1f, 0x20, 0x21, 0x22, 0x23,
                       0x24, 0x25, 0x26, 0x27, 0x28, 0x29, 0x2a, 0x2b, 0x2c, 0x2d, 0x2e,
                       0x2f, 0x30, 0x31, 0x32, 0x33, 0x34, 0x35, 0x36, 0x37]

          payload = "".join(map(chr, ipv4_header + icmp_ping))

          # Construct Ethernet packet with an IPv4 ICMP PING request as payload
          r = sendeth(pack(ethernet_packet),
                      pack(ipv4_header + icmp_ping))
          print("Sent Ethernet w/IPv4 ICMP PING payload of length %d bytes" % r)
]]></content:encoded>
      <dc:date>2016-06-06T22:30:30+00:00</dc:date>
    </item>
    <item>
      <title>How GCC fixes bad hand-optimizations</title>
      <link>https://csl.name/post/gcc-optimization-fix/</link>
      <description><![CDATA[The GCC and LLVM optimizers contain troves of arcane and
esoteric tricks to speed up code on different systems. Rather surprisingly,
GCC will even correct bad hand-optimizations!
]]></description>
      <pubDate>Sat, 04 Jun 2016 23:00:24 +0000</pubDate>
      <guid>https://csl.name/post/gcc-optimization-fix/</guid>
      <content:encoded><![CDATA[The GCC and LLVM optimizers contain <a
href="http://www.fefe.de/source-code-optimization.pdf">troves of arcane and
esoteric tricks</a> to speed up code on different systems. Rather surprisingly,
GCC will even correct bad hand-optimizations!

One such example is using shifts instead of multiplication.
In the olden days, this was a reliable way to speed up your
code, especially for game and demo programming. It's mentioned in <a
href="http://www.fefe.de/source-code-optimization.pdf">Hacker's Delight</a>,
which I highly recommend for bit fiddlers.

To draw a single pixel at given x and y positions, you must calculate the
screen offset:

    size_t offset = (x + y*width) * components_size;

Here, `components_size` is the size of each pixel. For 8-bit RGBA values, it
will be four bytes. In the nineties, graphics modes used color palettes, so
the component would be a one-byte index. Games and demos often used <a
href="https://en.wikipedia.org/wiki/Mode_13h">mode 13h</a>, which is 320 by 200
pixels. That gives

    size_t offset = x + y*320;

However, multiplication was quite expensive in those days, and with many
individual pixels being drawn (as opposed to image blocks, or scanning the
screen buffer a pixel at a time), an optimization would be to replace it with
instructions taking less cycles. Noticing that 320 = 2<sup>6</sup> +
2<sup>8</sup>, the expression can be transformed to

    size_t offset = x + (y << 6) + (y << 8);

On a CPU like the Intel 386, that would take up less cycles in total.

This was a tried-and-true technique, part of every graphics programmer's bag of
tricks.  I typed it out by reflex <a
href="https://news.ycombinator.com/item?id=4083414">for years</a>. 
However, on modern CPUs, such code is no longer optimal. Let's see the assembly
code for the function

    unsigned offset(unsigned x, unsigned y)
    {
      return x + (y << 6) + (y << 8);
    }

Using LLVM, we can compile the code using native 64-bit instructions with no
optimizations:

    $ llvm-gcc -m64 -march=native -mtune=native -O0 -S foo.c

The below code is from an Intel i7 on OS X.  First it loads `x` and `y` into
`esi` and `edi`, respectively, from the stack frame:

    movl    -4(%rbp), %esi
    movl    -8(%rbp), %edi

It then performs `x += (y << 6)`

    shll    $6, %edi
    addl    %edi, %esi

and then `x += (y << 8)`

    movl    -8(%rbp), %edi
    shll    $8, %edi
    addl    %edi, %esi

But turning on optimizations,

    $ llvm-gcc -m64 -march=native -mtune=native -O3 -S foo.c

it will change slightly to

    leal    (%rsi,%rsi,4), %eax
    shll    $6, %eax
    addl    %edi, %eax

While it still performs adds and shifts, it uses less cycles. We've excluded
the function prologue and epilogue. In pseudo-code, it does this:

    eax = y + y*4
    eax = eax << 6
    eax = eax + x

or

    return x + ((y+y*4) <<6 )

LLVM will do compile to the *exact* same code with this version

    unsigned offset(unsigned x, unsigned y)
    {
      return x + y*320;
    }

and 32-bit targets are structurally equivalent.  But what about **GCC**? It
goes even further!

Compiling the original shift-and-add code with

    $ gcc-5 -mtune=native -march=native -m64 -Ofast -S foo.c

produces

    imull $320, %esi, %eax
    addl  %edi, %eax
    ret

It's simply `return x + y*320`. But is it faster? Yes, it is!

Testing performance
--------------------

When it comes to performance testing, there's nothing like actually running
real code. So let's put the functions to the test.

We'll use <a href="http://www.nasm.us">NASM</a> to assemble the two versions
above, and GCC 5 to compile a test driver. We'll turn off optimizations where
needed.

The assembly code for the imul and shift + add functions are placed in
`offset.asm`. I'm on OSX right now, so we'll need to use the correct alignment,
otherwise the linker won't be able to find the functions. The file looks like
this:

    ; OSX 64-bit, requires special alignment
    bits 64
    align 16

    ; For the symbol table
    global _offset_imul
    global _offset_shift_add

    section .text

    ; Takes x and y, returns x + y*320
    _offset_imul:
      imul eax, esi, 320
      add eax, edi
      ret

    ; The same, but with shifts and adds
    _offset_shift_add:
      lea eax, [rsi, rsi*4]
      shl eax, 6
      add eax, edi
      ret

Compile this with

    $ nasm -fmacho64 offset.asm -ooffset.o

The test driver program, called `test-offset.c`, uses <a
href="https://developer.apple.com/library/ios/documentation/System/Conceptual/ManPages_iPhoneOS/man2/getrusage.2.html">getrusage</a>
to find the CPU time spent by each function. To compare the performance of each
function, we'll run one billion iterations of them each, then keep the *best*
time so far. That's the approach Facebook uses in their profiling code in <a
href="https://github.com/facebook/folly/blob/master/folly/Benchmark.cpp#L171">Folly</a> (apparently they went back to this after testing some statistical modelling).
For code like this, we don't want to measure nonsense things like average
running time and so on: The best run gives the most correct measurement of how
well the function performs.

    #include <assert.h>
    #include <stdio.h>
    #include <sys/resource.h>

    static unsigned correct_result = 0;
    typedef unsigned (*func_t)(unsigned, unsigned);

    /* Our assembly functions */
    extern unsigned __attribute__((optimize("O0")))
      offset_imul(unsigned x, unsigned y);
    extern unsigned __attribute__((optimize("O0")))
      offset_shift_add(unsigned x, unsigned y);

    /* To make sure that the functions actually work */
    static unsigned correct(const unsigned x, const unsigned y)
    {
      return x + y*320;
    }

    static double rusage()
    {
      struct rusage ru;
      getrusage(RUSAGE_SELF, &ru);
      return ru.ru_utime.tv_sec + ru.ru_utime.tv_usec / 1000000.0;
    }

    /* We'll disable optimizations for this one */
    static unsigned __attribute__((optimize("O0")))
      calc(const size_t its, func_t func)
    {
      unsigned check = 0xaaaaaaaa;

      for ( size_t n=0; n<its; ++n )
        check ^= func(n, n+1);

      return check;
    }

    static void
      __attribute__((optimize("O0")))
      best(const size_t its, double* best, func_t func)
    {
      const double start = rusage();
      const unsigned result = calc(its, func);
      assert(result == correct_result);
      const double secs = rusage() - start;

      if ( secs < *best )
        *best = secs;
    }

    int main()
    {
      double tc=9999, ti=9999, ts=9999;

      const size_t its = 1000000000;
      correct_result = calc(its, correct);

      /* Loop forever and print the running best results */
      for (;;) {
        best(its, &tc, correct);
        best(its, &ts, offset_shift_add);
        best(its, &ti, offset_imul);
        printf("correct %fs, shift+add %fs, imul %fs\n", tc, ts, ti);
      }
    }

Compile this with

    $ gcc-5 -W -Wall -Ofast offset.o test-offset.c -otest-offset

Results
-------

We're finally ready to party (according to some definition of "party"):

    $ ./test-offset
    correct 3.448534s, shift+add 3.301471s, imul 3.164973s
    correct 3.304378s, shift+add 3.281795s, imul 3.161455s
    correct 3.285387s, shift+add 3.254211s, imul 3.161455s
    ...
    correct 3.267140s, shift+add 3.254211s, imul 3.161455s

Clearly, the `imul` function is the fastest. But only by a small amount.
However, the point here was to show how GCC will fix bad hand-optimizations.
And clearly, it did that correctly!

What's interesting is that GCC 5 obviously didn't optimize the `correct`
function as well as either of `offset_imul` of `offset_shift_add` in this
program. You can take a look at the disassembly by doing `objdump -d
test-offset` and see for yourself. 

Obviously, the first mistake for the
`correct` function is that it creates a stack frame. If you recompile with
`-fomit-frame-pointer`, GCC quickly rectifies that, and we get the shift + add
version (but *not* the imul one!):

    0000000100000c00 <_correct>:
       100000c00: 8d 04 b6              lea    (%rsi,%rsi,4),%eax
       100000c03: c1 e0 06              shl    $0x6,%eax
       100000c06: 01 f8                 add    %edi,%eax
       100000c08: c3                    retq
       100000c09: 0f 1f 80 00 00 00 00  nopl   0x0(%rax)

Rerunning the tests, I got these results:

    correct 3.357759s, shift+add 3.311668s, imul 3.159475s
    correct 3.228023s, shift+add 3.311668s, imul 3.159475s
    ...
    correct 3.228023s, shift+add 3.244624s, imul 3.159475s

Note that `correct` and `shift+add` should actually get the exact same timings,
but they don't! That's most likely because of <a
href="https://en.wikipedia.org/wiki/Preemption_(computing)">preemption
noise</a> by the OS.  I didn't bother trying to run this in single user mode,
with `nice -10` and so on. If you try this out yourself, particularly if you're
on another OS, let me know your results in the comments!

Another question is why LLVM doesn't perform the same optimization. GCC is
supposedly somewhat faster than LLVM, but I haven't checked if this is still
true. Also, what we haven't looked at is any side-effects on the code. I'm
thinking about the cascading CPU pipeline and stuff like that. I don't expect
to see anything interesting here, though, because we're only accesing values on
the stack, and so on. Let me know in the comments if you have two cents to
spare!

I also tried adding `-m64` `-march=native` `-mtune=native` and the results I
got then was

    correct 3.404448s, shift+add 3.271211s, imul 3.217215s
    correct 3.269317s, shift+add 3.243117s, imul 3.173220s
    correct 3.269317s, shift+add 3.243117s, imul 3.123939s
    ...

Each run is a little bit different, though.

Versions used
-------------

    $ gcc-5 --version
    gcc-5 (Homebrew gcc 5.3.0 --without-multilib --with-jit) 5.3.0
    Copyright (C) 2015 Free Software Foundation, Inc.
    This is free software; see the source for copying conditions.  There is NO
    warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

    $ llvm-gcc --version
    Apple LLVM version 7.0.2 (clang-700.1.81)
    Target: x86_64-apple-darwin15.5.0
    Thread model: posix

    $ nasm -v
    NASM version 2.12.01 compiled on Mar 23 2016
]]></content:encoded>
      <dc:date>2016-06-04T23:00:24+00:00</dc:date>
    </item>
    <item>
      <title>Writing a shared C++ library and loading it in LuaJIT</title>
      <link>https://csl.name/post/luajit-cpp/</link>
      <description><![CDATA[This tutorial shows how to create a shared library in C and C++, and how to
load it in LuaJIT using its foreign-function interface (FFI).
The code here is available on GitHub.
]]></description>
      <pubDate>Sun, 15 May 2016 05:25:47 +0000</pubDate>
      <guid>https://csl.name/post/luajit-cpp/</guid>
      <content:encoded><![CDATA[This tutorial shows how to create a shared library in C and C++, and how to
load it in <a href="http://luajit.org">LuaJIT</a> using its foreign-function interface (FFI).
The code here is <a href="https://github.com/cslarsen/luajit-cpp">available on GitHub</a>.

There are obvious upsides to using LuaJIT: It's considerably faster than plain
Lua — up to <a href="http://luajit.org/performance.html">a hundred times
faster</a> —  and it comes with a really nice way of loading shared libraries
through its FFI library.

First, let's start off by creating a <a
href="http://www.oracle.com/technetwork/articles/servers-storage-dev/mixingcandcpluspluscode-305840.html">hybrid
C and C++ shared library</a>. That means you can either use a C or C++ compiler
to build it.

Put the following in a file called `first.cpp` or `first.c`.

    #ifdef __cplusplus
    extern "C"
    #endif
    int add(int a, int b)
    {
      return a + b;
    }

To compile it, pass `-fPIC` to generate position independent code, `-shared` to
produce a shared library and set the output file to `libfirst.so` (or
`libfirst.dylib  for OS X).

    $ g++ -W -Wall -g -fPIC -shared -o libfirst.so first.cpp

The C version of the program will be

    $ gcc -W -Wall -g -fPIC -shared -o libfirst.so first.c

Now that we've made `libfoo.so`, we can inspect it.

    $ file libfirst.so
    libfirst.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV),
    dynamically linked, not stripped

We can list its symbols

    $ nm --defined-only libfirst.so | grep add
    000000000000052a T add

If we remove the debugging symbols for `libfirst.so`, we can't use `nm`, but we
can use `objdump`.

    $ strip libfirst.so
    $ objdump -T libfirst.so

    libfirst.so:     file format elf64-x86-64

    DYNAMIC SYMBOL TABLE:
    00000000000004b8 l    d  .init  0000000000000000              .init
    ...
    00000000000005ba g    DF .text  0000000000000015  Base        add
    ...

We also know the signature of the function `add`, namely

    int add(int, int)

That's really all we need to load a shared library using LuaJIT. Let's try it
out in the REPL first.

    $ luajit
    > ffi = require("ffi")
    > first = ffi.load("libfirst.so")

We not have to lef `ffi` know input and output arguments for the function
`add`, by simply passing the function signature to `ffi.cdef`.

    > ffi.cdef("int add(int, int);")

Now we can call `add`:

    > io.write(first.add(11, 22) .. "\n")
    33

That's how easy it is. For Python users, there is the `cffi` module that comes
with `pypy`, which offers similar functionality (unlike `ctypes`, which doesn't
parse C signatures automatically). Many other languages have it too. Chicken
Scheme also parses C code. The difference is in how advanced the C parsers are.

But why would you interface with C? I think it's very obvious. Most interesting
platform specific functions will be available in C. Having a good FFI library
means you can easily access these in LuaJIT. It's also easy to interface with
libraries such as <a href="http://www.libsdl.org">SDL</a>.

Using C++ objects in LuaJIT
---------------------------

The next step is to use C++ objects. There are many ways of doing this, so I'll
focus on one. We'll create a C++ class in a file `foo.cpp`, then we'll expose
it through a C interface that we'll use from LuaJIT. Finally, we'll wrap this
interface back into an object-like one in LuaJIT.

Below is `foo.cpp`.

    class Person {
    public:
      Person(const std::string& name_,
             const int age_):
        name(name_),
        age(age_)
      {
      }

      const std::string name;
      const int age;
    };

The C interface will look like this:

    extern "C" void* new_person(const char* name, int age)
    {
      assert(name != NULL);
      Person *p = new Person(name, age);
      return reinterpret_cast<void*>(p);
    }

    extern "C" void delete_person(Person* p)
    {
      delete p;
    }

    extern "C" int age(const Person* p)
    {
      assert(p != NULL);
      return p->age;
    }

    extern "C" char* name(const Person* p)
    {
      assert(p != NULL);
      return strdup(p->name.c_str());
    }

Note that the `strdup` function in `name` is not guaranteed to exist on your
system. You can roll your own with something like

    static char* strdup(const char* in)
    {
      assert(s != NULL);
      char *out = (char*) malloc(strlen(in)+1);
      return strcpy(out, in);
    }

We need to tell LuaJIT's FFI library what the function signatures are.

    ffi.cdef[[
      /* From our library */
      typedef struct Person Person;
      Person* new_person(const char* name, const int age);
      char* name(const Person* p);
      int age(const Person* p);

      /* From the C library */
      void free(void*);
    ]]

We'll wrap this up in an <a
href="http://lua-users.org/lists/lua-l/2011-07/msg00496.html">object-like
structure using a trick</a>.

    local PersonWrapper = {}
    PersonWrapper.__index = PersonWrapper

    local function Person(...)
      local self = {super = foo.new_person(...)}
      ffi.gc(self.super, foo.delete_person)
      return setmetatable(self, PersonWrapper)
    end

Notice that we pass the pointer to `ffi.gc`, which makes sure to call
`foo.delete_person` on the pointer reclaiming it.

For the `name` function, we get a newly allocated string using `malloc`, so we
need to use `C.free` to remove it from the heap again. To load the C library,

    C = ffi.C

We can now implement the wrappers for `name` and `age`.

    function PersonWrapper.name(self)
      local name = foo.name(self.super)
      ffi.gc(name, C.free)
      return ffi.string(name)
    end

    function PersonWrapper.age(self)
      return foo.age(self.super)
    end

Finally, let's try it out:

    local ffi = ffi.require("ffi")
    local foo = ffi.load("libfoo.so")

    -- Insert the above code here

    local person = Person("Mark Twain", 74)
    io.write(string.format("'%s' is %d years old\n",
                           person:name(),
                           person:age()))

Running it produces

    $ luajit foo.lua
    'Mark Twain' is 74 years old

]]></content:encoded>
      <dc:date>2016-05-15T05:25:47+00:00</dc:date>
    </item>
    <item>
      <title>Lambdas, macros and continuations in Scheme — a tutorial</title>
      <link>https://csl.name/post/lambda-macros-continuations/</link>
      <description><![CDATA[
I gave an introductory talk
on R7RS Scheme in 2013 that included some neat examples using
closures, continuations and macros. This article expands on that, intended for
anyone curious about those concepts.

]]></description>
      <pubDate>Sat, 07 May 2016 07:30:18 +0000</pubDate>
      <guid>https://csl.name/post/lambda-macros-continuations/</guid>
      <content:encoded><![CDATA[<p class="lead">
I gave an <a href="https://speakerdeck.com/csl/r7rs-scheme">introductory talk
on R<sup>7</sup>RS Scheme</a> in 2013 that included some neat examples using
closures, continuations and macros. This article expands on that, intended for
anyone curious about those concepts.
</p>

If your programming language supports continuations, you can implement
[any control flow construct](controlflow) in native source code form. That
means you can import things like

    (import (goto)
            (exceptions)
            (continuable-exceptions)
            (coroutines)
            (cooperative-multithreading)
            (nondeterminism))

I'll give simple examples on how all of the above can be implemented using
closures, continuations and macros, using plain <a
href="http://trac.sacrideo.us/wg/wiki/R7RSHomePage">R<sup>7</sup>RS Scheme</a>.

**NOTE:** This post is a work in progress that I'm posting early. The later
examples doesn't include explanations, and some are outright missing. Also, if
I get anything wrong, please let me know in the comments at the bottom.

You can run the examples using [Chibi Scheme](Chibi Scheme).

Creating your first Scheme library
==================================

To get started, we need a function `println` that simply prints all of its
arguments. We'll create a variadic function that simply calls `display` on all
its arguments, and put it in a library called `print`.

{% highlight scheme %}
{% include scheme/goto/print.sld %}
{% endhighlight %}

Here's a test program

{% highlight scheme %}
{% include scheme/goto/print-example.scm %}
{% endhighlight %}

Running it produces

{% highlight bash %}
{% include scheme/goto/print-example.out %}
{% endhighlight %}

What are continuations?
=======================

There are many ways to explain what continuations are, but I will offer two
mental models that are very useful, with minor trade-offs in exactness.

A continuation represents the rest of the computation. A good way to think
about this is that whenever you capture a continuation, you essentially set a
label in the code that you can later jump back to. So it's almost like a GOTO
or JUMP instruction, with the additional benefit of being able to pass along
new values that will replace the value at the location where it jumps to.
Moreover, when you jump, the code will continue running from that location,
returning to the correct functions that were active when you first captured the
continuation.

So, if you have a function that computes some mathematical expression, you can
surround one of the variables with such a label. Later on, you can back jump to
this label, but with a new value for that variable.

Here's an example in pseudo-code that, unintentionally, looks like JavaScript:

    label = null;

    function print_person(name, age) {
      print("%s is %d years old.", name, set_label(age));
    }

    // At this point, label is still null and cannot be jumped to. The first
    // time we call print_person, set_label will be called, giving label some
    // imaginary value.

    print_person("John Doe", 123); // prints "John Doe is 123 years old."

    // Now we can jump to the label and pass it the value of 500

    goto_label(500); // prints "John Doe is 500 years old."

The function `set_label` will insert the current code location into the
variable `label` and return its input argument `age` immediately. After that,
`goto_label` will jump back to the location pointed to by `label` and continue
execution there, but now with the value of 500.

Running the program should print

    John Doe is 123 years old.
    John Doe is 500 years old.

Notice that after jumping to the label location, the program will continue
running at the `set_label(age)` location. At this point in time, the value of
`500` will be returned to the `print` function, which proceeds to print its
arguments. Finally, `print_person` itself has to return, and the return
location this time around will be right after the `goto_label(500)` statement.

Implementing goto
=================

If we translate the previous example to Scheme, we can implement `set_label`
and `goto_label`.

    (import (scheme base)
            (scheme write))

    ; A handy function that prints all of its arguments
    (define (println . args)
      (for-each display args)
      (newline))

    ; ... insert set-label and goto-label here

    (define label #f)

    (define (print-person name age)
      (println name " is " (set-label age) " years old."))

    (print-person "John Doe" 123)
    (goto-label 500)

To implement the final two functions, we'll use `call/cc`. It has a special
form:

    (call/cc
      (lambda (continuation)
        <body>))

Your code goes into `<body>`, and from there we have access to a variable
called `continuation`. It's actually a special function that lets you exit from
the surrounding `(call/cc ...)`-block with a return value. If you do
`(continuation 123)`, the program will jump back to the location of the
original `call/cc` and continue running from there as if `call/cc` returned
`123`. And you can do that as many times as you want.

For example,

    (define set-number-and-run-println-again #f)

    ; When this runs the first time, 3 will be printed.
    (println (+ 1
                (call/cc (lambda (k)
                  (set! set-number-and-run-println-again k)
                  2))))

    (set-number-and-run-println-again 100)
    ; The above will run from the call/cc location, but with the value 100.
    ; That value will go into (+ 1 100), which passes 101 to println, which
    ; prints it.

Now, the definition of `set-label` uses the above form:

    (define (set-label initial-return-value)
      (call/cc
        (lambda (continuation)
          (set! label continuation)
          initial-return-value)))

The code above will (1) capture the current continuation and put it inside the
variable `continuation`, (2) store the continuation in the global variable
`label`and (3) return the value `initial-return-value`.

Remember that our goal is to be able to *jump back* to where the `age` variable
is used, but with new values.

Now, since `label` will eventually contain a continuation, the implementation
of `goto-label` is straight-forward:

    (define (goto-label new-value)
      (label new-value))

The final program can be run in [Chibi Scheme](chibi). Put the following in a
file called `print-person.scm`:

{% highlight scheme %}
{% include scheme/goto/print-person.scm %}
{% endhighlight %}

Executing it gives:

{% highlight shell %}
{% include scheme/goto/print-person.out %}
{% endhighlight %}

Now I'll suggest a few mental models that can be used to understand how
continuations work. After that, we'll revisit this example, but wrap everything
up in a neat little library, so that we can write

    (import (goto))

in our Scheme programs.

Continuations as call stack manipulations
=========================================

Knowing a little bit about implementation strategies for continuations is of
great help in understanding them.

Many Schemes use [continuation passing style](cps), but I think it's much
easier to think of it in terms of copying and reinstating the call stack.

Recall that a [call stack](callstack) is a collection of stack frames. Each
frame contains a return address, function arguments and perhaps also a
placeholder for a return value.

In the program above, imagine that the program is a tree. At the root node, the
program first has a branch out to the definition of println, whose internal
definitions branch out from there. Next, it has a branch for the definition for
`make-label`, and so on:

    root
    |
    +-- import -- ...
    |
    +-- define label -- ...
    |
    +-- define print-person -- ...
    |
    +-- print-person -- ...
    |
    +-- goto-with-value -- ...

    ^
    The top-level

The vertical line going from root and downwards — the trunk, so to speak —
is what we call the *top-level* in Scheme.

Now, what `call/cc` does, in some implementations, is to take a copy of the
entire call stack from the top-level up to the current position.

So we copy a chunk of the call stack and put it in memory. This is what we mean
when we say we "take a continuation".

When we want to invoke a continuation, we want to continue running at the same
location that the continuation was taken. To do this, we just reinstate the
call stack: Chop off the current call stack from the top-level up to the
current position, and replace it with the one we have stored. Finally, we have
a value to pass on, so we place that in the correct location at the very top of
the stack where there is a place for the return-value. Then we continue running
from there.

I'm doing some hand-waving here, of course. There are many other details that
you'd have to do, but the point is to understand the general strategy.

Creating a "goto" library using macros
======================================

Recall the goto example given earlier. It used a global variable `label` to
hold the continuation. This is a problem if we want to make a general
goto-library, because it means you can only ever jump to *one* location.

So we should take an extra argument to `set-label` and `goto-label`. But Scheme
passes arguments by value and cannot — in general — mutate their "outer"
values: A function taking an argument can only modify it locally.

There are two ways to solve this: With or without macros.

Without macros, we could use label *names* instead of *variable*, but that
requires a bit of house-keeping code. Let's use macros instead. We're going to
need them for more advanced examples anyway.

A *macro* will expand its body and embed it at the location in the code where
it's used. Think of it as a copy-and-paste operation that substitutes
variables. The general form is:

    (define-syntax <name>
      (syntax-rules ()
        ((<usage pattern 1> <expands to this>)
         (<usage pattern 2> <expands to this>
         ... and so on))))

The `syntax-rules` tells which macro system to use. Yes, there are several
available. The one we use is pattern-based. The next empty `()` is a list of
literals we *don't* want to substitute. In our case it's empty.

As a simple example, consider the `unless` function, which will execute some
code if its first boolean value is false:

    (define (unless test value)
      (if (not test) value))

Or in pseudo-code:

    function unless(test, value) {
      if ( !test )
        return value;
      else
        return null;
    }

The above definitions have a serious problem. Because they take *values*, which
means that they will always be evaluated. Imagine a `wipe-root` function that
deletes everything on your drive and returns the number of important files you
lost. What would happen below?

    (unless #t (wipe-root))

Well, it will actually execute `(wipe-root)` *before* passing on its return
value to `unless`. To work around that, we could wrap `wipe-root` in a closure:

    (unless #t (lambda () (wipe-root)))

or, equivalently

    unless(false, function() {
      wipe_root();
    });

But that doesn't look anything like the ordinary if-statement. Another solution
would be to take a function by value, `(unless #f wipe-root)`, but that
wouldn't be very useful.

Nay, the elegant solution is to use macros to *control evaluation*:

    (import (scheme base)
            (scheme write))

    (define-syntax unless
      (syntax-rules ()
        ((unless test code)
          (if (not test) code))))

    (define (wipe-root)
      (display "Wiping root ..."))

    (unless #t (wipe-root))

Running the above code in [Chibi](chibi) will not call `wipe-root`, and
therefore not print anything.

Note that some people dislike macros, because it can obscure the exact
behaviour of your program. For example, things that look like functions may
actually be macros, meaning you don't really know when — or if — your
arguments are evaluated, and that makes it hard to reason about your program.
I think they're great if used with care.

What we want is to be able to write

    (import (scheme base)
            (scheme write)
            (goto))

    (let ((label #f))
      (make-label label)
      (write "Nyan ")
      (goto label))

and with value-passing:

{% highlight scheme %}
{% include scheme/goto/goto-example.scm %}
{% endhighlight %}

We'll use the same strategy as before, except that `set-label` and `goto-label`
can be invoked using a label and value, or only a label.

{% highlight scheme %}
{% include scheme/goto/goto.sld %}
{% endhighlight %}

The `goto-label` function uses `case-lambda`, which patterns matches on its
invocation form. The first line matches calls to `(goto-label <label>)`, while
the second matches `(goto-label` `<label>` `<value>)`.

The `set-label` macro also matches on the same two patterns. Here we use a
single underscore instead of typing out the full name of the macro.

Put that in a file called `goto.sld`, and you should be able to run the above
examples:

{% highlight bash %}
{% include scheme/goto/goto-example.out %}
{% endhighlight %}

Delimited and undelimited continuations
=======================================

Using the call stack model of explanation explained earlier, it means that
taking a continuation copies the entire call stack down to the second-to-last
frame of the top-level. This prevents us from having endless loops whenever we
reinstate a continuation. It's a detail, don't worry.

Dealing with continuations in Scheme is done through the ``call/cc`` form. It
provides [undelimited continuations](undelimited), which — while still
powerful — are not as general as [delimited continuations](delimited).

Now, the big deal about continuations is that you can use them to implement
[any other control flow construct](controlflow), from simple gotos to
exception mechanisms, coroutines, cooperative threads, non-deterministic
programming and so on.

But it turns out that undelimited continuations cannot do this without storing
one additional piece of state. That is also somewhat of a detail, but as you'll
see in the later examples. we always have to keep tabs on different
continuations. The more general continuation systems are *delimited*. They are
truly functional, and do not require explicitly storing continuations in
variables. Many Schemes provide these through constructs such as
`shift`-`reset` or `prompt-abort`. I won't go into those, but the main idea is
that instead of copying the call stack, you can put a marker somewhere and only
copy a *part* of it. That means that your continuations will be *true*
functions.

Anyway, undelimited continuations are unfortunately not part of the official
R<sup>7</sup>RS specification, so I will focus on `call/cc` here.

A simple exception system
=========================

Anyway, let's dive right into the examples. Let's say we want to implement a
simple exception system. We can do that using ``call/cc`` and then users can
have exception handling with a simple import statement of pure Scheme code.

To make our exception library useful, we'll wrap the functionality in macros.

The basic idea is to have a global ``throw`` function that we pass the
continuation on to.

{% highlight scheme %}
{% include scheme/exceptions/try-catch-0.scm %}
{% endhighlight %}

Running the above program,

{% highlight scheme %}
{% include scheme/exceptions/try-catch-0.out %}
{% endhighlight %}

A better try-catch library
--------------------------

    (define-library (try-catch)
      (import (scheme base)
              (print))

      (export
        try)

      (begin
        (define-syntax try
          (syntax-rules (catch)
            ((try 
               (exception-handler handler)
               body ...
               (catch exception-catcher))
             (begin
               (define handler (lambda (error) #f)) ; default: do nothing
               (call/cc
                 (lambda (exit)
                   (set! handler
                     (lambda (error)
                       (exception-catcher error)
                       (exit)))
                   body ...))))))))

Example usage:

    (import (scheme base)
            (print)
            (try-catch))

    (println  "--start--")

    (try
      (exception-handler oops)

      (define (divide a b)
        (if (zero? b)
          (oops "Division by zero")
          (println a "/" b " = " (inexact (/ a b)))))

      (divide 10 2)
      (divide 1 3)
      (divide 3 0)
      (println "This should not execute")

      (catch
        (lambda (error)
          (println "Hey, we caught an error: " error))))

    (println "--end--")

Restartable exceptions
----------------------

When you catch an exception, wouldn't it be cool to fix the error and then have
the program continue as if nothing happened? Here's one way of doing that.

    (define-library (try-restart)
      (import (scheme base)
              (print))

      (export
        try)

      (begin
        (define-syntax try
          (syntax-rules (catch)
            ((try 
               (exception-handler handler)
               (restart-handler the-restart)
               body ...
               (catch exception-catcher))
             (begin
               (define handler (lambda (error) #f)) ; default: do nothing
               (define the-restart #f); default
               (call/cc
                 (lambda (exit) ; exit try-scope

                   (set! handler
                     (lambda (error)
                       (call/cc
                         (lambda (current-restart)
                           (set! the-restart current-restart)
                           (exception-catcher error)
                           (exit))))) ; handler

                   (begin body ...)))))))))

Example usage:

    (import (scheme base)
            (print)
            (try-restart))

    (println  "--start--")

    (try
      (exception-handler oops)
      (restart-handler phew)

      (define (divide a b)
        (let
          ((b (if (not (zero? b))
                b
                (oops
                  (string-append
                    "Division by zero: "
                    (number->string a) "/"
                    (number->string b))))))
          (println a "/" b " = " (inexact (/ a b)))))

      (divide 10 2)
      (divide 3 0)
      (println "Whoa, we recovered from an error!")
      (println "Restartable exceptions are neat!")

      (catch
        (lambda (error)
          (begin
            (println "Hey, we caught an error: " error)
            (println "Restart division with 1 as numerator:")
            (phew 1)))))

    (println "--end--")

Lazy evaluation
===============

Any language with closures can implement lazy evaluation, but if you have a
macro system, you can change the user interface so that it feels like a natural
part of the language.

    (define-library (lazy-evaluation)
      (import (scheme base))

      (export
        delay-computation
        force-computation)

      (begin
        (define-syntax delay-computation
          (syntax-rules ()
            ((_ thunk)
             (list 'delayed (lambda () thunk)))))

        (define (force-computation delayed)
          (if (and (list? delayed)
                   (eq? (car delayed) 'delayed))
            ((cadr delayed))
            (error "Not a delayed computation")))))

Usage example
-------------

    (import (scheme base)
            (scheme eval)
            (print)
            (lazy-evaluation))

    (define (format-harddrive)
      (println "Formatting harddrive, oops!"))

    (define (calc expr)
      (println "The result of " expr " is " (eval expr)))

    (define delayed
      (list
        (delay-computation (format-harddrive))
        (delay-computation (calc '(* 12 12)))
        (delay-computation (calc '(+ 12 12)))))

    (force-computation (list-ref delayed 2))
    (force-computation (list-ref delayed 1))

Implementing a commenting system
================================

I forgot to say this, but macro expansions happens at *compile time*. That's
very important to remember. That means we should be able to provide our own
comment system to Scheme. Our system will allow for nested comments as well, as
in you can comment some code, but add an *uncomment* directive outside of
*that* to make it run again.

Pseudo-code:

    // This section will disappear during compilation
    comment(
      function foo() {
        print("foo()");
      }
    );

If we later on want to enable that part of the code, we can do

    // This part of the code will now work again
    uncomment(
      comment(
        function foo() {
          print("foo()");
          (comment But, the part here will *still* be a comment, because we
                   only uncommented the outer part.);
        }
      ));

The comments library:

    (define-library (comments)
      (import (scheme base))
      (export
        comment
        uncomment)
      (begin
        (define-syntax comment
          (syntax-rules ()
            ((comment body ...)
             (begin))))

        ;; Works because macros are expanded from the outside and in, unlike
        ;; evaluation, which is a depth-first traversal.
        ;;
        ;; Notice we explicitly match (uncomment (comment body ...))
        ;;                                        ^^^^^^^
        (define-syntax uncomment
          (syntax-rules ()
            ((uncomment (comment body ...))
             (begin body ...))))))

Commenting usage example
------------------------

    (import (scheme base)
            (print)
            (comments))

    (comment
      (define (hello)
        (println "The example did NOT work!")))

    (define (hello)
      (println "The example worked fine!"))

    (hello)

Another commenting example
--------------------------

    (import (scheme base)
            (print)
            (comments))

    ;; Notice that macros are expanded from the OUTSIDE IN,
    ;; unlike evaluation which is a depth-first traversal.
    ;;
    ;; That is why this works:
    (uncomment
      (comment
        (define (hello)
          (println "The example worked fine!"))))

    (comment
      (define (hello)
        (println "The example did NOT work!")))

    (hello)

Transforming code into strings
==============================

Imagine you have a unit-testing framework where you test some code. If it
fails, you want to print the expected result, the actual result but also the
source code that gave you the error.

This can be done in languages like C using their `#define`-macros, but it is a
bit limited and will not always let you define local variables and so on.

Here's a simple library that does that in Scheme.

    (define-library (quote-code)
      (import (scheme base)
              (scheme write))
      (export
        run
        code+result
        quote-code)
      (begin

        ;; Print "<code> ==> <result>"
        ;;
        (define-syntax run
          (syntax-rules ()
            ((_ body ...)
             (begin
               (display (code+result body ...))
               (newline)))))


        ;; Returns quoted code and its result
        ;;
        (define-syntax quote-code
          (syntax-rules ()
            ((_ body ...)
             (let
               ((code (quote body ...))
                (result (begin body ...)))
               (values code result)))))

        ;; Return string in form "code ==> result".
        ;;
        (define-syntax code+result
          (syntax-rules ()
            ((_ body ...)
             (call-with-values
               (lambda () (quote-code body ...))
               (lambda (code result)
                 (call-with-port (open-output-string)
                   (lambda (s)
                     (display code s)
                     (display " ==> " s)
                     (display result s)
                     (get-output-string s))))))))))

A generator library (coroutines)
================================

(I *think* I wrote this. I'll find out and give credit where due.)

    (define-library (generator)
      (import (scheme base))
      (export generator-lambda)
      (begin
        ;; NOTE: does not accept any parameters, yet...
        (define-syntax generator-lambda
          (syntax-rules ()
            ((generator-lambda yielder body ...)
              (letrec
                ((next
                   (lambda (return)
                     (let-syntax
                       ((yielder (syntax-rules ()
                                   ((_ value)
                                    (set! return
                                      (call/cc (lambda (here)
                                                 (return (cons here value)))))))))
                       (return (begin body ...))))))

                 ;; trampoline
                 (lambda ()
                   (let
                     ((v (call/cc (lambda (cc) (next cc)))))
                     (if (pair? v)
                       (begin
                         (set! next (car v))
                         (cdr v)) v)))))))))

Generator usage example
-----------------------

    (import (scheme base)
            (scheme write)
            (generator))

    (define (println . s)
      (for-each display s)
      (newline))

    (define num
      (generator-lambda yield
        (yield 10)
        (yield 20)
        (yield 30)
        -1))

    (println "1: " (num) " (should be 10)")
    (println "2: " (num) " (should be 20)")
    (println "3: " (num) " (should be 30)")
    (println "4: " (num) " (should be -1)")
    (println "5: " (num) " (should be -1)")

Memoization
===========

Probably the most boring thing to do, since you've probably done it yourself,
but here it is anyway.

    (define-library (memoization)
      (import (scheme base)
              (scheme write)
              (srfi 69))
      (export
        define-memoize
        lambda-memoize)
      (begin
        (define-syntax lambda-memoize
          (syntax-rules ()
            ((_ (arg ...) body ...)
             (let
               ((table (make-hash-table equal?)))
               (lambda (arg ...)
                 (let ((key (list arg ...)))
                   (if (hash-table-exists? table key)
                     (hash-table-ref table key)
                     (let
                       ((value (begin body ...)))
                       (hash-table-set! table key value)
                        value))))))))

        (define-syntax define-memoize
          (syntax-rules ()
            ((_ (name arg ...) body ...)
             (define name
               (lambda-memoize (arg ...)
                  (begin body ...))))))))

We also need the measure-time library:

    (define-library (measure-time)
      (import (scheme base)
              (scheme time)
              (print))
      (export
        measure-time
        report-time)
      (begin
        (define-syntax measure-time
          (syntax-rules ()
            ((_ body ...)
             (let*
               ((start (current-second))
                (value (begin body ...))
                (time-taken (- (current-second) start)))
               (values time-taken value)))))

        (define-syntax report-time
          (syntax-rules ()
            ((_ body ...)
             (let
               ((code (quote body ...)))
               (call-with-values
                 (lambda () (measure-time body ...))
                 (lambda (time value)
                   (println code " ==> " value " (" time " secs)")
                   value))))))))

Usage
-----

    (import (scheme base)
            (measure-time)
            (print)
            (quote-code)
            (memoization))

    (define mul
      (lambda-memoize (a b)
        (println "<calculating " a "*" b ">")
        (* a b)))

    (define (fibo-slow n)
      (if (<= n 1) n
          (+ (fibo-slow (- n 1))
             (fibo-slow (- n 2)))))

    (define-memoize (fibo-fast n)
      (if (<= n 1) n
          (+ (fibo-fast (- n 1))
             (fibo-fast (- n 2)))))

    (println "Prove that memoization works ...")

    (run (mul 12 12))
    (run (mul 12 12))
    (run (mul 12 12))
    (newline)

    (run (mul 21 21))
    (run (mul 12 12))
    (run (mul 21 21))
    (newline)

    (println "Measuring times for fibo-slow and fibo-fast ...")

    (report-time (fibo-slow 35))
    (report-time (fibo-fast 35))
    (report-time (fibo-fast 100))

Other things
============

* Object orientation as a library: Bryan's Object System (actually the guy who
wrote "Real World Haskell", I think)
* [Non-deterministic programming](http://c2.com/cgi/wiki?AmbSpecialForm)
* [Prorgramming with continuations](http://matt.might.net/articles/programming-with-continuations--exceptions-backtracking-search-threads-generators-coroutines/)
* [Green threads](https://en.wikipedia.org/wiki/Continuation)

What (R<sup>7</sup>RS) Scheme *doesn't* have
============================================

It doesn't have anaphoric macros, but most systems give you defmacro anyway.
It's just not standardized. However, the macro system in RnRS Scheme is
hygienic, which is a good thing.

It doesn't have delimited continuations, but again, most Schemes actually
provide them, it's just that there isn't a standardized interface.

What about Common Lisp? It probably has *all* of the above, either in the
language itself or in libraries.  Also, Common Lisp compilers are supposedly
*really* good at delivering optimized binaries.

Why would you care?
===================

Remember I used pseudo-JavaScript in the very first example? Well, there has
been talk about adding continuations to JavaScript.  So it's better to learn
about it now than later.

What other cool stuff can you implement with continuations? Take a look at what
the Scala people are using them for. One cool usage that was made in Scheme was
a web server that could *serialize* continuations and send them across
processes. If I recall correctly, they used continuations to plug the stateless
hole you have when doing the server-client-server round dance, so that you
could program as if the user was there all the time, as in:

    name = ask_user("What is your name?");
    age = ask_user("What is your age?");
    print_to_user("Your name is %s and your age is %d", name, age);

The above program serves a complete HTML page to the user, asking his name.
If he chooses to answer — and after any amount of time — the program will
extract his reply, put it in the `name` variable and continue running as if
nothing had happened in between. In other words, we plug the statelessness hole
of HTTP using continuations, and can write programs that look like any other,
even though they go through an endless server-client round dance.

[callstack]: https://en.wikipedia.org/wiki/Call_stack#Structure
[chibi]: https://github.com/ashinn/chibi-scheme
[controlflows]: http://cstheory.stackexchange.com/q/16312/5442
[cps]: https://en.wikipedia.org/wiki/Continuation-passing_style
[delimited]: https://en.wikipedia.org/wiki/Delimited_continuation
[gls]: https://en.wikipedia.org/wiki/Guy_L._Steele,_Jr.
[undelimited]: https://en.wikipedia.org/wiki/Continuation
]]></content:encoded>
      <dc:date>2016-05-07T07:30:18+00:00</dc:date>
    </item>
    <item>
      <title>Commodore 64 assembly coding on the command line</title>
      <link>https://csl.name/post/c64-coding/</link>
      <description><![CDATA[
Have you ever had this nagging feeling that you're less worth because you never
actually coded assembly on the C64? Of course you have! Fortunately, here's how
you can finally do something about it, using the OS X or Linux command line.

]]></description>
      <pubDate>Fri, 06 May 2016 21:37:13 +0000</pubDate>
      <guid>https://csl.name/post/c64-coding/</guid>
      <content:encoded><![CDATA[<p class="lead">
Have you ever had this nagging feeling that you're less worth because you never
actually coded assembly on the C64? Of course you have! Fortunately, here's how
you can finally do something about it, using the OS X or Linux command line.
</p>

Fun fact: The original Lucasfilm Commodore 64 games, such as Maniac Mansion,
were cross-compiled on UNIX workstations, transferred over the network and
uploaded to running C64s — way back in 1987! (Source: The <a
href="https://blog.thimbleweedpark.com">Thimbleweed Park podcasts</a>, which I
highly recommend.) That's an excellent workflow that we'll mimic here. But
instead of using real C64s, we'll be using the <a
href="http://vice-emu.sourceforge.net">VICE</a> emulator.

By the way, if you happen to be a fan of Sublime, you can download a complete
package over at <a
href="http://dustlayer.com/c64-coding-tutorials/2013/2/10/dust-c64-command-line-tool">Dustlayer</a>.
Here, we'll be using good old `make` and the <a
href="https://web.archive.org/web/20150520143433/https://www.esw-heim.tu-clausthal.de/~marco/smorbrod/acme/">ACME
assembler</a>.

Installing the tools
--------------------

To install ACME and VICE on OSX,

    $ brew install acme vice

On Linux, you *may* find those in your package manager. If you don't, just
download VICE and compile from source. It's really simple and straight forward,
and you get the C64 ROMs with the source (but not with the binary packages).
Just don't install to a non-standard location; VICE didn't like that the last
time I tried. The ACME assembler is very old, but should be <a
href="https://web.archive.org/web/20150520143433/https://www.esw-heim.tu-clausthal.de/~marco/smorbrod/acme/">readily
available from archive.org</a> . It's very small and doesn't even have a kludgy
`configure` script, if I recall correctly.

To compile a C64 assembly file with ACME, type

    $ acme --cpu 6510 --outfile foo.prg foo.asm

This produces `foo.prg`.  I am aware that the 6502 and the C64's 6510 CPUs are
supposed to be instruction set compatible; I pass `--cpu 6510` anyway, just in
case there should be minor ISA differences.

If you use the <a
href="https://github.com/actraiser/dust-tutorial-c64-first-intro">Dust demo</a>
(referenced above), you can type `acme index.asm` to compile it. Files are
placed in the `build` directory.

With VICE you can execute the `.prg` file directly, but using its `c1541`
utility you can also put them into `.d64` disk images. On OSX, the tool is
located in /Applications/Vice64/tools/c1541. For the Dust demo, the image can
be made with

    $ c1541 -format diskname,id d64 image_name.d64 -write build/hello_world.prg hello.prg

I've just used `diskname` as the name of the dist, and `id` as its ID.

The Makefile
------------

Now that you know how to compile and create disk images, you can put it
together in a makefile.

    TARGETS := foo
    C1541 := /Applications/Vice64/tools/c1541
    X64 := open /Applications/Vice64/x64.app

    .PRECIOUS: %.d64

    all: $(TARGETS)

    %.prg: %.asm
      acme --cpu 6510 --format cbm --outfile $@ $<

    %.d64: %.prg
      $(C1541) -format foo,id d64 $@ -write $<

    %: %.d64
      $(X64) $<

    clean:
      rm -f $(TARGETS) *.prg *.d64

If you have a file `foo.asm` in the same directory, you can type

    $ make foo

to compile `foo.prg`, put it into a `foo.d64` disk image and run it from VICE.
Since the `prg`-file is an implicit target, `make` will delete it before
executing the code. You can keep it with `make foo.prg` or by adding it to
`.PRECIOUS`.

You can also launch VICE and run the code the old-fashioned way.  This
*requires* a disk image. From VICE, open the file browser to mount the image,
then do

    LOAD "$",8
    LIST

to list the disk contents. Then do

    LOAD "FOO.PRG",8,1
    RUN

to load and run the program. While loading, hit Command-W to enter warp speed,
making it load faster, then disable warp mode again before running the code.

A BASIC loader
--------------

The `.prg` file format is very simple: It starts with a two-byte memory address
which the rest of the contents are loaded into. You have to specify the
secondary load argument `,1` to make sure the contents are loaded into this
memory address: The default is to load the contents into the start of BASIC
memory at `$0801`, meaning it expects the `.prg` file to contain just a BASIC
program. The usual trick is to add a small BASIC program at the beginning of
the `.prg` that just jumps to the real assembly program in memory.

So, if your assembly program is set to start at `$0900`, then the `.prg` file
will contain

    <$0801 (two-byte load address)>
    <a small BASIC program, e.g. "10 SYS 2304", where $0900 = 2304>
    <filler space ...>
    <your assembly program starting at, e.g. $0900>

You can omit the starting BASIC program in your files if you're very clever,
but most programs seem to use this trick.

You can see some example files at <a
href="https://github.com/cslarsen/c64-examples">https://github.com/cslarsen/c64-examples</a>.

The BASIC loader I use there is given below. It may be a bit much to understand
as a first tutorial, but it just encodes a `10 SYS <start address>` program:

    ; A BASIC booter, encodes `10 SYS <address>`.
    ; Macroified from http://www.pouet.net/topic.php?which=6541

    !source "constants.asm"

    !macro start_at .address {
      * = basic
      !byte $0c,$08,$00,$00,$9e
      !if .address >= 10000 { !byte 48 + ((.address / 10000) % 10) }
      !if .address >=  1000 { !byte 48 + ((.address /  1000) % 10) }
      !if .address >=   100 { !byte 48 + ((.address /   100) % 10) }
      !if .address >=    10 { !byte 48 + ((.address /    10) % 10) }
      !byte $30 + (.address % 10), $00, $00, $00
      * = .address
    }

    ; A cooler example is to write
    ;
    ;   10 SYS <address>: REM <backspaces>Your comment
    ;
    ; When the user types LIST, he will just see
    ;
    ;   10 Your comment
    ;
    ; but still be able to run it.
    ; For this, see http://codebase64.org/doku.php?id=base:acme-macro-tu

The contents of `constants.asm` is just some memory mapped locations for the
start of BASIC memory and foreground and background colors. By writing to the
color locations, you can change the colors.

    ;; Start of BASIC program
    basic = $0801

    ;; Background color
    bgcol = $d021

    ;; Border color
    bocol = $d020

Example code
------------

To use the BASIC booter, include the file and invoke the macro with `+start_at
<address>`:

    !source "basic-boot.asm"

    +start_at $0900

    ; Set background and border to black
    ldx #$00
    stx bgcol
    stx bocol

    ; Flicker border and background
    .loop
      inc bgcol
      inc bocol
      jmp .loop

It wraps the loader in an ACME macro `start_at`. The main assembly here starts
at `$0900`, meaning it loads super fast: The BASIC loader starts `$0801` and
the rest of the code at `$0900`. Now, the `.PRG` file format simply consists of
a destination address in memory to load the file contents into. If the space
between your BASIC loader (which *must* start at `$0801`) and your entry point
is huge, then you'll waste space, and the file will take forever to load, even
if you're using warp mode in your emulator. If you need more space, let the
first `.PRG` file be a loader so your program gets up and running quickly.

If you put the above code into `flicker.asm`, using the above makefile you can
now run it by typing

    $ make flicker
    acme --cpu 6510 --format cbm --outfile flicker.prg flicker.asm
    /Applications/Vice64/tools/c1541 -format foo,id d64 flicker.d64 -write flicker.prg
    Unit: 0
    Formatting in unit 8...
    Writing file `FLICKER.PRG' as `FLICKER.PRG' to unit 8.
    open /Applications/Vice64/x64.app flicker.d64
    rm flicker.prg

The output is given below

<img class="u-max-full-width"
     src="/gfx/post/c64-loading.png"
     alt="Commodore 64 loading screen">

<img class="u-max-full-width"
     src="/gfx/post/c64-flicker.png"
     alt="Demo running on a Commodore 64, showing flickering colors">

What next?
----------

The only thing you need now is a lot of time on your hands, a good C64
reference manual and memory map, and you're set for hours of fun (*after*
you've made a stable raster, of course).
]]></content:encoded>
      <dc:date>2016-05-06T21:37:13+00:00</dc:date>
    </item>
    <item>
      <title>Imputing the presence of the HLA-B27 antigen using your 23andMe genome</title>
      <link>https://csl.name/post/hla-b27/</link>
      <description><![CDATA[
Recently, I wanted to see if I could impute the
presence of the HLA-B27
antigen using my raw 23andMe DNA data. This is a pretty important antigen,
because it is associated with several diseases. Although I used a very
small reference data set, I managed to get a rough result that happened to
coincide with a blood test.

]]></description>
      <pubDate>Tue, 23 Feb 2016 21:18:00 +0000</pubDate>
      <guid>https://csl.name/post/hla-b27/</guid>
      <content:encoded><![CDATA[<p class="lead">
Recently, I wanted to see if I could <a
href="https://en.wikipedia.org/wiki/Imputation_(genetics)">impute</a> the
presence of the <a href="https://en.wikipedia.org/wiki/HLA-B27">HLA-B27
antigen</a> using my raw 23andMe DNA data. This is a pretty important antigen,
because it is associated with several diseases. Although I used a very
small reference data set, I managed to get a rough result that happened to
coincide with a blood test.
</p>

**NOTE**: I'm just a <a href="https://www.biostars.org/p/165472/">hobbyist</a>,
so tread carefully.

Why is this cool? Because you can't use a service 23andMe to find out if you
have HLA-B27: There is no known SNP algorithm that can *accurately* tell you if
you have it (although <a
href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3640413/">rs4349859</a> comes
close, but 23andMe doesn't sequence it). So the idea is to statistically
compare your genome with others whose HLA-B27 presence or absence is known, and
then *infer* the probability that you have it or not.

Prerequisites
-------------

  * You will need [SNP2HLA](https://www.broadinstitute.org/mpg/snp2hla/) from
    the Broad Institute. Install it along with third party software by
    following [the installation
    instructions](https://www.broadinstitute.org/mpg/snp2hla/snp2hla_manual.html).
  * I used [plink version 2](https://www.cog-genomics.org/plink2) instead of
    version 1.
  * Install [Beagle](http://faculty.washington.edu/browning/beagle/b3.html).
    SNP2HLA is very specific about requiring [version
    3.0.4](http://faculty.washington.edu/browning/beagle/recent.versions/beagle_3.0.4_05May09.zip).
  * Download your [raw 23andMe genome file](https://www.23andme.com/you/download/).

Converting your 23andMe files
-----------------------------

To convert your them to plink output files (`.bed`, `.bim` and `.fam`), we use
plink2:

    $ plink2 --23file genome.txt SURNAME FORENAME M --out foo

Here `SURNAME` and `FORENAME` is the individual's name and must contain *no*
spaces, but doesn't have to be in uppercase. They're just used as a marker to
distinguish between people, if you want to perform bulk operations. `M` means
*male* (it can also be deduced, obviously, but may be incorrect in some cases).
Use `F` for female. The `--out foo` means plink will create output files
`foo.bed`, `foo.bim` and `foo.fam`.

Imputing the presence of HLA-B27
--------------------------------

Next, we need to perform the actual imputation for HLA. To do this, we need a
reference data set, and we'll use the HapMap CEU reference data set bundled
with SNP2HLA.

This is a very small example set, consisting of some 124 individuals. Meaning,
your results will be very inaccurate, and if you're not of European descent,
may be completely useless.

The SNP2HLA authors have a larger data set with over 5000 individuals that used
to be bundled with the software. It was redacted as of version 1.0.3,
because of privacy and security. But if you're a serious researcher, you can
ask for a copy of the full set. No, I don't have it.

To perform the actual imputation, simply do

    ./SNP2HLA.csh foo HM_CEU_REF foo2hla `which plink2` 2000 1000

The first argument, `foo` is the name of the output files in the previous step.
`HM_CEU_REF` is the set to base the imputation on, `foo2hla` is the output
base name for this operation (I like to discern between plink and SNP2HLA
output), then there's a path to plink (I use `plink2`). The last two arguments
are memory limits. It's really only needed when processing large groups of, but
I kept them anyway.

So, the output files you get now:

    foo2hla.bed
    foo2hla.bgl.gprobs
    foo2hla.bgl.log
    foo2hla.bgl.phased
    foo2hla.bgl.r2
    foo2hla.bim
    foo2hla.dosage
    foo2hla.fam

In `foo2hla.bgl.phased`, you can see the inferred presence or absence for each
of the two chromosomes.

    $ grep HLA_B_27 foo2hla.bgl.phased
    M HLA_B_27 A A
    M HLA_B_2705 A A

This also says that HLA-B27 is `A`bsent in both chromosomes for HLA-B27 and the
HLA-B2705 allele.

As for the probabilities,

    $ grep HLA_B_27 foo2hla.bgl.gprobs
    foo2hla.bgl.gprobs:HLA_B_27 P A 0,002 0,110 0,888
    foo2hla.bgl.gprobs:HLA_B_2705 P A 0,002 0,110 0,888

The first line is for HLA-B27 in general, ignoring any sub types. The last three
numbers are the probabilities for presence in both chromosomes (`PP`), one
present and one absent (`PA`) and absent in both (`AA`). So the probability is
a whooping 88.8% for the complete absence of HLA-B27 in this case (but 11.2%
for the presence of at least one copy). In comparison, about <a
href="https://en.wikipedia.org/wiki/HLA-B27">eight percent of Caucasians</a>
posses this gene.  The second line is for the `HLA-B*2705` sub type, and it has
the same probabilities.

So, even if there is such a big uncertainty in the results, it does give a very
crude indication, even for the small reference data set. And, it did match up
with the result of the blood test, which was fun, but could just as well have
been pure luck.

Bottom line: It's pretty awesome that ordinary people can do stuff like this.
I can easily imagine a professional service built around imputation: Patient
gets genotyped, it's stored on a secure server. The doctor can then, based on a
permission scheme, run imputation for things like HLA-B27 as *one* of many tools
when diagnosing. 

A blood test will _always_ be more accurate than imputation, but is time
consuming and costs money. If a doctor could quickly see that the probability
of HLA-B27 is low, and depending on the context, it may not be necessary to
order a blood test at all.
]]></content:encoded>
      <dc:date>2016-02-23T21:18:00+00:00</dc:date>
    </item>
    <item>
      <title>Using GNU Global with C++ and Git</title>
      <link>https://csl.name/post/gtags/</link>
      <description><![CDATA[
GNU global is a very
compelling way to quickly search your project from the command line.  Here I
show how to index your code and update it automatically when using git.

]]></description>
      <pubDate>Tue, 23 Feb 2016 19:19:20 +0000</pubDate>
      <guid>https://csl.name/post/gtags/</guid>
      <content:encoded><![CDATA[<p class="lead">
<a href="https://www.gnu.org/software/global/">GNU global</a> is a very
compelling way to quickly search your project from the command line.  Here I
show how to index your code and update it automatically when using git.
</p>

Indexing your code
------------------

To index your code, simply run the following in your project root directory:

    $ gtags

This will parse your code for tags and create the database files `GPATH`,
`GRTAGS` and `GTAGS`. It's blazing fast, too: A project with several million
lines of code took only seven seconds on my workstation.

For C++ projects, note that `gtags` may interpret `.h` files as C code. You can
fix that up with `~/.globalrc`, but so far I haven't had any big problem with
that.

Querying the code
-----------------

Now that you have indexed your project, you can query for specific tags by
typing `global name`. For example, to see the definition for `value_t`, just
type

    $ global value_t
    lib/optional/libffi/ffi.cpp

Add `-x` to get a code excerpt:

    $ global -x value_t
    value_t            28 lib/optional/libffi/ffi.cpp struct value_t {

To see where `value_t` is referenced, add `-r`:

    $ global -xr value_t
    value_t           415 lib/optional/libffi/ffi.cpp   value_t *retval = new value_t(size);
    value_t           463 lib/optional/libffi/ffi.cpp   value_t* value = static_cast<value_t*>(car(p)->pointer->value);
    value_t           471 lib/optional/libffi/ffi.cpp   value_t* value = static_cast<value_t*>(car(p)->pointer->value);
    value_t           479 lib/optional/libffi/ffi.cpp   value_t* value = static_cast<value_t*>(car(p)->pointer->value);
    value_t           487 lib/optional/libffi/ffi.cpp   value_t* value = static_cast<value_t*>(car(p)->pointer->value);
    value_t           495 lib/optional/libffi/ffi.cpp   value_t* value = static_cast<value_t*>(car(p)->pointer->value);
    value_t           503 lib/optional/libffi/ffi.cpp   value_t* value = static_cast<value_t*>(car(p)->pointer->value);
    value_t           511 lib/optional/libffi/ffi.cpp   value_t* value = static_cast<value_t*>(car(p)->pointer->value);

You can also use `-g` to grep, so you don't have to spell out the correct name:

    $ global -xg make_clo
    make_clo           34 include/mickey/eval.h cons_t* make_closure(cons_t* args, cons_t* body, environment_t* e);
    make_clo          371 src/core-transition.cpp    * (or we could call make_closure here):
    make_clo          218 src/eval.cpp               cons_t *closure = make_closure(def_args, def_body, e->extend());
    make_clo          272 src/eval.cpp             return make_closure(args, body, e->extend());
    make_clo          613 src/library/scheme-base.cpp    * (or we could call make_closure here):
    make_clo           60 src/library/srfi-16.cpp   return make_closure(symbol("args"), cons(cond_cases), e);
    make_clo           14 src/make-closure.cpp cons_t* make_closure(cons_t* args, cons_t* body, environment_t* e)

You can also use regular expressions with `-e`. See `man global` for many other
options.

Pro tip: Create an alias for `global`:

    $ alias gx='global -x'
    $ gx main
    main              144 src/mickey.cpp   int main(int argc, char** argv)
    $ gx -r make_closure
    make_closure       34 include/mickey/eval.h cons_t* make_closure(cons_t* args, cons_t* body, environment_t* e);
    make_closure      218 src/eval.cpp               cons_t *closure = make_closure(def_args, def_body, e->extend());
    make_closure      272 src/eval.cpp             return make_closure(args, body, e->extend());
    make_closure       60 src/library/srfi-16.cpp   return make_closure(symbol("args"), cons(cond_cases), e);

There is also the `gtags-cscope` utility. It lets you interactively search for
stuff and open an editor. I'm not sure I'll be using it much, but it's there.

Automatically syncing gtags on git updates
------------------------------------------

At work, I've set up so that the tag-files are automatically updated whenever
git changes the files on disk, for example when I fetch and checkout,
cherry-pick and so on. I'll show how to do that.

First, add `GPATH`, `GRTAGS` and `GTAGS` to `.gitignore`:

    $ echo "GPATH\nGRTAGS\nGTAGS" >> .gitignore

Next, you need to set up some git hooks.  The whole approach here is taken from
[Effortless Ctags with
Git](http://tbaggery.com/2011/08/08/effortless-ctags-with-git.html) by Tim
Pope, and slightly modified to work with GNU global.

Put the below script in `.git/hooks/gtags`:

    #! /bin/sh
    set -e
    dir="`git rev-parse --git-dir`"
    trap 'rm -f "$dir/$$.gtags"' EXIT
    mkdir "$dir/$$.gtags"
    git ls-files | \
        gtags --file=- --skip-unreadable "$dir/$$.gtags"
    mv $dir/$$.gtags/* "$dir/.."

Unfortunately, GNU global doesn't allow relative paths in the environment
variable `GTAGSROOT`, so we can't store the database under `.git`.

Put the following in `post-commit`, `post-merge`, `post-checkout`:

    #! /bin/sh
    .git/hooks/gtags >/dev/null 2>&1 &

Notice that it runs `gtags` in the background, which is nice.

Put this in `post-rewrite`:

    #! /bin/sh
    case "$1" in
      rebase) exec .git/hooks/post-merge ;;
    esac

For an explanation of the above hooks, look at the post by Tim Pope above. It
suggests putting them in a git template directory, which means they are
automatically copied over to new or cloned repositories.

Finally, make them executable:

    $ chmod +x .git/hooks/gtags \
               .git/hooks/post-commit \
               .git/hooks/post-merge \
               .git/hooks/post-checkout \
               .git/hooks/post-rewrite

Now, whenever you checkout etc., the index will be updated.

As Tim Pope writes, you can even create a command `git gtags` so you can update
on demand:

    $ git config --global alias.gtags '!.git/hooks/gtags'

This lets you do

    $ git gtags

to manually update the database. However, if you have only changed a few files
and want to re-index, you might be better off using `global -u` to perform an
update. It could be faster.
]]></content:encoded>
      <dc:date>2016-02-23T19:19:20+00:00</dc:date>
    </item>
    <item>
      <title>Adding a TLS certificate to your NearlyFreeSpeech site</title>
      <link>https://csl.name/post/tls-nfsn/</link>
      <description><![CDATA[I recently had to renew the TLS certificate on csl.name,
which is hosted on NearlyFreeSpeech (NFSN).
But I had forgotten how to. So I decided to post a condensed version of the
steps found here on my own site.
]]></description>
      <pubDate>Sat, 23 Jan 2016 21:54:29 +0000</pubDate>
      <guid>https://csl.name/post/tls-nfsn/</guid>
      <content:encoded><![CDATA[I recently had to renew the TLS certificate on [csl.name](https://csl.name),
which is hosted on [NearlyFreeSpeech (NFSN)](https://www.nearlyfreespeech.net).
But I had forgotten how to. So I decided to post a [condensed version of the
steps found here][steps] on my own site.

First I made a secret key on the NearlyFreeSpeech login server (aliases as
`nfsn` here):

    ssh nfsn
    mkdir /home/protected/ssl
    cd /home/protected/ssl
    openssl genrsa -out csl.name.key 4096

Using this, I made a certificate signing request (CSR). I left most fields
blank, except FQDN, which had to be `csl.name`:

    openssl req -new -sha256 -key csl.name.key -out csl.name.csr

After buying the [Comodo PositiveSSL certificate on
NameCheap](https://www.namecheap.com/security/ssl-certificates/comodo/positivessl.aspx),
pasting the CSR into their SSL wizard and completing the DCV verification (any
method is fine), I got a ZIP-file containing the already bundled-up [TLS
certificate chain][tls chain] along with the certificate itself.

Next I copied the files over and verified them:

    openssl verify -untrusted csl.name.chn csl.name.crt
    csl.name: OK

Note that this process failed locally, probably because I had an older version
of OpenSSL.

The final step was to file a support request with NFSN to install the
certificate, which they promptly did. Just be sure that the
`/home/protected/ssl` directory contains the _all_ the necessary files:

    csl.name.chn
    csl.name.crt
    csl.name.csr
    csl.name.key

By the way, if you're thinking about doing this you may want to consider trying
[LetsEncrypt.org](https://letsencrypt.org), which give you free TLS
certificates. I don't know much about them, or how many browsers support their
CA, and so on, but it's worth a shot.

[steps]: https://www.mc-guinness.co.uk/blog/20150710/set-up-https-ssl-tls-encryption-access-to-nearlyfreespeech-hosted-sites/
[tls chain]: https://faq.nearlyfreespeech.net/section/ourservice/tlschain#tlschain
]]></content:encoded>
      <dc:date>2016-01-23T21:54:29+00:00</dc:date>
    </item>
    <item>
      <title>Making a simple virtual machine interpreter in Python</title>
      <link>https://csl.name/post/vm/</link>
      <description><![CDATA[
]]></description>
      <pubDate>Wed, 28 Jan 2015 20:59:50 +0000</pubDate>
      <guid>https://csl.name/post/vm/</guid>
      <content:encoded><![CDATA[<p class="lead">

I was asked if I could write a simple blog post on how to make a
<a href="https://en.wikipedia.org/wiki/Stack_machine">(process) virtual
machine</a> &mdash; specifically, a
<a href="https://en.wikipedia.org/wiki/Virtual_machine">stack machine</a>.
Using Python, I'm going to convince you how easy it is!

</p>

**Update**: Based on [comments from r/Python][r/Python], I've made some small
code changes. Thanks to robin-gvx, bs4h and Dagur! The code in this post is now
available on [GitHub][github].

A stack machine doesn't have [registers][register-machine]. Instead, it puts
values on a stack and operate on that.  Stack machines are extremely simple,
but very powerful. There's a reason why Python, Java, PostScript,
[Forth][forth] and many other languages have chosen a stack machine as their
VM.

Anyway, let's talk a little bit more about the stacks. We need an instruction
pointer stack, which we'll use to store return addresses. This is so that we
can call a subroutine (a function, for instance) and then jump back from where
we came. We *could* have used self-modifying code instead, like Donald Knuth's
original [MIX][mix] did, but then you'd have to manage a stack yourself if you
wanted recursion to work.  In this post, I won't really implement subroutine
calls, but they're trivial anyway (consider it an exercise).

With the _data stack_ you get a lot of stuff for free.  For instance, consider
an expression like `(2+3)*4`.  On a stack machine, the equivalent code for this
expression would be `2 3 + 4 *`: Push two and three on the stack, then the next
instruction is `+`, so pop off two numbers and push back the sum.  Next, push
four on the stack, then pop two values and push their product. Easy!

So, let's start writing a simple class for a stack.  We'll just inherit from
`collections.deque`:

    from collections import deque

    class Stack(deque):
        push = deque.append

        def top(self):
            return self[-1]

We can now `push` and `pop` to this stack and peek at the top value with `top`.

Next, let's make a class for the virtual machine itself. As noted, we need two
stacks and also some memory for the code itself. We'll rely on Python's dynamic
typing so we can put anything in the list. Only problem is that we really don't
discern between strings and built-in functions. The correct way would be to
insert actual Python functions in the list. Perhaps I'll do that in a future
update.

We also need an instruction pointer, pointing to the next item to execute in
the code.

    class Machine:
        def __init__(self, code):
            self.data_stack = Stack()
            self.return_addr_stack = Stack()
            self.instruction_pointer = 0
            self.code = code
            self.dispatch_map = {...} # see below for values

Now we'll create some convenience functions to reduce some typing:

        def pop(self):
            return self.data_stack.pop()

        def push(self, value):
            self.data_stack.push(value)

        def top(self):
            return self.data_stack.top()

We'll create a `dispatch` function that takes an "opcode" (we don't really use
codes, just dynamic types, but you get my point) and executes it. But first,
let's just create the interpreter loop:

        def run(self):
            while self.instruction_pointer < len(self.code):
                opcode = self.code[self.instruction_pointer]
                self.instruction_pointer += 1
                self.dispatch(opcode)

As you can see, it does one thing, and does it pretty well: Fetch the next
instruction, increment the instruction pointer and then dispatch based on the
opcode.  The `dispatch` function is a tad more longer:

        def dispatch(self, op):
            if op in self.dispatch_map:
                self.dispatch_map[op]()
            elif isinstance(op, int):
                # push numbers on the data stack
                self.push(op)
            elif isinstance(op, str) and op[0]==op[-1]=='"':
                # push quoted strings on the data stack
                self.push(op[1:-1])
            else:
                raise RuntimeError("Unknown opcode: '%s'" % op)

**Update:** The `dispatch_map` referenced in `__init__` should contain the
values below. It used to be constructed in `dispatch`, but as [EnTerr][EnTerr]
pointed out in the comments below, that is grossly inefficient.

        dispatch_map = {
            "%":        self.mod,
            "*":        self.mul,
            "+":        self.plus,
            "-":        self.minus,
            "/":        self.div,
            "==":       self.eq,
            "cast_int": self.cast_int,
            "cast_str": self.cast_str,
            "drop":     self.drop,
            "dup":      self.dup,
            "if":       self.if_stmt,
            "jmp":      self.jmp,
            "over":     self.over,
            "print":    self.print_,
            "println":  self.println,
            "read":     self.read,
            "stack":    self.dump_stack,
            "swap":     self.swap,
        }

Basically, `dispatch` looks up the opcode, and sees if it's a built-in function
like `*` or `drop` or `dup`.  By the way, those are [Forth][forth] words, a
brilliant language that you should check out.  In fact, the code you'll see
here is basically a simple Forth.

Anyway, it looks up an opcode like `*`, sees it should call `self.mul` then
executes it. It looks like this:

        def mul(self):
            self.push(self.pop() * self.pop())

All the other functions are like this.  If it can't find the operation in the
dispatch map, it will first see if it's a number. Numbers are automatically
pushed on the data stack.  If it's a quoted string, it will push that.

So, there you have it! Congrats!

Let's define a few more operations, then write a program using our newly
designed virtual machine and [p-code language][p-code]:

        # Allow to use "print" as a name for our own method:
        from __future__ import print_function

        # ...

        def plus(self):
            self.push(self.pop() + self.pop())

        def minus(self):
            last = self.pop()
            self.push(self.pop() - last)

        def mul(self):
            self.push(self.pop() * self.pop())

        def div(self):
            last = self.pop()
            self.push(self.pop() / last)

        def print(self):
            sys.stdout.write(str(self.pop()))
            sys.stdout.flush()

        def println(self):
            sys.stdout.write("%s\n" % self.pop())
            sys.stdout.flush()

Let's write that `print((2+3)*4)` example using our virtual machine:

    Machine([2, 3, "+", 4, "*", "println"]).run()

You can even try running it!

Now, let's introduce a jump operation, a _go-to_ operation:

        def jmp(self):
            addr = self.pop()
            if isinstance(addr, int) and 0 <= addr < len(self.code):
                self.instruction_pointer = addr
            else:
                raise RuntimeError("JMP address must be a valid integer.")

It just changes the instruction pointer.  Now let's look at branching:

        def if_stmt(self):
            false_clause = self.pop()
            true_clause = self.pop()
            test = self.pop()
            self.push(true_clause if test else false_clause)

This is also very straight-forward. If you wanted to add a conditional jump,
you'd have to simply do `test-value true-value false-value IF JMP`.  (As
branching is performed often, many VMs provide instructions like `JNE`, "jump
if not equal", for convenience).

Here's a program that asks the user for two numbers, then prints their sum and
product:

    Machine([
        '"Enter a number: "', "print", "read", "cast_int",
        '"Enter another number: "', "print", "read", "cast_int",
        "over", "over",
        '"Their sum is: "', "print", "+", "println",
        '"Their product is: "', "print", "*", "println"
    ]).run()

The `over`, `read` and `cast_int` operations look like this:

        def cast_int(self):
            self.push(int(self.pop()))

        def over(self):
            b = self.pop()
            a = self.pop()
            self.push(a)
            self.push(b)
            self.push(a)

        def read(self):
            self.push(raw_input())

Here's a simple program that asks the user for a number, prints if it's even or
odd, then loops:

    Machine([
        '"Enter a number: "', "print", "read", "cast_int",
        '"The number "', "print", "dup", "print", '" is "', "print",
        2, "%", 0, "==", '"even."', '"odd."', "if", "println",
        0, "jmp" # loop forever!
    ]).run()

Now, some exercises for you: Create `call` and `return` commands. The `call`
will push its current address on the return stack, then call `self.jmp()`.
The `return` operation should simply pop the return stack and set the
instruction pointer to this value (jumps back, or _returns_ from a `call`).
When you've done that, you've got subroutines. 

A simple parser for this language
---------------------------------

Let's create a small language that mimics the code.  We'll _compile_ to our
machine code:

    import tokenize
    from StringIO import StringIO

    # ...

    def parse(text):
        tokens = tokenize.generate_tokens(StringIO(text).readline)
        for toknum, tokval, _, _, _ in tokens:
            if toknum == tokenize.NUMBER:
                yield int(tokval)
            elif toknum in [tokenize.OP, tokenize.STRING, tokenize.NAME]:
                yield tokval
            elif toknum == tokenize.ENDMARKER:
                break
            else:
                raise RuntimeError("Unknown token %s: '%s'" %
                        (tokenize.tok_name[toknum], tokval))

A simple optimizer: Constant folding
------------------------------------

[Constant folding][constant folding] is an example of [peephole
optimization][peephole optimization] that looks for obvious pieces of code that
can be precomputed at compile time.  For instance, mathematical expressions
involving constants, e.g. `2 3 +`.  That should be quite simple to implement.

    def constant_fold(code):
        """Constant-folds simple mathematical expressions like 2 3 + to 5."""
        while True:
            # Find two consecutive numbers and an arithmetic operator
            for i, (a, b, op) in enumerate(zip(code, code[1:], code[2:])):
                if isinstance(a, int) and isinstance(b, int) \
                        and op in {"+", "-", "*", "/"}:
                    m = Machine((a, b, op))
                    m.run()
                    code[i:i+3] = [m.top()]
                    print("Constant-folded %s%s%s to %s" % (a,op,b,m.top()))
                    break
            else:
                break
        return code

The only problem with this approach is that we would have to update jump
locations, and that is hard to do in many cases (e.g. `read cast_int jmp`).
There are many solutions to this, but a simple work-around is to only allow
jumping to named labels in the _language_, then resolve their actual locations
after doing optimizations.

If you implement Forth _words_, or functions, you can do more optimizations,
like removing code that is provably never used (also known as [dead code
elimination][dead code elimination]).

A REPL
------

Now we can make a simple REPL like so:

    def repl():
        print('Hit CTRL+D or type "exit" to quit.')

        while True:
            try:
                source = raw_input("> ")
                code = list(parse(source))
                code = constant_fold(code)
                Machine(code).run()
            except (RuntimeError, IndexError) as e:
                print("IndexError: %s" % e)
            except KeyboardInterrupt:
                print("\nKeyboardInterrupt")

We can thus test some simple programs:

    > 2 3 + 4 * println
    Constant-folded 2+3 to 5
    Constant-folded 5*4 to 20
    20
    > 12 dup * println
    144
    > "Hello, world!" dup println println
    Hello, world!
    Hello, world!

As you can see, the constant-folder seems to work great!  In the first example,
it optimizes away all the code down to simply `20 println`.

Next steps
----------

When you have added `call` and `return`, you can let the user define new
functions.  In [Forth][forth], functions are called _words_ and they begin with
a colon, a name and ends with a semicolon.  For example, an integer square word
would be:

    : square dup * ;

You can actually try this out in a program like [Gforth][gforth]:

    $ gforth
    Gforth 0.7.3, Copyright (C) 1995-2008 Free Software Foundation, Inc.
    Gforth comes with ABSOLUTELY NO WARRANTY; for details type `license'
    Type `bye' to exit
    : square dup * ;  ok
    12 square . 144  ok

You can also add support for this by looking for `:` in the parser. When you
find one, you need to record the name to insert the name along with an address
(e.g., the current position in the code) and insert them into a [symbol
table][symbol-table].  For simplicity, you could probably even do better by
just inserting the whole code up to the concluding semicolon in a dictionary,
e.g.:

    symbol_table = {
      "square": ["dup", "*"]
      # ...
    }

When you're finished parsing, you can [link][linker] the program: Go through
the main code and look for calls to user-defined functions in the symbol table.
Whenever you find one, add that code to the _end_ of the main code, unless it's
already there. Then replace the `square` operation with a `<address>` `call`,
where `<address>` is the location where the word/function was inserted.

For this to work correctly, you should consider removing the `jmp` instruction.
Otherwise, you'll have to resolve them as well.  It *can* work, but then you
have to keep track of their references in the same order that the user wrote
the program. I.e., if you want to move around subroutines, you have to be a bit
careful. It's fully doable, though.  You should probably also add an `exit`
function to stop the program (perhaps with an exit-code to the OS?), so that
the main code execution won't continue running into the subroutines.

Actually, a good program layout would probably be to put the main code in a
subroutine itself, called `main`.  Or whatever, you decide!

As you can see, this is a lot of fun and teaches you a lot about code
generation, linking, program space layout and so on.

Even more cool things to do
---------------------------

You could use a Python bytecode generation library to attempt to translate the
VM code to native Python bytecode.  Or implement it in Java and do it on the
JVM; then you'll get [JITing][jit] for free!

Also, it would be cool to try to make a [register machine][register-machine].
You could try to implement a [call stack][call stack] with
[stack frames][stack frames] and establish a calling convention.

Finally, if you don't like the Forth-like language defined here, you can create
your own simple language that compiles down to this VM.  For instance, you
should be able to convert infix notation like `(2+3)*4` to `2 3 + 4 *` and emit
code for your VM.  You could allow for C-style code blocks a la `{ ... }` so
that a statement like `if ( test )` `{ ... }` `else` `{ ... }` would be
translated to:

    <true/false test>
    <address of true block>
    <address of false block>
    if
    jmp

    <true block>
    <address of end of entire if-statement> jmp

    <false block>
    <address of end of entire if-statement> jmp

For instance,

    Address  Code
    -------  ----
     0       2 3 >
     3       7        # Address of true-block
     4       11       # Address of false-block
     5       if
     6       jmp      # Conditional jump based on test

    # True-block
     7       "Two is greater than three."
     8       println
     9       15       # Continue main program
    10       jmp

    # False-block ("else { ... }")
    11       "Two is less than three."
    12       println
    13       15       # Continue main program
    14       jmp

    # If-statement finished, main program continues here
    15       ...

Oh, you also need to add the comparison operators `!= < <= > >=` for this to
work.

[I've done some of these things in my C++ stack machine][csl-stack-machine], so
you can get some hints from there.

I've also turned the code here into a project called [Crianza][code]. It has
more optimizations and an experimental mode to compile down to Python bytecode.

The complete code
-----------------

The complete code is available at [vm.py on github][vm.py].

[EnTerr]: https://csl.name/post/vm/#comment-2467436000
[call stack]: https://en.wikipedia.org/wiki/Call_stack
[code]: https://github.com/cslarsen/crianza
[constant folding]: https://en.wikipedia.org/wiki/Constant_folding
[csl-stack-machine]: https://github.com/cslarsen/stack-machine
[dead code elimination]: https://en.wikipedia.org/wiki/Dead_code_elimination
[forth]: https://en.wikipedia.org/wiki/Forth_(programming_language)
[gforth]: https://www.gnu.org/software/gforth/
[github]: https://github.com/cslarsen/python-simple-vm
[jit]: https://en.wikipedia.org/wiki/Just-in-time_compilation
[linker]: https://en.wikipedia.org/wiki/Linker_(computing)
[mix]: https://en.wikipedia.org/wiki/MIX
[p-code]: https://en.wikipedia.org/wiki/P-code_machine
[peephole optimization]: https://en.wikipedia.org/wiki/Peephole_optimization
[process-vm]: https://en.wikipedia.org/wiki/Virtual_machine#Process_virtual_machines
[r/Python]: https://pay.reddit.com/r/Python/comments/35tg6b/making_a_simple_vm_interpreter_in_python/
[register-machine]: https://en.wikipedia.org/wiki/Register_machine
[stack frames]: https://en.wikipedia.org/wiki/Call_stack#STACK-FRAME
[symbol-table]: https://en.wikipedia.org/wiki/Symbol_table
[vm.py]: https://github.com/cslarsen/python-simple-vm/blob/master/vm.py
]]></content:encoded>
      <dc:date>2015-01-28T20:59:50+00:00</dc:date>
    </item>
    <item>
      <title>Compiling and using em-dosbox</title>
      <link>https://csl.name/post/em-dosbox/</link>
      <description><![CDATA[Archive.org has released a large number of MS-DOS
games that can be
played in the browser using
em-dosbox.  Here I&#39;ll show you how
you can compile em-dosbox and put up your own MS-DOS programs on the web.  This
guide assumes you&#39;re using Mac OS X and Homebrew, but should be
informative for other UNIX users as well.
]]></description>
      <pubDate>Thu, 22 Jan 2015 21:08:00 +0000</pubDate>
      <guid>https://csl.name/post/em-dosbox/</guid>
      <content:encoded><![CDATA[Archive.org has released a [large number of MS-DOS
games](https://archive.org/details/softwarelibrary_msdos_games/v2) that can be
played in the browser using
[em-dosbox](https://github.com/dreamlayers/em-dosbox).  Here I'll show you how
you can compile em-dosbox and put up your own MS-DOS programs on the web.  This
guide assumes you're using Mac OS X and [Homebrew](http://brew.sh), but should be
informative for other UNIX users as well.

Installing Emscripten
---------------------

The [em-dosbox](https://github.com/dreamlayers/em-dosbox) project
is a special fork of [DOSBox](http://www.dosbox.com) that uses
[emscripten](https://github.com/kripken/emscripten) to cross-compile LLVM
bitcode to JavaScript so it can run in your browser.  Emscripten can actually
be used to cross-compile practically _any_ LLVM-compilable code to JavaScript!

So before we can build em-dosbox, we need to install emscripten. With Homebrew,
you can review installation options with:

    $ brew options emscripten

I used

    $ brew install emscripten --with-closure-compiler

Homebrew tells you that you need to update your `~/.emscripten` after running
`emcc` for the first time:

    Manually set LLVM_ROOT to
      /usr/local/opt/emscripten/libexec/llvm/bin

So let's run `emcc` first.

    $ emcc
    WARNING  root: (Emscripten: system change: 1.28.2|asmjs-unknown-emscripten||6.0 vs 1.4.7|le32-unknown-nacl, clearing cache)
    WARNING  root: LLVM version appears incorrect (seeing "6.0", expected "3.4")
    WARNING  root: could not check fastcomp: [Errno 2] No such file or directory
    INFO     root: (Emscripten: Running sanity checks)
    CRITICAL root: Cannot find clang++, check the paths in ~/.emscripten

Now edit `~/.emscripten` and comment out the line with `LLVM_ROOT`. Set it to
what Homebrew suggests:

    LLVM_ROOT = "/usr/local/opt/emscripten/libexec/llvm/bin"

Now try running `emcc` to verify that it runs without any errors.

    $ emcc
    WARNING  root: (Emscripten: settings file has changed, clearing cache)
    INFO     root: (Emscripten: Running sanity checks)
    WARNING  root: no input files


Installing em-dosbox
--------------------

You'll need the [em-dosbox source
code](https://github.com/dreamlayers/em-dosbox), and then run `autogen.sh`.  If
autogen fails, you probably need to install
[autotools](https://www.gnu.org/software/autoconf/).

    $ git clone https://github.com/dreamlayers/em-dosbox.git
    $ cd em-dosbox
    $ ./autogen.sh

That should create a `configure` file.  Now run that through `emconfigure`:

    $ emconfigure ./configure

If that works fine, you should be able to build em-dosbox:

    $ make -j4

On a successful build, you should find `dosbox.js` and `dosbox.html` in `src/`.
These are templates for the web page that you'll use to run MS-DOS programs.

To take an MS-DOS program called `TEST.EXE` and bundle it with em-dosbox, you
simply do:

    $ cd src
    $ ./packager.py test TEST.EXE

That should create the files `test.data` and `test.html`.

However, on my system, it complains that it can't find `file_packager.py`.
This is part of the _emscripten_ package. With Homebrew, emscripten is
symlinked to the `Cellar` directory.  You should therefore set
`EMSCRIPTEN_ROOT` in `~/.emscripten` to point to where the emscripten files are
actually located.

You can check what value `EMSCRIPTEN_ROOT` is currently set to:

    $ em-config EMSCRIPTEN_ROOT

On my system I had to set this to

    /usr/local/Cellar/emscripten/1.28.2/libexec

by modifying `~/.emscripten` so it looked like:

    import os

    EMSCRIPTEN_ROOT = "/usr/local/Cellar/emscripten/1.28.2/libexec"
    LLVM_ROOT = "/usr/local/opt/emscripten/libexec/llvm/bin"

    # ...

Now, I have an old intro I made with Turbo Pascal and assembly, back in the
day. The file is called `A-SYSTEM.EXE`, so I ran it through the packager:

    $ cd em-dosbox/src
    $ ./packager.py a-system A-SYSTEM.EXE

This creates `a-system.data` and `a-system.html`.  But if I open this 
in my browser, it won't be able to load the data files because of the
[same-origin policy](https://en.wikipedia.org/wiki/Same-origin_policy).
Therefore, I'll just start a simple web server in the current directory and
access it through that:

    $ python -m SimpleHTTPServer
    $ open http://localhost:8000/a-system.html

This works pretty well, but the intro runs quite slow.  Hitting `CTRL-F12`
speeds up the CPU and framerate considerably. But such settings can be put in a
configuration file.  For this to work, you need to move the EXE-file into a
subdirectory from `src/` and add a
[`dosbox.conf`](http://www.dosbox.com/wiki/Dosbox.conf) file.

I put this in `dosbox.conf`:

    [cpu]
    core = simple
    cycles = fixed 20000

    [dosbox]

    [midi]
    mpu401 = none
    mididevice = none

Then I built the entire directory with:

    $ ./packager.py a-system a-system A-SYSTEM.EXE

The first argument is still the prefix for the `.data` and `.html`files, the
second is the directory containing the EXE-file and config file, and the final
argument is which DOS program to embed.

I haven't [tweaked the `dosbox.conf`
settings yet](http://www.dosbox.com/wiki/Dosbox.conf), but you can [view the
A-SYSTEM 28k intro here](/a-system/).  Just be patient while it loads. 
It uses XMLHttpRequest to download `dosbox.html.mem` and `a-system.data`, which
are about 30 Mb each.  For some reason, this is slow.

**Update:** I've tried compressing those files with gzip. While it seems to
work fine with Firefox and Safari, on Chrome the emulator seems to ends
abruptly.  By the way, Firefox seems to be the one giving the best performance
with em-dosbox.
]]></content:encoded>
      <dc:date>2015-01-22T21:08:00+00:00</dc:date>
    </item>
    <item>
      <title>A Short R⁷RS Scheme Tutorial</title>
      <link>https://csl.name/post/scheme-tutorial/</link>
      <description><![CDATA[This is a work-in-progress introduction to the Scheme programming
language. Specifically, it&#39;s aimed at the latest version, R⁷RS.
This little tutorial will be updated in the time ahead, so be sure to post
comments so I know what needs to be done!
]]></description>
      <pubDate>Thu, 01 Jan 2015 10:58:04 +0000</pubDate>
      <guid>https://csl.name/post/scheme-tutorial/</guid>
      <content:encoded><![CDATA[This is a *work-in-progress* introduction to the Scheme programming
language. Specifically, it's aimed at the latest version, R⁷RS.
This little tutorial will be updated in the time ahead, so be sure to post
comments so I know what needs to be done!

<img class="u-max-full-width"
     src="/gfx/post/scheme-tutorial/sea.jpg"
     alt="Cold waves crashing onto stones" />

About Scheme
------------

Scheme is a powerful and elegant language in the Lisp family of languages.
Many people find it to be elegant because its core is farily small. The [latest
specification][spec] runs only 80 pages and is very accessible. Use it as a
reference when learning Scheme.  In addition to being small, the entire
language itself is very close to pure lambda calculus: It is possible to base
the entire language on [*four* primitives][cowan-video]
([slides][cowan-slides]).  Because of this, the language is very cohesive and
fits logically together.

I want to be honest up front and mention some downsides as well. The language
is quite fragmented across implementations due to several reasons. First of
all, the specifications leave some details unspecified so as to keep it small
and leave enough wiggle room for implementations to decide how to do things.
Most importantly, there wasn't an official library system until R⁶RS in 2007.
Even worse, many people disagreed with its design, so the new R⁷RS spec has its
own system.  This means that code is not by default portable between
implementations.  In practice, this means that people usually stick to one or
two implementation silos. Andy Wingo gives some [good suggestions on how to
pick one][wingo-impls].

Let's get started!
------------------

In this tutorial I will use the so-called example R⁷RS implementation [Chibi
Scheme][chibi-scheme].

When you invoke `chibi-scheme`, there are only two functions that are defined:
`import` and `cond-expand`. So to write "Hello, world!" you need to import
libraries.  `cond-expand` can be used to conditionally run code based on
particular features an implementation may have. It's like `#ifdef`s, if you
come from C.

Save the following in `hello-world.scm`.

<pre>
{% include scheme/tutorial/hello-world.scm %}
</pre>

This can be run by typing

    $ chibi-scheme hello-world.scm
    Hello, world!

Yeah, I almost forgot. You probably know that in Lisp we use _s-expressions_ to
write both code and data.  It means that everything is in prefix form, so that
an expression like `2 * (3 + 4)` must be written as `(* 2 (+ 3 4))`.
Personally I really love this way of writing programs, for several reasons:
It's terse but readable, it's machine-readable, there aren't syntactical ambiquities, and so on.
In particular, I like that *scope* is extremely visible because of the
parenthesis, and you work very close to the [AST][ast].

Variables
---------

You can define variables by using `define` and change existing ones using
`set!`.  Almost anything in Scheme is a first class citizen, so you use
`define` to bind functions to variables.  Here's a `cube` function:

    (define cube
      (lambda (n)
        (* n n n)))

What's going on here is that we create a function that takes one parameter `n`.
By default, the *last* expression is used as a return value, and here that will
be `(* n n)`.  We bind this function to the variable `cube`.  To cube a number,
you just call `(cube 12)`.

Since function definitions are so common, we can leave out the `lambda` by
using the shorthand form `(define (cube n) ...)`.  But sometimes, e.g. if you
want to return functions, it may be better to use a plain `lambda`.

Libraries
---------

The function `display` takes one or two arguments: An object to print and an
optional *port* --- an output destination like a file or a string buffer.

However, it only takes one argument, and is thus cumbersome to work with. E.g.,
to print a number and a string, we'd have to do

    (display (string-append "12^3 = " (number->string (cube 12))))

Let's create a small family of variadic `print` functions that all print to the
default output port. We'll bundle them up in a library.

<pre>
{% include scheme/tutorial/print.sld %}
</pre>

To use them, you need to `(import (print))` in your code.  The implementations
differ a bit in how they handle libraries.  E.g., Chibi Scheme requires that
`(define-library ...)` be in a separate file with the same name as the library
name. Also, with Chibi Scheme you can specify library search paths using the
`-I` option.

Proper Tail Recursion
---------------------

The Scheme specification requires that implementations are *properly tail
recursive*.  To explain this, let's make a short detour.

One thing that all programming languages have is an [abstract syntax tree][ast]
(AST). In Lisp dialects, this is very explicit. You're essentially coding very
close to the AST. This has several benefits, but I'll only mention one. It
becomes very clear how code is evaluated.

For instance, consider the following factorial function written in Java.

    public static int fact(int n)
    {
      if ( n == 0 )
        return 1;
      else
        return n*fact(n-1);
    }

The AST for this function could be something like

<img src="/gfx/post/scheme-tutorial/ast-java-fact.svg"
     class="u-full-width"
     style="max-height: 384px;"
     alt="Java factorial AST" />

Now, the way to evaluate this AST is to start at the top node, then descend to
each child, left-to-right.  If we do that, we can write out the scheme code
directly.  The only difference is that we'll use `equal?` instead of `==`.

    (if (equal? n 0) 1
      (* n (fact (- n 1))))

If you take a good look at this s-expression, you'll see that it is an *exact*
representation of the AST.

The reason I'm showing you this is because I want to talk about the tail call
elimination that all compliant Scheme implementations have.  It lets you write
recursive functions that will never blow up the stack: Every active tail call
is associated with a constant amount of stack space.  But this is *only*
possible when the *last thing* a function does is to *perform a function call*.

Traverse the AST depth first from left to right. What's the last thing the
function does before returning, assuming n is nonzero? Well, it has to multiply
`n` and the result of `(fact (- n 1))`.  It also means that this function may
blow up the stack for big numbers. But if we arrange so that the last thing it
does is to call itself, Scheme will use tail call optimization.

Looking at the AST again, we'll remove the nodes `*` and its child `n` and move
`fact` up so that it's a child of the `if` tree.  We'll then add an accumulator
that computes the result for us, or `(* n acc)`.

<img src="/gfx/post/scheme-tutorial/ast-fact-tail.svg"
     class="u-full-width"
     style="max-height: 324px;"
     alt="Tail-recursive factorial" />

Since we'll now take two parameters, we'll call the function `fact-helper`.
Also, instead of doing `(equal? n 0)` we'll just use `(zero? n)`.

    (define (fact-helper n acc)
      (if (zero? n) acc
        (fact-helper (- n 1)
                     (* n acc))))

For the final polish, we'll create a front-end function `fact`.

    (define (fact n)
      (define (fact-helper n acc)
        (if (zero? n) acc
          (fact-helper (- n 1)
                       (* n acc))))
      (fact-helper n 1))

Now, the function is tail recursive and thus will never blow up the stack. In
fact, a good implementation will reuse the stack frame used to call
`fact-helper` so that each call is simply a jump instruction; as fast as an
iterative version.

Numbers and Exactness
---------------------

Numbers in Scheme have an *exactness* property that is orthogonal to any
number. It basically means that you can tag a number as being exact, and
operations between exact numbers will produce an exact result if possible.
Inexactness is contagious; any operation involving them will generally be
inexact.

To mark a number as exact, just add the `#e` prefix.  To test if a number is
exact, you can use `exact?`.

    $ chibi-scheme
    > (exact? 1)
    #t
    > (exact? (+ 1 2))
    #t

This is a bit more interesting for real numbers.  E.g., Python, which uses
[IEEE-754][ieee-754] to represent floating point numbers, won't be able to
represent exact floats out-of-the-box.

    $ python
    >>> 1.1*1.1
    1.2100000000000002

This is also true in Scheme,

    > (exact? (* 1.1 1.1))
    #f

unless you mark them as exact,

    > (exact? (* #e1.1 #e1.1))
    #t

However, irrational numbers cannot be represented exactly in numerical form.

    > (import (scheme complex))
    > (exact? 1+1i)
    #t
    > (exact? (magnitude 1+1i))
    #f

There are some built-in operations that work with exact numbers. For instance,
`exact-integer-sqrt` will produce the integer square root and any remainder.

    > (exact-integer-sqrt 4)
    ((values) 2 0)
    > (exact-integer-sqrt 8)
    ((values) 2 4)

In Chibi Scheme, integers are exact by default, but you can mark them as
inexact. This is useful if you work with numbers that come from sources you
know to be inexact.  You do that with the `#i` prefix.

    > (exact? (* #e2 #i3))
    #f

You can work with binary and hexadecimal numbers as well.  Just add the `#b`
and `#x` prefixes, respectively.

    > #xff
    255
    > #b10101
    21
    > (exact? #i#xff)
    #f

You can also use `exact` and `inexact` to change the exactness of a number.

Here's how you represent different numbers:

<table class="table">
  <thead>
    <tr>
      <td>Type</td>
      <td>Syntax</td>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Integer</td>
      <td><code>123</code>, <code>#e3e10</code>, <code>(exact 3e10)</code></td>
    </tr>
    <tr>
      <td>Real</td>
      <td><code>1.21</code>, <code>1.3e12</code></td>
    </tr>
    <tr>
      <td>Rational</td>
      <td><code>1/3</code></td>
    </tr>
    <tr>
      <td>Complex</td>
      <td><code>2+3i</code></td>
    </tr>
  </tbody>
</table>

This forms the so-called numeric tower in Scheme.  Note that the standard
generally lets the implementations choose which numbers are supported.  A fully
compliant implementation may only support a well defined subset of these, for
instance.

Also, exact complex numbers are not required by the R7RS-small specification.
However, there was a vote on this for the R7RS-large specification.

Macros
------

Macros is a way to rewrite code and control evaluation.  It's very important to
remember that macros are *always* and *only* expanded at *compilation time*.

<i>
Scheme macros are *hygienic*.  It means that you'll when using identifiers
local to your macro, they will never collide with identifiers at run-time.
This is a good thing, but one downside is that you can't write <a
href="https://en.wikipedia.org/wiki/Anaphoric_macro">anaphoric macros</a>.
However, while R⁷RS only specifies hygienic macros using `syntax-rules`, most
implementations also provide a `defmacro` or other macro systems.
</i>

Let's create a new branching macro.  It will be called `when`, and we want to
be able to say `(when true-or-false do-something)`.

Why can't this be a function? Because we don't want to evaluate arguments in
the case that the predicate is false.  For instance, if `when` was a function,
then

    (when #false (format-harddrive))

would call `(format-harddrive)`.  I'll just show you how you can do this using
`syntax-rules` right now, and in a later update I'll explain what's going on.

    (define-syntax when
      (syntax-rules ()
        ((when test code ...)
         (if test (begin code ...)))))

Continuations
-------------

The Scheme standard only has *undelimited* continuations via `call/cc`, but
many implementations offer delimited continuations as well.

Matt Might has a really [good introduction to Scheme's built-in
continuations][matt.callcc].

This section will be covered later. Check back for updates!

Some Useful Data Structures
---------------------------

You don't only work with lists in Scheme. To use hash tables, you can import
the [SRFI-69][srfi-69] module.  SRFIs are requests for implementations to support
various stuff.  Most implementations support many of them, but you never know
which.  R7RS-large will incorporate some of them in the specification, though.

I won't cover much of them here, but you generally want to be able to build
hash tables. Here's how.

    > (import (srfi 69))
    > (define names (make-hash-table eq?))
    > (hash-table-set! names 'jmc "John McCarthy")
    > (hash-table-set! names 'gjs "Gerald Jay Sussman")
    > (hash-table-ref names 'jmc)
    "John McCarthy"

As you can see, I'm using symbolic keys:

    > (hash-table-keys names)
    (jmc gjs)

When you create a hash table using [SRFI-69][srfi-69], you need to tell how
keys are compared. We're just using `eq?` here.  You need to read up on the
[different equivalence predicates][eqv-preds] in the Scheme specification, as
they have different trade-offs.

Wrapping Up
-----------

That's all for now! But check back later for more updates.

[anaphoric]: https://en.wikipedia.org/wiki/Anaphoric_macro
[ast]: https://en.wikipedia.org/wiki/Abstract_syntax_tree
[chibi-scheme]: https://code.google.com/p/chibi-scheme/
[cowan-slides]: http://ccil.org/~cowan/scheme-2011-09.pdf
[cowan-video]: http://vimeo.com/29391029
[eqv-preds]: https://en.wikipedia.org/wiki/Scheme_(programming_language)#Equivalence_predicates
[ieee-754]: https://en.wikipedia.org/wiki/IEEE_floating_point
[matt.callcc]: http://matt.might.net/articles/programming-with-continuations--exceptions-backtracking-search-threads-generators-coroutines/
[spec]: http://trac.sacrideo.us/wg/raw-attachment/wiki/WikiStart/r7rs.pdf
[srfi-1]: http://srfi.schemers.org/srfi-1/srfi-1.html
[srfi-69]: http://srfi.schemers.org/srfi-69/srfi-69.html
[wingo-impls]: http://wingolog.org/archives/2013/01/07/an-opinionated-guide-to-scheme-implementations
]]></content:encoded>
      <dc:date>2015-01-01T10:58:04+00:00</dc:date>
    </item>
    <item>
      <title>Building Qt apps on the command line</title>
      <link>https://csl.name/post/qt-gcc/</link>
      <description><![CDATA[Most Qt tutorials seem to assume you want to use Qt Creator.  This shows you
how to build Qt apps on the command line, meaning you can experiment much
quicker while feeling that you&#39;re in control.
]]></description>
      <pubDate>Sun, 25 Nov 2012 00:00:00 +0000</pubDate>
      <guid>https://csl.name/post/qt-gcc/</guid>
      <content:encoded><![CDATA[Most Qt tutorials seem to assume you want to use Qt Creator.  This shows you
how to build Qt apps on the command line, meaning you can experiment much
quicker while feeling that you're in control.

Create a directory `foo` and then put the code below into `foo.cpp`.

{% highlight C++ %}
#include <qapplication.h>
#include <qpushbutton.h>

int main(int argc, char** argv)
{
  QApplication app(argc, argv);

  QPushButton hello("Hello world!", 0);
  hello.resize(100, 30);
  hello.show();

  return app.exec();
}
{% endhighlight %}

Using `qmake`, set up a new Qt project and create a GNU makefile.

    $ qmake -project -o foo.pro

With newer versions of Qt, you have to edit `foo.pro` after creating it. Add
the line `qt += WIDGETS` to be able to compile:

    TEMPLATE = app
    TARGET = foo
    INCLUDEPATH += .
    QT += widgets # <== Add this

    # Input
    SOURCES += foo.cpp

Now, generate a makefile:

    $ qmake -makefile
    $ ls
    Makefile foo.cpp foo.pro

If you use other parts of Qt, for example networking, you may have to add
additional such lines (`QT += networking`). In that case, you have to recreate
the makefile with the above command.

Now you can simply `make` the app.

    $ make

It depends on the system what the final binary will be called.  On Mac OS X, it
will be `foo.app`, and on Windows I guess it will be `foo.exe`. To open the
application bundle on OS X, do

    $ open foo.app

The result should look similar to the image below.

<img class="u-max-full-width"
     src="/gfx/post/qt-gcc.png"
     alt="A 'Hello, world!' desktop application made with Qt">

Using the meta-object compiler
==============================

You'll soon want to use the meta-object compiler to be able to play with slots
and signals. The makefile already supports this, but you have to explicitly
call the target to build the `.moc` files. Type

    $ make mocables

to build them. If you copy the below code, this command will create `foo.moc`.
After that, you need to `#include "foo.moc"` at the end of `foo.cpp` to make
sure the linker doesn't complain about missing vtables.

Here's a really simple example to get you going:

    #include <qapplication.h>
    #include <qdialog.h>
    #include <qmessagebox.h>
    #include <qobject.h>
    #include <qpushbutton.h>

    class MyApp : public QDialog {
      Q_OBJECT
    public:
        MyApp(QObject* /*parent*/ = 0):
          button(this)
        {
          button.setText("Hello world!");
          button.resize(100, 30);

          // When the button is clicked, run button_clicked
          connect(&button, &QPushButton::clicked, this, &MyApp::button_clicked);
        }

    public slots:
        void button_clicked() {
          QMessageBox box;
          box.setWindowTitle("Howdy");
          box.setText("You clicked the button");
          box.show();
          box.exec();
        }

    protected:
      QPushButton button;
    };

    int main(int argc, char** argv)
    {
      QApplication app(argc, argv);

      MyApp myapp;
      myapp.show();

      return app.exec();
    }

    #include "foo.moc"

Compile with

    $ make mocables all
    $ open foo.app

This will give you a window with a single button, which opens a pop-up window
when clicked.

You can hack the makefile to automatically build `foo.moc`, but since it's
automatically generated, I suggest you either just build the mocables
explicitly, or find out how to add the moc targets to `foo.pro`.
]]></content:encoded>
      <dc:date>2012-11-25T00:00:00+00:00</dc:date>
    </item>
    <item>
      <title>Rendering the Mandelbrot Set</title>
      <link>https://csl.name/post/mandelbrot-rendering/</link>
      <description><![CDATA[In 2012, I made a Mandelbrot renderer in
JavaScript. Here I will explain how you can do it yourself, including how
to implement smooth coloring and making it fast.
]]></description>
      <pubDate>Wed, 22 Feb 2012 00:00:00 +0000</pubDate>
      <guid>https://csl.name/post/mandelbrot-rendering/</guid>
      <content:encoded><![CDATA[In 2012, I made a <a href="https://csl.name/mandelbrot/">Mandelbrot renderer in
JavaScript</a>. Here I will explain how you can do it yourself, including how
to implement smooth coloring and making it fast.

<p>
  <img class="u-max-full-width"
       src="/gfx/post/mandelbrot-2.jpg"
       alt="The Mandelbrot Set" />
</p>

Theory
------

The famous [Mandelbrot set](https://en.wikipedia.org/wiki/Mandelbrot_set) is
a set of points in the complex plane.  In essence, what we want to find out
is if the iterative function C below will _converge_ to some constant or _diverge_
to infinity.

The definition is simply

>  z<sub>n+1</sub> = z<sub>n</sub><sup>2</sup> + c

with the initial condition simply formed by taking the coordinates in the
complex plane,

>  z<sub>0</sub> = c = x + iy

Pretend for a moment that you've never seen the Mandelbrot plot before.
By only looking at the definition above, can you guess how it would look like when
rendered? First we need to look at a few expansions of z expressed with 
<span>x</span> and <span>y</span>.

> z<sub>0</sub> = x + iy

> z<sub>1</sub> = (x + iy)<sup>2</sup> + (x + iy) =
>  (x<sup>2</sup> - y<sup>2</sup> + x) + i(2xy + y)

> z<sub>2</sub> = z<sub>1</sub><sup>2</sup> + (x + iy) = ...

So, as <span>n &rarr; &infin;</span>, for which values of <span>x</span> and
<span>y</span> will <span>z<sub>n</sub></span> converge, and for which will it
diverge?

Well, for large values of <span>|x|</span> and small for <span>|y|</span>
(points close to origo), the sequence might diverge, because
<span>x<sup>2</sup></span> would be dominating over <span>y<sup>2</sup></span>.
Diverged points are painted black, so we can guess that the plot will be black
in all directions some distance from origo.

<div>
<i>
<b>Note:</b> The mathematical description here is very imprecise.

Starting with <span>x</span> and <span>y</span>, for each iteration you'll get
a new expression <span>a + ib</span>, i.e. a new point at <span>(a,b)</span>.
As you iterate, you jump to new points in the complex plane.  For
convergent sequences, we will jump closer and closer to an attractor point
&mdash; usually curving towards it instead of a straight line.

For the Mandelbrot set, it means that if
any point lands outside a circle with radius two around origo, the sequence
will diverge.

<p class="u-pull-right">
  <a href="https://en.wikipedia.org/wiki/Complex_quadratic_polynomial#Critical_orbit">Read more on Wikipedia</a>
</p>
</i>
</div>

But for points close to origo &mdash; say, <span>|x|</span> and
<span>|y|</span> less than 1 &mdash; we would expect it to converge, because
the product of two numbers less than one will always be smaller than each of
its parts, giving small values for <span>x<sup>2</sup> - y<sup>2</sup></span>.  So in a
circle around origo with a radius of one, we'd expect to see colored pixels.

But what if the signs of x and y differ? Then things quickly get more
complicated.  In fact, if we plot using a computer, we won't get a
nice colored disc centered at origo.  What we get is an infinitely complex and
fractured plot.

We can even zoom endlessly into the plot and it will <em>still</em> be as
non-uniform and complex as before.  While a disc would have a dimension of two,
a fractal has a so-called so-called [Hausdorff dimension](http://en.wikipedia.org/wiki/Hausdorff_dimension)
which is something in between a line and a plane.  We'd get a non-integer
dimension; a *fractal*.

Calculating the Mandelbrot Set
------------------------------

Calculating the Mandelbrot set numerically is easy.

Given the equations above, take any point <span>z<sub>0</sub> = (x, y)</span>
and then calculate <span>z<sub>1</sub> = (x+iy)<sup>2</sup> + (x+iy)</span> and
continue doing this.  For practical purposes, let's decide on a _threshold_
value.  If the magnitude of <span>z</span> &mdash; the distance to origo, or
<span>&radic;(x^2+y^2)</span>&mdash; ever becomes larger than this threshold
value we will assume that it will diverge into infinity.  If so, stop the
calculation and plot a _black dot_ at the current location.

<i>
For the Mandelbrot set, it can be shown that the threshold value is exactly two, i.e.
any sequence with a <span>|z<sub>n</sub>| > 2</span> will diverge.
<span class="u-pull-right">
  <a href="http://www.mi.sanu.ac.rs/vismath/javier/b2.htm">Read more about this</a>.
</span>
</i>

If <span>|z|</span> has not exceeded the threshold value after a predecided
number of iterations (which we choose at will), we will assume that the current
parameters makes the function converge.  In this case, plot a non-black dot at
the current location.

Colorizing the plot
-------------------

I said above that if the function diverges, one should plot a non-black dot.
One could simply paint a white dot here.  But instead, maybe we want to get
an idea of _how fast_ the function is diverging to infinity at this point.

To do this, just take the current value of the number of steps performed
and _map_ that against a color spectrum, and paint that color.

So, functions diverging quickly will get about the same color.

Smooth coloring
---------------

If you use the number of iterations to pick a color, you'll get ugly color
bands in the plot.  There is a really cool trick to get smooth, gradual
color changes.

So, you <em>basically</em> calculate `Z = Z^2` until it diverges and make a note of
the iteration count.  What we really want, though, is a _fractional_
iteration count, so we can multiply that with a color value to get smooth
colors.

The trick is to note that when you calculate `Z = Z^2` you'll get values `Z,
Z^2, Z^4, Z^8` and so on.  If you take the logarithm of this, you'll get the
values 1, 2, 4, 8 etc.  If you take the logarithm one more time, you'll get
1, 2, 3, 4, 5 etc.  So to get a fractional number of iterations, just do:

    log(log |Z|) / log 2

This is all explained over at [linas.org](http://linas.org/art-gallery/escape/smooth.html).

In my code, I originally used the following smoothing equation:

    1 + n - Math.log(Math.log(Math.sqrt(Zr*Zr+Zi*Zi)))/Math.log(2.0);

With some elementary logarithm rules, we can simplify this to

    // Some constants
    var logBase = 1.0 / Math.log(2.0);
    var logHalfBase = Math.log(0.5)*logBase;
    // ...
    return 5 + n - logHalfBase - Math.log(Math.log(Tr+Ti))*logBase;

which is faster.  The constant `5` is another little trick, which should
be explained in the code itself.

Anti-aliasing and supersampling
-------------------------------

Finally, when you calculate the color value of a single pixel, it is in
reality just the color of a single point in the Mandelbrot set that is
situated somewhere _inside_ that pixel.

What I'm saying is that you'll basically get pixel artifacts in the image,
especially in dense areas where the color changes (near the black set, for
instance).

So what I do is to use random sampling:  Just sample a given number of
random points inside the pixel and average the sum of the color values.
This is equivalent to rendering the plot at a higher resolution and scaling
down.

There are many supersampling techniques to use, and the random sampling was
chosen because of its simplicity.  The problem is that the resulting picture
will look a bit blurry (there are ways around this as well).

Optimizing the calculation for performance
==========================================

Calculating the Mandelbrot set is quite slow, but there are a lot of tricks
to speed it up.

When speeding up any code, the first step (after making the code _correct_,
of course) is to look at the algorithm and try to use one with a simpler
complexity class.  Unfortunately, for the Mandelbrot set, this isn't really
possible.  So the tricks mentioned here are all cases of 
_micro-optimizations_.  Nevertheless, they will improve the running time
quite a lot.

We also have to remember that we're using Javascript here, which is a
relatively slow language because of its dynamic nature.  What's interesting
in this regard, though, is to identify performance hot spots in the typical
javascript engines.  It's interesting to test the code on different browsers.

Removing the square root operation
----------------------------------

First, let's look at the inner loop.  It continually calculates the
magnitude of the complex number C, and compares this with a threshold value.
Observe that it takes the square root in doing so:

    if ( sqrt(x^2 + y^2) > threshold ) ...

If we just square the treshold value, we should be able to do away with the
square root operation:

    threshold_squared = threshold^2
    // ...
    if ( (x^2 + y^2) > threshold_squared ) ...

Taking advantage of symmetry
----------------------------

You've probably noticed that the plot is reflected vertically over the line
<span>y = 0</span>.  You can take advantage of this mirroring to halve the
computation time. I don't, because you'll mostly render plots that are
massively zoomed in.

Splitting up the main equation
------------------------------

The main equation is

> <span>z<sub>n+1</sub> = z<sub>n</sub><sup>2</sup> + c</span>

Setting `C = z` and `Cr = Re(z)` and `Ci = Im(z)`, we get

    C_{n+1} = Cr^2 + 2Cr*Ci*i - Ci*Ci + C_{0}
    C_{n+1} = (Cr^2 - Ci^2) + i(2Cr*Ci) + C_{0}

giving us

    Re (C_{n+1}) = Cr^2 - Ci^2 + x
    Im (C_{n+1}) = 2*Cr*Ci + y
    Mag(C_{n+1}) = sqrt(Cr^2 + Ci^2)

If we introduce two variables `Tr = Cr^2` and `Ti = Ci^2`, we get

    Re (C_{n+1})   = Tr - Ti + x
    Im (C_{n+1})   = 2*Cr*Ci + y
    Mag(C_{n+1})^2 = Tr + Ti
    Tr             = Re(C_{n+1}))^2
    Ti             = Im(C_{n+1}))^2

So we have now replaced some multiplications with additions, which is
normally faster in most CPUs.  But, again, this is javascript, and
javascript has quite a different performance profile.  The code above indeed
does _not_ give us any **significant** speedup --- for a 640x480 image, we
only save a hundred milliseconds, or so.

Fast indexing into the image data struct
----------------------------------------

To plot individual pixels in HTML5 canvas, you get an array and you have to
calculate the array offset for a given coordinate pair.

I.e., given RGBA pixel format (four positions), an (x, y) coordinate pair
and a width and height, you calculate it by

    offset = 4*x + 4*y*width

so that you can now set the RGBA values as

    array[offset+0] = red
    array[offset+1] = green
    array[offset+2] = blue
    array[offset+3] = alpha

There are several ways of optimizing this.  For instance, we can simply
multiply the whole offset by four, which is the same as shifting all bits
left two positions.  However, javascript works in mysterious ways, so the
customary shift operations may not be as fast as in other languages like C
and C++.  The reason _probably_ has to do with the fact that javascript only
has _one_ data type for numbers, and my guess is that it's some kind of
float.

Anyway, we now have

    offset = (x + y*width) << 2

Another trick I'd like to mention.  Say that the width and height are fixed
to, say 640 and 480, respectively.  And old trick to multiply y by 640 would
be notice that 640 = 512 + 128 = 2^9 + 2^7, giving us

    y*640 = y*512 + y*128 = y*2^9 + y*2^7 = y<<9 + y<<7

So now we've converted one multiplication into two shifts and an add.  In
your commodity language and hardware, this might be quite fast in tight
innerloops.

Anyway, we still want to be able to use arbitrary heights and widths, so
let's skip that one.

By far, the fastest way of accessing the array is by doing it sequentially.

That is, instead of doing

    for ( y=0; y<height; ++y )
    for ( x=0; x<width; ++x ) {
      // calculate RGBA
      var offset = 4*(x + y*with);
      image.data[offset + 0] = R;
      image.data[offset + 1] = G;
      image.data[offset + 2] = B;
      image.data[offset + 3] = A;
    }

a _much_ faster way would be to do

    var offset = 0;
    for ( y=0; y<height; ++y )
    for ( x=0; x<width; ++x ) {
      image.data[offset++] = R;
      image.data[offset++] = G;
      image.data[offset++] = B;
      image.data[offset++] = A;
    }

So now we've basically saved the work of doing `2*width*height`
multiplications, or 600 thousand of them, assuming a 640x480 image.

Fast copying of the image data
------------------------------

To draw in the canvas, you request an array, update it and copy it back to
the canvas.

Of course, you want to reduce the number of such operations.  Because we
want an animation showing each line as it is drawn, we'll do this:

  * Get an image data array
  * For each line: Update the array
  * For each line: Copy the array back to the canvas

The trick here, though is to _not_ use `getImageData`.  You're going to
overwrite all existing image data, so you can use the same buffer for every
line.  So instead, we'll use these operations:

  * Get a line buffer by calling `createImageData(canvas.width, 1)`
  * For each line: Update the line buffer array
  * For each line: Call `putImageData(linebuffer, 0, y_position)` to copy only _one_ line

This ensures that we only copy _one_ line per frame update.

Embarrassingly parallel
-----------------------

Since the algorithm above is referentially transparent, meaning that it
always produces the same output for the same input (where input is defined
as `x, y, steps, threshold`), you could in theory calculate all points in
parallel.

Such algorithms are colloquially called
[embarrassingly parallel](http://en.wikipedia.org/wiki/Embarrassingly_parallel).

Now, JavaScript is inherently single-threaded:  You can only use so-called
green threads, meaning that the javascript engine will yield control between
them.

However, there is a new HTML5 APi called web workers that you can use to
create real, OS-level threads.  That should make it easy to split up
plotting into several threads (preferrably one per logical core).

Using vectorized procedures
---------------------------

The algorithm is very well suited for vector operations.  Most modern computers
come with hardware optimizations for such operations (SSE or the GPU, etc.)
However, we are again limited to what the javascript engines will do for us.

Even more optimizations
-----------------------

Take a look at the optimizations done to the Mandelbrot set in
[The Computer Language Benchmarks Game](http://shootout.alioth.debian.org/u32/performance.php?test=mandelbrot)

There are a lot of cool tricks going on there.  Most of _those_ use SSE
parallelism for hardware speedup or offloads to the GPU.
]]></content:encoded>
      <dc:date>2012-02-22T00:00:00+00:00</dc:date>
    </item>
    <item>
      <title>Two quines in C</title>
      <link>https://csl.name/post/two-quines/</link>
      <description><![CDATA[A quine is a program that, when run, produces an exact copy of its original
source code.
]]></description>
      <pubDate>Sat, 24 Dec 2011 11:35:05 +0000</pubDate>
      <guid>https://csl.name/post/two-quines/</guid>
      <content:encoded><![CDATA[A quine is a program that, when run, produces an exact copy of its original
source code.

In [Reflections on trusting trust][trust], [Ken Thompson][ken] urges the
reader to try write a quine.  If you haven't done so yet, please try before
reading on.

Here are two quines I made wrote in C&mdash;without cheating! They are both
based on the same idea. The first one is macro&dash;based, utilizing the
ability for macros to quote parameters.

{% highlight C++ %}
#define Q(x) #x;x
char *q=Q(main(){printf("#define Q(x) #x;x\nchar *q=Q(");printf(q);printf(")\n");})
{% endhighlight %}

Compiling and running it produces an exact copy of itself:

    $ gcc --no-warnings shortq.c -o shortq && ./shortq
    #define Q(x) #x;x
    char *q=Q(main(){printf("#define Q(x) #x;x\nchar *q=Q(");printf(q);printf(")\n");})

In the next quine I tried to do away with macros.  First I tried quoting
program code, but then I had to quote quotation characters, and for each
compilation of the next quine, the quotation degraded quickly.  The trick I
used was just to use `putchar` instead.  Not extremely elegant, but simple
to understand.

{% highlight C++ %}
char*p="main(){putchar(99);putchar(104);putchar(97);putchar(114);putchar(42);putchar(112);putchar(61);putchar(34);printf(p);putchar(34);putchar(59);putchar(10);put
s(p);}";
main(){putchar(99);putchar(104);putchar(97);putchar(114);putchar(42);putchar(112);putchar(61);putchar(34);printf(p);putchar(34);putchar(59);putchar(10);puts(p);}
{% endhighlight %}

Compiling and running the above program produces

{% highlight bash %}
$ gcc --no-warnings putchar-quine.c -o p && ./p
char*p="main(){putchar(99);putchar(104);putchar(97);putchar(114);putchar(42);putchar(112);putchar(61);putchar(34);printf(p);putchar(34);putchar(59);putchar(10);puts(p);}";
main(){putchar(99);putchar(104);putchar(97);putchar(114);putchar(42);putchar(112);putchar(61);putchar(34);printf(p);putchar(34);putchar(59);putchar(10);puts(p);}
{% endhighlight %}

Now, let's ponder the last example for a bit. You can see that I'm just
writing out a bunch of numerical codes, with some boilerplate code to get it
all started. In other words, we have some <i>driver code</i> and then a
<i>payload</i> of specific data we print out. We can actually create a quine
out of <i>any</i> program. That is the main point in Thompson's article.

This document is the HTML&dash;version of [a gist][gist] I made earlier.

[gist]: https://gist.github.com/1517172
[trust]: http://cm.bell-labs.com/who/ken/trust.html
[ken]: https://en.wikipedia.org/wiki/Ken_Thompson
]]></content:encoded>
      <dc:date>2011-12-24T11:35:05+00:00</dc:date>
    </item>
    <item>
      <title>Palindromes in The Gettysburg Address and Shakespeare&apos;s collected works</title>
      <link>https://csl.name/post/palindromes-gettysburg-shakespeare/</link>
      <description><![CDATA[Some years ago, I solved some programming puzzles posted by Greplin (later
Cue, then acquired by Apple).
]]></description>
      <pubDate>Thu, 03 Mar 2011 17:10:27 +0000</pubDate>
      <guid>https://csl.name/post/palindromes-gettysburg-shakespeare/</guid>
      <content:encoded><![CDATA[Some years ago, I [solved some programming puzzles][gist] posted by Greplin (later
Cue, then acquired by Apple).

The task was to find the longest [palindrome][palindrome]&mdash;words or
sentences that read the same forwards and backwards&mdash;in Abraham
Lincoln's [Gettysburg Address][gettysburg].

Of course, the code had to be as fast and elegant as possible.  (Correct
submissions lead to recruitment talks. I did talk with them, but as a
foreigner I did not have any US worker's visa, and they were too small to
have any time and money to spend on getting me one.)

Anyway, there are many algorithms for doing this. Mine is just based on a brute
force scan of the text. I see I claim that the running time is linear on the
average input and quadratic in the worst case. Update: Today I would have used
the word *in practice* instead of *on average*, because there really is nothing
fancy about this code (except that it's a bit cute).

{% highlight C++ %}
/*
 * Find the longest palindrome in the text.
 *
 * This is Greplin's first challenge, and I originally solved it in Python.
 *
 * This algorithm is linear on the average input, but has a quadratic
 * worst case running time.  There exists an even better algorithm, but
 * this should do.
 *
 * There might also be a small error below, but you get the general idea.
 *
 * Christian Stigen Larsen
 */

#include <stdio.h>

static const char text[] =
  "Fourscoreandsevenyearsagoourfaathersbroughtforthonthiscontainentanewnati"
  "onconceivedinzLibertyanddedicatedtothepropositionthatallmenarecreatedequ"
  "alNowweareengagedinagreahtcivilwartestingwhetherthatnaptionoranynartions"
  "oconceivedandsodedicatedcanlongendureWeareqmetonagreatbattlefiemldoftzha"
  "twarWehavecometodedicpateaportionofthatfieldasafinalrestingplaceforthose"
  "whoheregavetheirlivesthatthatnationmightliveItisaltogetherfangandpropert"
  "hatweshoulddothisButinalargersensewecannotdedicatewecannotconsecrateweca"
  "nnothallowthisgroundThebravelmenlivinganddeadwhostruggledherehaveconsecr"
  "ateditfaraboveourpoorponwertoaddordetractTgheworldadswfilllittlenotlenor"
  "longrememberwhatwesayherebutitcanneverforgetwhattheydidhereItisforusthel"
  "ivingrathertobededicatedheretotheulnfinishedworkwhichtheywhofoughthereha"
  "vethusfarsonoblyadvancedItisratherforustobeherededicatedtothegreattdafsk"
  "remainingbeforeusthatfromthesehonoreddeadwetakeincreaseddevotiontothatca"
  "useforwhichtheygavethelastpfullmeasureofdevotionthatweherehighlyresolvet"
  "hatthesedeadshallnothavediedinvainthatthisnationunsderGodshallhaveanewbi"
  "rthoffreedomandthatgovernmentofthepeoplebythepeopleforthepeopleshallnotp"
  "erishfromtheearth";

void print_best(const char* l, const char* r)
{
  static int best = 2;

  if ( r-l > best )
    printf("%.*s\n", (best=r-l), l);
}

int main()
{
  const char *l, *r;

  for ( const char *p = text + 1; *p; ++p ) {
    l = p; r = p - 1;
    while ( *--l == *++r );

    print_best(l+1, r);

    l = r = p;
    while ( *--l == *++r );

    print_best(l+1, r);
  }

  return 0;
}
{% endhighlight %}

The output is

{% highlight bash %}
$ gcc -O3 -W -Wall gettysburg.c -ogetty && time ./getty
eve
ranynar

real  0m0.004s
user  0m0.001s
sys   0m0.002s
{% endhighlight %}

So the longest palindrome in The Gettysburg Address is <i>ranynar</i>.

The above code is quite fast as well. It runs through the complete works of
Shakespeare in about 0.03 seconds of wall clock time, or a processing rate
of 128 Mb/second, and grows linearly with the input. It has a worst case
running time which is quadratic, though; if you feed it a string of same
characters, for instance.

To find the palindromes in Shakespeare's collected works, I first modified
the code to load the text from disk. I changed `print_best` to print all
palindromes equal to or longer than the current best, so we get a longer
list of palindromes. I also had to prepare Shakespeare's collected works
into a format suitable for processing: First I removed the name of which
character is speaking (done by `sed`), then I converted all text to lowercase
and deleted all non-alphanumeric characters.

Since Shakespeare wrote his plays more than 300 years before copyright law
was invented, you can download and use it freely. I got mine off a site with
all the stuff in different directories. Here's what I did to prepare them,
all in one jolly line of UNIX goodness:

{% highlight bash %}
$ find shakespeare/ -type f \        # for all files
    | xargs sed 's/^[A-Za-z]*//g' \  # only keep alphabetical chars
    | tr A-Z a-z \                   # translate to lowercase
    | tr -dC a-z \                   # remove anything BUT alphabetical chars
    > shakespeare.txt                # output to one big file
{% endhighlight %}

I compiled my code and ran it on the file:

{% highlight bash %}
$ llvm-g++ -O4 -flto pali.cpp -o pali
$ cat shakespeare.txt | time ./pali
ll
ll
ll
ama
lonanol
tomymot
withtiw
iwerewi
erewere
tarorat
rownwor
sieveis
tomymot
imadami
madammadam
ereherehere
reherehereher
hereherehereh
illitmadamtilli
madammadammadam
madammadammadam

real    0m0.026s
user    0m0.022s
sys     0m0.004s
{% endhighlight %}

The longest palindromes appear in the lines:

> GLOUCESTER
>
> So w<mark>ill it madam till I</mark> lie with you.

from the play [Richard III][rich3]. The preparsing of the collected works
above includes the character's name. Without it, the output is a bit
different:

{% highlight bash %}
$ find shakespeare/ -type f \
    | xargs cat \
    | sed -e 's/^[A-Za-z]*//g' \
    | tr A-Z a-z \
    | tr -dC a-z > collected-works-no-names.txt

$ cat collected-works-no-names.txt | ./pali
ama
lonanol
nymedemyn
ereherehere
reherehereher
illitmadamtilli
{% endhighlight %}

The second last character-by-character palindrome comes from the tragedy
[Troilus and Cressida][cressida]:

> PANDARUS
>
> Hark! they are coming from the field: shall we
> stand up here, and see them as they pass toward
> Ilium? good niece, do, sweet niece Cressida.
>
> CRESSIDA
>
> At your pleasu<mark>re</mark>.
>
> PANDARUS
>
> <mark>Here, here, her</mark>e's an excellent place; here we may
> see most bravely: I'll tell you them all by their
> names as they pass by; but mark Troilus above the rest.

You can see it ignores a punctuation mark, which I think is fine.

There are probably other, much better algorithms for finding palindromes. If
you're interested in <em>generating</em> palindromes, you should check out
[Peter Norvig's supposed world record palindrome][norvig].

[gist]: https://gist.github.com/cslarsen/851611
[palindrome]: https://en.wikipedia.org/wiki/Palindrome
[gettysburg]: https://en.wikipedia.org/wiki/Gettysburg_Address
[norvig]: http://norvig.com/palindrome.html
[cressida]: https://en.wikipedia.org/wiki/Troilus_and_Cressida
[rich3]: https://en.wikipedia.org/wiki/Richard_III_(play)
]]></content:encoded>
      <dc:date>2011-03-03T17:10:27+00:00</dc:date>
    </item>
    <item>
      <title>A UDP syslog client in Python</title>
      <link>https://csl.name/post/python-syslog-client/</link>
      <description><![CDATA[While the Python standard library offers a syslog module,
it seem to be a wrapper around the POSIX syslog system
calls. This means you cannot use it to send syslog messages
over the network.
]]></description>
      <pubDate>Fri, 14 Nov 2008 09:53:22 +0000</pubDate>
      <guid>https://csl.name/post/python-syslog-client/</guid>
      <content:encoded><![CDATA[While the Python standard library offers a [syslog][python.syslog] module,
it seem to be a wrapper around the [POSIX syslog system
calls][posix.syslog]. This means you cannot use it to send syslog messages
over the network.

The code below implements a `send` function as described in 
[RFC 3164][rfc3164]. It has been used in production on Windows boxes sending
messages to a Linux syslog server.

For this to work you must configure your syslog daemon to accept logs from
the network.

{% highlight python %}
{% include python/syslog_client.py %}
{% endhighlight %}

If you put it in a file `syslog_client.py` you can use it as a module.

{% highlight python %}
>>> import syslog_client
>>> log = syslog_client.Syslog("remote-host-name")
>>> log.send("howdy", syslog_client.WARNING)
{% endhighlight %}

You can easily extend the class in several ways. E.g., you may want to add
some convenience functions like `warn()`, etc.

[rfc3164]: http://www.ietf.org/rfc/rfc3164.txt
[python.syslog]: http://docs.python.org/2/library/syslog.html
[posix.syslog]: http://pubs.opengroup.org/onlinepubs/007904975/basedefs/syslog.h.html
]]></content:encoded>
      <dc:date>2008-11-14T09:53:22+00:00</dc:date>
    </item>
    <item>
      <title>Einstein's problem and a solution by elimination</title>
      <link>https://csl.name/post/einsteins-puzzle/</link>
      <description><![CDATA[
The Zebra Puzzle is a
famous puzzle that has been said to have been invented by Einstein. It is not.
Neither is it true that only 2% of people can solve it. But it is a very fun
puzzle and I invite everyone to try to solve it on their own.
]]></description>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://csl.name/post/einsteins-puzzle/</guid>
      <content:encoded><![CDATA[<p class="lead">
The <a href="https://en.wikipedia.org/wiki/Zebra_Puzzle">Zebra Puzzle</a> is a
famous puzzle that has been said to have been invented by Einstein. It is not.
Neither is it true that only 2% of people can solve it. But it *is* a very fun
puzzle and I invite everyone to try to solve it on their own.

Below I sketch out how I did it.
</p>

The Puzzle
----------

* There are five houses in unique colors: Blue, green, red, white and
  yellow.
* In each house lives a person of unique nationality: British, Danish,
  German, Norwegian and Swedish.
* Each person drinks a unique beverage: Beer, coffee, milk, tea and water.
* Each person smokes a unique cigar brand: Blue Master, Dunhill, Pall Mall,
  Prince and blend.
* Each person keeps a unique pet: Cats, birds, dogs, fish and horses.

The following facts are given:

1.  The Brit lives in a red house.
2.  The Swede keeps dogs as pets.
3.  The Dane drinks tea.
4.  The green house is on the left of the white, next to it.
5.  The green house owner drinks coffee.
6.  The person who smokes Pall Mall rears birds.
7.  The owner of the yellow house smokes Dunhill.
8.  The man living in the house right in the center drinks milk.
9.  The Norwegian lives in the first house.
10. The man who smokes blend lives next to the one who keeps cats.
11. The man who keeps horses lives next to the man who smokes Dunhill.
12. The owner who smokes Blue Master drinks beer.
13. The German smokes Prince.
14. The Norwegian lives next to the blue house.
15. The man who smokes blend has a neighour who drinks water.

The question you need to answer is: **Who keeps fish?**

Try to work on this problem yourself before looking at the solution! It is
really not that hard once you find a good approach.

A Solution
----------

Given facts 8 and 9, we can safely assume that the houses are positioned in
a row. This lets us set up a table to hold all the information we find.

We will use the following codes for the various pets, cigar brands and so
on:

* **b**lue, **g**reen, **r**ed, **w**hite, **y**ellow
* **b**ritish, **d**anish, **g**erman, **n**orwegian, **s**wede
* **b**eer, **c**offee, **m**ilk, **t**ea, **w**ater
* **b**lue master, **d**unhill, **p**all mall, p**r**ince, b**l**end
* **b**irds, **c**ats, **d**ogs, **f**ish, **h**orses

Instead of starting with an empty table and filling in the possible values,
I found it much easier to just insert all possibilities up front. We will
then eliminate possible values in each cell as we read the facts. This can
easily be done with pen and paper.

<table class="table">
	<tr>
		<td>Colour</td>
		<td>bgrwy</td>
		<td>bgrwy</td>
		<td>bgrwy</td>
		<td>bgrwy</td>
		<td>bgrwy</td>
	</tr>
	<tr>
		<td>Nationality</td>
		<td>bdgns</td>
		<td>bdgns</td>
		<td>bdgns</td>
		<td>bdgns</td>
		<td>bdgns</td>
	</tr>
	<tr>
		<td>Beverage</td>
		<td>bcmtw</td>
		<td>bcmtw</td>
		<td>bcmtw</td>
		<td>bcmtw</td>
		<td>bcmtw</td>
	</tr>
	<tr>
		<td>Cigar</td>
		<td>bdprl</td>
		<td>bdprl</td>
		<td>bdprl</td>
		<td>bdprl</td>
		<td>bdprl</td>
	</tr>
	<tr>
		<td>Pet</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
	</tr>
</table>

First up we simply eliminate possibilities by using facts 8, 9 and 10.

<table class="table">
	<tr>
		<td>Colour</td>
		<td>grwy</td>
		<td>b</td>
		<td>grwy</td>
		<td>grwy</td>
		<td>grwy</td>
	</tr>
	<tr>
		<td>Nationality</td>
		<td>n</td>
		<td>bdgs</td>
		<td>bdgs</td>
		<td>bdgs</td>
		<td>bdgs</td>
	</tr>
	<tr>
		<td>Beverage</td>
		<td>bctw</td>
		<td>bctw</td>
		<td>m</td>
		<td>bctw</td>
		<td>bctw</td>
	</tr>
	<tr>
		<td>Cigar</td>
		<td>bdprl</td>
		<td>bdprl</td>
		<td>bdprl</td>
		<td>bdprl</td>
		<td>bdprl</td>
	</tr>
	<tr>
		<td>Pet</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
	</tr>
</table>

The Brit lives in a red house (fact 1), so remove colour option r from all
houses which doesnt have **b**rit as a possible nationality.

<table class="table">
	<tr>
		<td>Colour</td>
		<td>gwy</td>
		<td>b</td>
		<td>grwy</td>
		<td>grwy</td>
		<td>grwy</td>
	</tr>
	<tr>
		<td>Nationality</td>
		<td>n</td>
		<td>dgs</td>
		<td>bdgs</td>
		<td>bdgs</td>
		<td>bdgs</td>
	</tr>
	<tr>
		<td>Beverage</td>
		<td>bctw</td>
		<td>bctw</td>
		<td>m</td>
		<td>bctw</td>
		<td>bctw</td>
	</tr>
	<tr>
		<td>Cigar</td>
		<td>bdprl</td>
		<td>bdprl</td>
		<td>bdprl</td>
		<td>bdprl</td>
		<td>bdprl</td>
	</tr>
	<tr>
		<td>Pet</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
	</tr>
</table>

Fact 4 says the green house is on the left side, next to the white.
So the first house cannot be green or white, leaving yellow as the
only possibility.  The last house cannot be green.  By fact 5, remove
green as an option for the center house.  This house also cannot be
white, due to fact 4, leaving the colour red.  This also leads us to
conclude that the second and last houses are green and white, respectively.

We can now insert **b** for Brit in the red house.

<table class="table">
	<tr>
		<td>Colour</td>
		<td>y</td>
		<td>b</td>
		<td>r</td>
		<td>g</td>
		<td>w</td>
	</tr>
	<tr>
		<td>Nationality</td>
		<td>n</td>
		<td>dgs</td>
		<td>b</td>
		<td>dgs</td>
		<td>dgs</td>
	</tr>
	<tr>
		<td>Beverage</td>
		<td>btw</td>
		<td>btw</td>
		<td>m</td>
		<td>c</td>
		<td>btw</td>
	</tr>
	<tr>
		<td>Cigar</td>
		<td>bdprl</td>
		<td>bdprl</td>
		<td>bdprl</td>
		<td>bdprl</td>
		<td>bdprl</td>
	</tr>
	<tr>
		<td>Pet</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
		<td>bcdfh</td>
	</tr>
</table>

Continuing with fact 3, we remove the Dane as an option for the green house.
Also remove tea from the first house.  Fact 2 says the Swede keeps dogs, so
remove d for dogs in all houses which doesn't have **S**wede as an option.
Fact 7 says the Norwegian smokes Dunhill, and fact 10 says the next house
keeps horses.

Fact 12 says the one smoking Blue Master drinks beer, so remove b from all
houses that don't match &mdash; therefore the Norwegian must drink water.
We will also remove b for Blue Master in houses that doesn't have a beer
option.

The Swede keeps dogs, so remove s for Swede from the blue house, which has
horses.

<table class="table">
	<tr>
		<td>Colour</td>
		<td>y</td>
		<td>b</td>
		<td>r</td>
		<td>g</td>
		<td>w</td>
	</tr>
	<tr>
		<td>Nationality</td>
		<td>n</td>
		<td>dg</td>
		<td>b</td>
		<td>gs</td>
		<td>dgs</td>
	</tr>
	<tr>
		<td>Beverage</td>
		<td>w</td>
		<td>bt</td>
		<td>m</td>
		<td>c</td>
		<td>bt</td>
	</tr>
	<tr>
		<td>Cigar</td>
		<td>d</td>
		<td>bprl</td>
		<td>prl</td>
		<td>prl</td>
		<td>bprl</td>
	</tr>
	<tr>
		<td>Pet</td>
		<td>bcf</td>
		<td>h</td>
		<td>bcf</td>
		<td>bcdf</td>
		<td>bcdf</td>
	</tr>
</table>

As you can see, this puzzle is quite trivial to solve once we have a good
approach.  Just keep chipping away at the problem, removing possible values
for each cell.

Fact 15 means that the blend (l) is smoked in the second house, so update
the table according to this.  Fact 6 says the first house cannot have a bird
option, as Dunhill is smoked there.  Fact 13 forces us to remove the German
option for the second house, since blend is smoked there.  Fact 3 again
gives us tea for this house, and this leaves beer for the last house.

Fact 10 means the Norwegian keeps cats.

<table class="table">
	<tr>
		<td>Colour</td>
		<td>y</td>
		<td>b</td>
		<td>r</td>
		<td>g</td>
		<td>w</td>
	</tr>
	<tr>
		<td>Nationality</td>
		<td>n</td>
		<td>d</td>
		<td>b</td>
		<td>gs</td>
		<td>gs</td>
	</tr>
	<tr>
		<td>Beverage</td>
		<td>w</td>
		<td>t</td>
		<td>m</td>
		<td>c</td>
		<td>b</td>
	</tr>
	<tr>
		<td>Cigar</td>
		<td>d</td>
		<td>l</td>
		<td>pr</td>
		<td>pr</td>
		<td>bpr</td>
	</tr>
	<tr>
		<td>Pet</td>
		<td>c</td>
		<td>h</td>
		<td>bf</td>
		<td>bdf</td>
		<td>bdf</td>
	</tr>
</table>

Now everything falls into place.  We will now skip all the intermediate
steps and just show you the final table.

<table class="table">
	<tr>
		<td>Colour</td>
		<td>y</td>
		<td>b</td>
		<td>r</td>
		<td>g</td>
		<td>w</td>
	</tr>
	<tr>
		<td>Nationality</td>
		<td>n</td>
		<td>d</td>
		<td>b</td>
		<td>g</td>
		<td>s</td>
	</tr>
	<tr>
		<td>Beverage</td>
		<td>w</td>
		<td>t</td>
		<td>m</td>
		<td>c</td>
		<td>b</td>
	</tr>
	<tr>
		<td>Cigar</td>
		<td>d</td>
		<td>l</td>
		<td>p</td>
		<td>r</td>
		<td>b</td>
	</tr>
	<tr>
		<td>Pet</td>
		<td>c</td>
		<td>h</td>
		<td>b</td>
		<td>f</td>
		<td>d</td>
	</tr>
</table>

[zebra]: https://en.wikipedia.org/wiki/Zebra_Puzzle
]]></content:encoded>
      <dc:date>2008-01-01T00:00:00+00:00</dc:date>
    </item>
    <item>
      <title>Using Lua with C++</title>
      <link>https://csl.name/post/lua-and-cpp/</link>
      <description><![CDATA[In this short tutorial I&#39;ll show how to run Lua programs from C and C++ and how
to expose functions to them. It&#39;s easy!
]]></description>
      <pubDate>Fri, 20 Oct 2006 19:42:24 +0000</pubDate>
      <guid>https://csl.name/post/lua-and-cpp/</guid>
      <content:encoded><![CDATA[In this short tutorial I'll show how to run Lua programs from C and C++ and how
to expose functions to them. It's easy!

**Update:** The code in this post has been updated for Lua 5.2.4. I haven't
checked if the Lua 5.3 C API is backwards-compatible with 5.2. All the code
here is available <a href="https://github.com/cslarsen/lua-cpp">on GitHub</a>.

The first program will just create a Lua state object and exit. It will be a
hybrid between C and C++. Since the two languages must include different files,
we need to discern between them by checking for the existence of the
`__cplusplus` macro.

    #ifdef __cplusplus
    # include <lua5.2/lua.hpp>
    #else
    # include <lua5.2/lua.h>
    # include <lua5.2/lualib.h>
    # include <lua5.2/lauxlib.h>
    #endif

    int main()
    {
      lua_State *state = luaL_newstate();
      lua_close(state);
      return 0;
    }

Notice that I'm being explicit about which version of Lua I'm using in the
code. If you trust that the Lua developers care about compatibility, you can
just `#include <lua.hpp>` and so on directly.

The purpose of the program is just to make sure that we can compile, link and
run it without errors.

You need to let the compiler know where it can find the include files and the
Lua shared library. The include files are usually located in
`/usr/local/include` and the library files in `/usr/local/lib`. Search your
system directories if needed.  To compile the above program, pass the
directories with `-I` and `-L`, respectively.

    $ g++ -W -Wall -g -o first first.cpp \
        -I/usr/local/include -L/usr/local/lib -llua

You may swap out `g++` with `llvm-g++`, or just `c++`, depending on your
compiler. If you're using a C compiler, use `gcc` or `llvm-gcc` — but
remember to rename the file to `first.c`.

Now try to run the program to make sure it doesn't segfault:

    $ ./first
    $ echo $?
    0

This one worked just fine.

Executing Lua programs from a host
----------------------------------

The next step is to execute Lua programs from your C or C++ code. We'll create
the Lua state object as above, load a file from disk and execute it.

Put this into `runlua.cpp` or `runlua.c`:

    #include <stdio.h>

    #ifdef __cplusplus
    # include <lua5.2/lua.hpp>
    #else
    # include <lua5.2/lua.h>
    # include <lua5.2/lualib.h>
    # include <lua5.2/lauxlib.h>
    #endif

    void print_error(lua_State* state) {
      // The error message is on top of the stack.
      // Fetch it, print it and then pop it off the stack.
      const char* message = lua_tostring(state, -1);
      puts(message);
      lua_pop(state, 1);
    }

    void execute(const char* filename)
    {
      lua_State *state = luaL_newstate();

      // Make standard libraries available in the Lua object
      luaL_openlibs(state);

      int result;

      // Load the program; this supports both source code and bytecode files.
      result = luaL_loadfile(state, filename);

      if ( result != LUA_OK ) {
        print_error(state);
        return;
      }

      // Finally, execute the program by calling into it.
      // Change the arguments if you're not running vanilla Lua code.

      result = lua_pcall(state, 0, LUA_MULTRET, 0);

      if ( result != LUA_OK ) {
        print_error(state);
        return;
      }
    }

    int main(int argc, char** argv)
    {
      if ( argc <= 1 ) {
        puts("Usage: runlua file(s)");
        puts("Loads and executes Lua programs.");
        return 1;
      }

      // Execute all programs on the command line
      for ( int n=1; n<argc; ++n ) {
        execute(argv[n]);
      }

      return 0;
    }

You can reuse the compilation arguments from above:

    $ g++ -W -Wall -g -I/usr/local/include \
        -L/usr/local/lib -llua runlua.cpp -o runlua

or

    $ gcc -W -Wall -g -I/usr/local/include \
        -L/usr/local/lib -llua runlua.c -o runlua

Running Lua programs
--------------------

Let's test this with some Lua programs. The first one prints the Lua version
and exits.

    io.write(string.format("Hello from %s\n", _VERSION))

You may want to double-check that it works by running `lua hello.lua`. It may
not be important for this trivial program, but can become important when you
try more advanced ones.

    $ lua lua/hello.lua
    Hello from Lua 5.2

Now try it with `runlua`:

    $ ./runlua lua/hello.lua
    Hello from Lua 5.2

You can even run bytecode-compiled programs:

    $ luac -o lua/hello.luac lua/hello.lua
    $ ./runlua lua/hello.luac
    Hello from Lua 5.2

We should also check that the error handling works. Put some garbage in a file
called `error.lua`, for example

    This file is not a Lua program.

Running it produces

    $ ./runlua lua/error.lua
    lua/error.lua:1: syntax error near 'is'

Calling C functions from Lua
----------------------------

It gets very interesting when Lua programs call back to your C or C++
functions. We'll create a function called `howdy` that prints its input
arguments and returns the integer 123.

To be on the safe side, we'll declare C linkage for the function in the C++
version of the program. This has to do with <a
href="https://en.wikipedia.org/wiki/Name_mangling#C.2B.2B">name mangling</a>,
but in this case, it really doesn't matter: Lua just receives a pointer to a
function, and that's that. But if you start using dynamic loading of shared
libraries through `dlopen` and `dlsym`, this will be an issue. So let's do it
correct from the start.

Copy the above program into a file called `callback.cpp` and add the `howdy`
function.

    #ifdef __cplusplus
    extern "C"
    #endif
    int howdy(lua_State* state)
    {
      // The number of function arguments will be on top of the stack.
      int args = lua_gettop(state);

      printf("howdy() was called with %d arguments:\n", args);

      for ( int n=1; n<=args; ++n) {
        printf("  argument %d: '%s'\n", n, lua_tostring(state, n));
      }

      // Push the return value on top of the stack. NOTE: We haven't popped the
      // input arguments to our function. To be honest, I haven't checked if we
      // must, but at least in stack machines like the JVM, the stack will be
      // cleaned between each function call.

      lua_pushnumber(state, 123);

      // Let Lua know how many return values we've passed
      return 1;
    }

We have to pass the address of this function to Lua along with a name. Put the
following line somewhere between the call to `lua_newstate` and
`luaL_loadfile`:

    // Make howdy() available to Lua programs under the same name.
    lua_register(state, "howdy", howdy);

Create a test program called `callback.lua`

    io.write("Calling howdy() ...\n")
    local value = howdy("First", "Second", 112233)
    io.write(string.format("howdy() returned: %s\n", tostring(value)))

Compile and test it

    $ g++ -W -Wall -g -I/usr/local/include -L/usr/local/lib \
        -llua  callback.cpp -o callback
    $ ./callback lua/callback.lua
    Calling howdy() ...
    howdy() was called with 3 arguments:
      argument 1: 'First'
      argument 2: 'Second'
      argument 3: '112233'
    howdy() returned: 123

I told you it was easy!

What next?
----------

Read the <a href="https://www.lua.org/manual/5.2/manual.html#4">Lua C API
Reference</a>. You've learned enough now to get going with it. Did you see my
note about clearing the stack in `howdy`? You may want to investigate that.

Find out how to integrate Lua closures with your C functions.

If you want to hide or catch console output from Lua, you need to figure that
out as well. I once did it by trapping `io.write()`; I copied its code from
`lualib.c` and changed `io_write` to point to my own function. There is
probably a better way to do it, though. Doing so is useful for things like game
programming.

Use <a
href="https://en.wikipedia.org/wiki/Resource_Acquisition_Is_Initialization">RAII</a>
or smart pointers to manage resources like `lua_State`.

I also strongly recommend to try out <a href="http://luajit.org">LuaJIT</a>.
Calling into your functions there is even easier, using LuaJIT's foreign
function library. <a href="/post/luajit-cpp">I'll write a blog post on how to
do that as well</a>. In short, just create ordinary C functions, compile as a
shared library, copy their signatures into pure Lua source code and hook them
up with <a href="http://luajit.org/ext_ffi_tutorial.html">LuaJIT's FFI
library</a>.

LuaJIT runs between 10-20 and up to 135 times faster than interpreted Lua, so
it's definitely worth it.
]]></content:encoded>
      <dc:date>2006-10-20T19:42:24+00:00</dc:date>
    </item>
    <item>
      <title>Calling C functions from Python</title>
      <link>https://csl.name/post/c-functions-python/</link>
      <description><![CDATA[Here&#39;s a small tutorial on how to call your C functions from Python.
]]></description>
      <pubDate>Tue, 28 Mar 2006 00:00:00 +0000</pubDate>
      <guid>https://csl.name/post/c-functions-python/</guid>
      <content:encoded><![CDATA[Here's a small tutorial on how to call your C functions from Python.

Let's make some simple functions in C. We'll call the file `myModule.c`.

{% highlight C %}
#include <Python.h>

/*
 * Function to be called from Python
 */
static PyObject* py_myFunction(PyObject* self, PyObject* args)
{
  char *s = "Hello from C!";
  return Py_BuildValue("s", s);
}

/*
 * Another function to be called from Python
 */
static PyObject* py_myOtherFunction(PyObject* self, PyObject* args)
{
  double x, y;
  PyArg_ParseTuple(args, "dd", &x, &y);
  return Py_BuildValue("d", x*y);
}

/*
 * Bind Python function names to our C functions
 */
static PyMethodDef myModule_methods[] = {
  {"myFunction", py_myFunction, METH_VARARGS},
  {"myOtherFunction", py_myOtherFunction, METH_VARARGS},
  {NULL, NULL}
};

/*
 * Python calls this to let us initialize our module
 */
void initmyModule()
{
  (void) Py_InitModule("myModule", myModule_methods);
}
{% endhighlight %}

Compiling dynamic libraries on Mac OS X is different from the usual gcc -shared
you might be used to:

    gcc -dynamiclib -I/usr/include/python2.3/ -lpython2.3 -o myModule.dylib myModule.c

Now you have to do something awkward; rename myModule.dylib to myModule.so, so
that Python will find the correct file (this is a bug in Python, it should've
been fixed, but that's as far as I know):

    mv myModule.dylib myModule.so

If you are using a system that supports -shared you can simply do this:

    gcc -shared -I/usr/include/python2.3/ -lpython2.3 -o myModule.so myModule.c

On Windows you can reportedly type

    gcc -shared -IC:\Python27\include -LC:\Python27\libs myModule.c -lpython27 -o myModule.pyd

Here's a simple program in Python to call your functions:

{% highlight Python %}
from myModule import *

print "Result from myFunction:", myFunction()
print "Result from myOtherFunction(4.0, 5.0):", myOtherFunction(4.0, 5.0)
{% endhighlight %}

The output is:

    Result from myFunction(): Hello from C!
    Result from myOtherFunction(4.0, 5.0): 20.0

If you are going to make bigger libraries available from Python I *strongly*
suggest you check out [SWIG][swig] or [Boost Python][boost.python].

[boost.python]: http://www.boost.org/doc/libs/1_57_0/libs/python/doc/index.html
[swig]: http://www.swig.org
]]></content:encoded>
      <dc:date>2006-03-28T00:00:00+00:00</dc:date>
    </item>
    <dc:date>2017-11-16T19:52:12+00:00</dc:date>
  </channel>
</rss>